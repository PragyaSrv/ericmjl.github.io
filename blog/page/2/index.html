<!doctype html>

<head>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta charset="utf-8">
    <!-- Bootstrap v4beta Imports -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.1/css/bootstrap.min.css" integrity="sha384-WskhaSGFgHYWDcbwN70/dfYBj47jz9qbsMId/iRN3ewGhXQFZCSftd1LZCfmhktB" crossorigin="anonymous">

    <style media="screen">
        body {
            padding-top: 70px;
            padding-bottom: 70px;
        }
    </style>

    
    

    <!-- Google Analytics -->
    <script>
        (function(i, s, o, g, r, a, m) {
            i['GoogleAnalyticsObject'] = r;
            i[r] = i[r] || function() {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date();
            a = s.createElement(o),
                m = s.getElementsByTagName(o)[0];
            a.async = 1;
            a.src = g;
            m.parentNode.insertBefore(a, m)
        })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');

        ga('create', 'UA-12498603-2', 'auto');
        ga('send', 'pageview');
    </script>

    <!-- ClustrMaps Tracking -->
    <!-- <script type="text/javascript" id="clstr_globe" src="//cdn.clustrmaps.com/globe.js?d=nhKoDpoTjWz4pC6CwI-fSy4hPoJ1uXwTLCfMCT3OK_8"></script> -->

    <!-- FontAwesome embed -->
    <!-- <script src="https://use.fontawesome.com/cb9dbe8e41.js"></script> -->
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.1.1/css/all.css" integrity="sha384-O8whS3fhG2OnA5Kas0Y9l3cfpmYjapjI0E4theH4iuMD+pLhbf6JI0jIMfYcK3yZ" crossorigin="anonymous">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/static/css/custom.css">
</head>

<title>Blog - Eric J. Ma's Personal Site</title>
<body class="body">
    <nav class="navbar navbar-expand-sm navbar-light fixed-top bg-light">
        <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#local-links" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="local-links">
            <ul class="navbar-nav mr-auto">
                
                <li class="nav-item">
                    <a class="nav-link" href="/"><i class="fa fa-home" aria-hidden="true"></i> Home</a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="https://ericmjl.github.io/resume"><i class="fa far fa-file-alt" aria-hidden="true"></i> Resume</a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="/blog"><i class="fa fa-rss" aria-hidden="true"></i> Blog</a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="/open-source"><i class="fa fa-code" aria-hidden="true"></i> Open Source</a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="/projects"><i class="fa fa-briefcase" aria-hidden="true"></i> Projects</a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="/talks"><i class="fa fa-microphone" aria-hidden="true"></i> Talks</a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="/teaching"><i class="fa fa-university" aria-hidden="true"></i> Teaching</a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="/bio"><i class="fa fa-user" aria-hidden="true"></i> Bio</a>
                </li>
                
            </ul>
        </div>
    </nav>
    <div class="container">
        
  
    
  <!-- Import stylesheet -->
  <link rel="stylesheet" href="../../../static/pygments.css">
  <div>

  <!-- Set title style -->
  
    <h1><a href="../../../blog/2018/12/16/gaussian-process-notes/">Gaussian Process Notes</a></h1>
  

  <!-- Append author -->
  <p class="meta">
    written by
    
      <a href="https://twitter.com/ericmjl">Eric J. Ma</a>
    
    on 2018-12-16
  
  </p>
  <!-- Append tags after author -->
  <p>
      <i class="fas fa-tag"></i>
      <a href="../../../blog/tag/data science/">
          data science
      </a>
      <i class="fas fa-tag"></i>
      <a href="../../../blog/tag/bayesian/">
          bayesian
      </a>
      
  </p>
  

  <!-- Put post body -->
  <p>I first learned GPs about two years back, and have been fascinated by the idea. I learned it through a video by David MacKay, and managed to grok it enough that I could put it to use in simple settings. That was reflected in my <a href="https://fluforecaster.herokuapp.com">Flu Forecaster project</a>, in which my GPs were trained only on individual latent spaces.</p>
<p>Recently, though, I decided to seriously sit down and try to grok the math behind GPs (and other machine learning models). To do so, I worked through <a href="https://youtu.be/4vGiHC35j9s">Nando de Freitas' YouTube videos on GPs</a>. (Super thankful that he has opted to put these videos up online!)</p>
<p>The product of this learning is two-fold. Firstly, I have added a <a href="https://github.com/ericmjl/bayesian-analysis-recipes/blob/master/notebooks/gp-test.ipynb">GP notebook</a> to my <a href="https://github.com/ericmjl/bayesian-analysis-recipes">Bayesian analysis recipes</a> repository.</p>
<p>Secondly, I have also put together some hand-written notes on GPs. (For those who are curious, I first hand-wrote them on paper, then copied them into my iPad mini using a Wacom stylus. We don't have the budget at the moment for an iPad Pro!) They can be downloaded <a href="../../../blog/2018/12/16/gaussian-process-notes/gaussian-processes.pdf">here</a>.</p>
<p>Some lessons learned:</p>
<ul>
<li>Algebra is indeed a technology of sorts (to quote Jeremy Kun's book). Being less sloppy than I used to be gives me the opportunity to connect ideas on the page to ideas in my head, and express them more succinctly.</li>
<li>Grokking the math behind GPs at the minimum requires one thing: remembering, or else knowing how to derive, the formula for how to get the distribution parameters of a multivariate Gaussian conditioned on some of of its variables.</li>
<li>Once I grokked the math, implementing a GP using only NumPy was trivial; also, extending it to higher dimensions was similarly trivial!</li>
</ul>

  </div>

    <p><i>Did you enjoy this blog post? <a href="../../../blog/2018/12/16/gaussian-process-notes/#disqus">Let's discuss more</a>!</i></p>
    <hr class="fancy">
  
    
  <!-- Import stylesheet -->
  <link rel="stylesheet" href="../../../static/pygments.css">
  <div>

  <!-- Set title style -->
  
    <h1><a href="../../../blog/2018/12/9/mathematical-intuition/">Mathematical Intuition</a></h1>
  

  <!-- Append author -->
  <p class="meta">
    written by
    
      <a href="https://twitter.com/ericmjl">Eric J. Ma</a>
    
    on 2018-12-09
  
  </p>
  <!-- Append tags after author -->
  <p>
      <i class="fas fa-tag"></i>
      <a href="../../../blog/tag/deep learning/">
          deep learning
      </a>
      <i class="fas fa-tag"></i>
      <a href="../../../blog/tag/bayesian/">
          bayesian
      </a>
      <i class="fas fa-tag"></i>
      <a href="../../../blog/tag/math/">
          math
      </a>
      <i class="fas fa-tag"></i>
      <a href="../../../blog/tag/data science/">
          data science
      </a>
      
  </p>
  

  <!-- Put post body -->
  <p>Last week, I picked up Jeremy Kun's book, "A Programmer's Introduction to Mathematics". In it, I finally found an explanation for my frustrations when reading math papers:</p>
<blockquote><p>What programmers would consider “sloppy” notation is one symptom of the problem, but there there are other expectations on the reader that, for better or worse, decelerate the pace of reading. Unfortunately I have no solution here. Part of the power and expressiveness of mathematics is the ability for its practitioners to overload, redefine, and omit in a suggestive manner. Mathematicians also have thousands of years of “legacy” math that require backward compatibility. Enforcing a single specification for all of mathematics—a suggestion I frequently hear from software engineers—would be horrendously counterproductive.</p>
</blockquote>
<p>Reading just <em>that</em> paragraph explained, in such a lucid manner, how my frustrations reading mathematically-oriented papers, stemmed from mismatched expectations. I come into a paper thinking like a software engineer. Descriptive variable names (as encouraged by Python), which are standardized as well, with structured abstractions providing a hierarchy of logic between chunks of code... No, mathematicians are more like Shakespeare - or perhaps linguists - in that they will take a symbol and imbibe it with a subtly new meaning or interpretation inspired by a new field. That "L" you see in one field of math doesn't always exactly mean the same thing in another field.</p>
<h2 id="biology-vs.-math?">Biology vs. Math?</h2><p>The contrast is stark when compared against reading a biology paper. With a biology paper, if you know the key wet-bench experiment types (and there's not that many), you can essentially get the gist of a paper by reading the abstract and dissecting the figures, which, granted, are described and labelled with field-specific jargon, but are at least descriptive names. With a math-oriented paper, the equations are the star, and one has to really grok each element of the equations to know what they mean. It means taking the time to dissect each equation and ask what each symbol is, what each group of symbols means, and how those underlying ideas connect with one another and with other ideas. It's not unlike a biology paper, but requiring a different kind of patience, one that I wasn't trained in.</p>
<h2 id="learning-to-learn-by-teaching">Learning to Learn by Teaching</h2><p>As Jeremy Kun wrote in his book, programmers do have some sort of a leg-up when it comes to reading and understanding math. It's a bit more than what Kun wrote, I think - yes, many programming ideas have deep mathematical connections. But I think there's more.</p>
<p>One thing we know from research into how people learn is that teaching someone something is an incredible way to learn that something. From my prior experience, the less background a student has in a material, the more demands are placed on the teacher's understanding of the material, as we work out how the multiple representations in our head to try to communicate it to them.</p>
<p>As it turns out, we programmers have the ultimate dumb "student" available at our fingertips: Our computers! By implementing mathematical ideas in code, we are essentially "teaching" the computer to do something mathematical. Computers are not smart; they are programmed to do exactly what we input to them. If we get an idea wrong, our implementation of the math will likely be wrong. That fundamental law of computing shows up again: Garbage in, garbage out.</p>
<h2 id="hierarchy-of-ideas">Hierarchy of Ideas</h2><p>More than just that, when we programmers implement a mathematical idea in code, we can start putting our "good software engineering" ideas into place! It helps the math become stickier when we can see, through code, the hierarchy of concepts that are involved.</p>
<p>An example, for me, comes from the deep learning world. I had an attempt dissecting two math-y deep learning papers last week. Skimming through the papers didn't do much good for my understanding of the paper. Neither did trying to read the paper like I do a biology paper. Sure, I could perhaps just read the ideas that the authors were describing in prose, but I had no intuition on which to base a proper critique of the idea's usefulness. It took implementing those papers in Python code, writing tests for them, and using abstractions that I had previously written, to come to a place where I felt like the ideas in the paper were a flexibly wieldable tool in my toolkit.</p>
<p>Reinventing the wheel, such that we can learn the wheel, can in fact help us decompose the wheel so that we can do other new things with it. Human creativity is such a wonderful thing!</p>

  </div>

    <p><i>Did you enjoy this blog post? <a href="../../../blog/2018/12/9/mathematical-intuition/#disqus">Let's discuss more</a>!</i></p>
    <hr class="fancy">
  
    
  <!-- Import stylesheet -->
  <link rel="stylesheet" href="../../../static/pygments.css">
  <div>

  <!-- Set title style -->
  
    <h1><a href="../../../blog/2018/11/13/solving-problems-actionably/">Solving Problems Actionably</a></h1>
  

  <!-- Append author -->
  <p class="meta">
    written by
    
      <a href="https://twitter.com/ericmjl">Eric J. Ma</a>
    
    on 2018-11-13
  
  </p>
  <!-- Append tags after author -->
  <p>
      <i class="fas fa-tag"></i>
      <a href="../../../blog/tag/data science/">
          data science
      </a>
      <i class="fas fa-tag"></i>
      <a href="../../../blog/tag/insight data science/">
          insight data science
      </a>
      
  </p>
  

  <!-- Put post body -->
  <p>There's a quote by John Tukey that has been a recurrent theme at work.</p>
<blockquote class="blockquote">
<p>It's better to solve the right problem approximately than to solve the wrong problem exactly.</p>
</blockquote><p>Continuing on the theme of quoting two Georges:</p>
<blockquote class="blockquote"> 
<p>All models are wrong, but some are more useful than others.</p>
</blockquote><p>H/T Allen Downey for pointing out that our minds think alike.</p>
<p>I have been working on a modelling effort for colleagues at work. There were two curves involved, and the second depended on the first one.</p>
<p>In both cases, I started with a simple model, and made judgment calls along the way as to whether to continue improving the model, or to stop there because the current iteration of the model was sufficient enough to act on. With first curve, the first model was actionable for me. With the second curve, the first model I wrote clearly wasn't good enough to be actionable, so I spent lots more rounds of iteration on it.</p>
<p>But wait, how does one determine "actionability"?</p>
<h2 id="actionability">Actionability</h2><p><strong>For myself</strong>, it has generally meant that I'm confident enough in the results to take the next modelling step. My second curves depended on the first curves, and after double-checking multiple ways, I thought the first curve fits, though not perfect, were good enough when applied across a large number of samples that I could instead move on to the second curves.</p>
<p><strong>For others</strong>, particularly at my workplace, it generally means a scientist can make a decision about what next experiment to run.</p>
<h2 id="insight-s-mvp-influence">Insight's MVP Influence</h2><p>Going through Insight Data Science drilled into us an instinct for developing an MVP for our problem before going on to perfect it. I think that general model works well. My project's final modelling results will be the result of chains of modelling assumptions at every step. Documenting those steps clearly, and then being willing to revisit those assumptions, is going always a good thing.</p>

  </div>

    <p><i>Did you enjoy this blog post? <a href="../../../blog/2018/11/13/solving-problems-actionably/#disqus">Let's discuss more</a>!</i></p>
    <hr class="fancy">
  
    
  <!-- Import stylesheet -->
  <link rel="stylesheet" href="../../../static/pygments.css">
  <div>

  <!-- Set title style -->
  
    <h1><a href="../../../blog/2018/11/12/thoughts-on-black/">Thoughts on Black</a></h1>
  

  <!-- Append author -->
  <p class="meta">
    written by
    
      <a href="https://twitter.com/ericmjl">Eric J. Ma</a>
    
    on 2018-11-12
  
  </p>
  <!-- Append tags after author -->
  <p>
      <i class="fas fa-tag"></i>
      <a href="../../../blog/tag/python/">
          python
      </a>
      <i class="fas fa-tag"></i>
      <a href="../../../blog/tag/code style/">
          code style
      </a>
      
  </p>
  

  <!-- Put post body -->
  <p>Having used Black for quite a while now, I have a hunch that it will continue to surpass its current popularity amongst projects.</p>
<p>It's one thing to be opinionated about things that matter for a project, but don't matter personally. Like code style. It's another thing to actually build a tool that, with one command, realizes those opinions in (milli)seconds. That's exactly what Black does.</p>
<p>At the end of the day, it was, and still is, a tool that has a very good human API - that of convenience.</p>
<p>By being opinionated about what code <em>ought</em> to look like, <code>black</code> has very few configurable parameters. Its interface is very simple. <em>Convenient.</em></p>
<p>By automagically formatting <em>every</em> Python file in subdirectories (if not otherwise configured so), it makes code formatting quick and easy. <em>Convenient.</em></p>
<p>In particular, by being opinionated about conforming to community standards for code style with Python, <code>black</code> ensures that formatted code is consistently formatted and thus easy to read. <em>Convenient!</em></p>
<p>Because of this, I highly recommend the use of <code>black</code> for code formatting.</p>
<pre><code>pip install black
</code></pre>

  </div>

    <p><i>Did you enjoy this blog post? <a href="../../../blog/2018/11/12/thoughts-on-black/#disqus">Let's discuss more</a>!</i></p>
    <hr class="fancy">
  
    
  <!-- Import stylesheet -->
  <link rel="stylesheet" href="../../../static/pygments.css">
  <div>

  <!-- Set title style -->
  
    <h1><a href="../../../blog/2018/11/7/bayesian-modelling-is-hard-work/">Bayesian Modelling is Hard Work!</a></h1>
  

  <!-- Append author -->
  <p class="meta">
    written by
    
      <a href="https://twitter.com/ericmjl">Eric J. Ma</a>
    
    on 2018-11-07
  
  </p>
  <!-- Append tags after author -->
  <p>
      <i class="fas fa-tag"></i>
      <a href="../../../blog/tag/bayesian/">
          bayesian
      </a>
      <i class="fas fa-tag"></i>
      <a href="../../../blog/tag/data science/">
          data science
      </a>
      <i class="fas fa-tag"></i>
      <a href="../../../blog/tag/statistics/">
          statistics
      </a>
      
  </p>
  

  <!-- Put post body -->
  <p>It’s definitely not easy work; anybody trying to tell you that you can "just apply this 
model and just be done with it" is probably wrong.</p>
<h2 id="simple-models">Simple Models</h2><p>Let me clarify: I agree that doing the first half of the statement, "just apply this model", 
is a good starting point, but I disagree with the latter half, "and just be done with it". I 
have found that writing and fitting a very naive Bayesian model to the data I have is a very 
simple thing. But doing the right thing is not. Let’s not be confused: I don’t mean a Naive 
Bayes model, I mean naively writing down a Bayesian model that is structured very simply 
with the simplest of priors that you can think of.</p>
<p>Write down the model, including any transformations that you may need on the variables, and 
then lazily put in a bunch of priors. For example, you might just start with Gaussians 
everywhere a parameter could take on negative to positive infinity values, or a bounded Half 
Gaussian if it can only take values above (or below) a certain value. You might assume 
Gaussian-distributed noise in the output.</p>
<p>Let’s still not be confused: Obviously this would not apply to a beta-bernoulli/binomial 
model!</p>
<p>Doing the right thing, however, is where the tricky parts come in. To butcher and mash-up 
two quotes:</p>
<blockquote class="blockquote">
<p>All models are wrong, but some are useful (Box), yet some models are more wrong than others (modifying from Orwell). 
</blockquote><h2 id="critiquing-models">Critiquing Models</h2><p>When doing modeling, a series of questions comes up:</p>
<ul>
<li>Do my naive assumptions about "Gaussians everywhere" hold? </li>
<li>Given that my output data are continuous, is there a better distribution that can describe 
the likelihood?</li>
<li>Is there are more principled prior for some of the variables?</li>
<li>Does my link function, which joins the input data to the output parameters, properly 
describe their relationship?</li>
<li>Instead of independent priors per group, would a group prior be justifiable?</li>
<li>Does my model yield posterior distributions that are within bounds of reasonable ranges, 
which come from my prior knowledge? If it does not, do I need to bound my priors instead of 
naively assuming the full support for those distributions?</li>
</ul>
<p>I am quite sure that this list is non-exhaustive, and probably only covers the bare minimum 
we have to think about.</p>
<p>Doing these model critiques is not easy. Yet, if we are to work towards truthful and 
actionable conclusions, it is a necessity. We want to know ground truth, so that we can act 
on it accordingly, and hence take appropriate actions.</p>
<h2 id="prior-experience">Prior Experience</h2><p>I have experienced this modeling loop that Mike Betancourt describes (in his <a href="https://github.com/betanalpha/jupyter_case_studies/blob/master/principled_bayesian_workflow/principled_bayesian_workflow.ipynb">Principled 
Bayesian Workflow notebook</a>) more than once. One involved count data, with a data 
scientist from TripAdvisor last year at the SciPy conference; another involved estimating 
cycle time distributions at work, and yet another involved a whole 4-parameter dose-response 
curve. In each scenario, model fitting and critique took hours at the minimum; I’d also note 
that with real world data, I didn’t necessarily get to the "win" was looking for.</p>
<p>With the count data, the TripAdvisor data scientist and I reached a point where after 5 
rounds of tweaking his model, we had a model that fit the data, and described a data 
generating process that mimics closely to what we would expect given his process. It took us 
5 rounds, and 3 hours of staring at his model and data, to get there!</p>
<p>Yet with cycle time distributions from work, a task ostensibly much easier ("just fit a 
distribution to the data"), none of my distribution choices, which reflected what I thought 
would be the data generating process, gave me a "good fit" to the data. I checked by many 
means: K-S tests, visual inspection, etc. I ended up abandoning the fitting procedure, and 
used empirical distributions instead.</p>
<p>With a 4-parameter dose-response curve, it took me 6 hours to go through 6 rounds of 
modeling to get to a point where I felt comfortable with the model. I started with a 
simplifying "Gaussians everywhere" assumption. Later, though, I hesitantly and tentatively 
putting in bound priors because I knew some posterior distributions were completely out of 
range under the naive assumptions of the first model, and were likely a result of 
insufficient range in the concentrations tested. Yet even that model remained unsatisfying: 
I was stuck with some compounds that didn’t change the output regardless of concentration, 
and that data are fundamentally very hard to fit with a dose response curve. Thus I the next 
afternoon,I modeled the dose response relationship using a Gaussian Process instead. Neither 
model is completely satisfying to the degree that the count data model was, but both the GP 
and the dose-response curve are and will be roughly correct modeling choices (with the GP 
probably being more flexible), and importantly, both are actionable by the experimentalists.</p>
<h2 id="thoughts">Thoughts</h2><p>As you probably can see, whenever we either (1) don’t know ground truth, and/or (2) have 
messy, real world data that don’t fit idealized assumptions about the data generating 
process, <strong>getting the model "right" is a very hard thing to do!</strong> Moreover, data are 
insufficient on their own to critique the model; we will always need to bring in prior 
knowledge. Much as all probability is conditional probability (Venn), all modeling involves 
prior knowledge. Sometimes it comes up in non-modellable ways, though as far as possible, 
it’s a good exercise to try incorporating that into the model definition.</p>
<h2 id="canned-models?">Canned Models?</h2><p>Even with that said, I’m still a fan of canned models, such as those provided by 
<code>pymc-learn</code> and <code>scikit-learn</code> - provided we recognize that their "canned" nature and are 
equipped to critique and modify said models. Yes, they provide easy, convenient baselines 
that we can get started with. We can "just apply this model". But we can’t "just be done 
with it": the hard part of getting the model right takes much longer and much more hard 
work. <strong><em>Veritas!</em></strong></p>

  </div>

    <p><i>Did you enjoy this blog post? <a href="../../../blog/2018/11/7/bayesian-modelling-is-hard-work/#disqus">Let's discuss more</a>!</i></p>
    <hr class="fancy">
  

  
  <div class="pagination">
    
      <a href="../../../blog/">&laquo; Previous</a>
    
    | 2 |
    
      <a href="../../../blog/page/3/">Next &raquo;</a>
    
  </div>


    </div>

    <nav class="navbar navbar-expand-sm navbar-light fixed-bottom bg-light">
        <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#external-links" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="external-links">
            <ul class="navbar-nav mr-auto">
                
                <li class="nav-item">
                    <a class="nav-link" href="https://www.linkedin.com/in/ericmjl"><i class="fab fa-linkedin" aria-hidden="true"></i> LinkedIn</a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="http://twitter.com/ericmjl"><i class="fab fa-twitter" aria-hidden="true"></i> Twitter</a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="http://github.com/ericmjl"><i class="fab fa-github" aria-hidden="true"></i> GitHub</a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="http://stackoverflow.com/users/1274908/ericmjl"><i class="fab fa-stack-overflow" aria-hidden="true"></i> Stack Overflow</a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="http://shortwhale.com/ericmjl"><i class="far fa-envelope" aria-hidden="true"></i> Contact Me</a>
                </li>
                
            </ul>
        </div>
    </nav>
    <!-- Boostrap JS imports -->
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.1/js/bootstrap.min.js" integrity="sha384-smHYKdLADwkXOn1EmN1qk/HfnUcbVRZyYmZ4qpPea6sjB/pTJ0euyQp0Mk8ck+5T" crossorigin="anonymous"></script>

</body>
