<!doctype html>

<head>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta charset="utf-8">
    <!-- Bootstrap v4beta Imports -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.1/css/bootstrap.min.css" integrity="sha384-WskhaSGFgHYWDcbwN70/dfYBj47jz9qbsMId/iRN3ewGhXQFZCSftd1LZCfmhktB" crossorigin="anonymous">

    <style media="screen">
        body {
            padding-top: 70px;
            padding-bottom: 70px;
        }
    </style>

    
    

    <!-- Google Analytics -->
    <script>
        (function(i, s, o, g, r, a, m) {
            i['GoogleAnalyticsObject'] = r;
            i[r] = i[r] || function() {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date();
            a = s.createElement(o),
                m = s.getElementsByTagName(o)[0];
            a.async = 1;
            a.src = g;
            m.parentNode.insertBefore(a, m)
        })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');

        ga('create', 'UA-12498603-2', 'auto');
        ga('send', 'pageview');
    </script>

    <!-- ClustrMaps Tracking -->
    <!-- <script type="text/javascript" id="clstr_globe" src="//cdn.clustrmaps.com/globe.js?d=nhKoDpoTjWz4pC6CwI-fSy4hPoJ1uXwTLCfMCT3OK_8"></script> -->

    <!-- FontAwesome embed -->
    <!-- <script src="https://use.fontawesome.com/cb9dbe8e41.js"></script> -->
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.1.1/css/all.css" integrity="sha384-O8whS3fhG2OnA5Kas0Y9l3cfpmYjapjI0E4theH4iuMD+pLhbf6JI0jIMfYcK3yZ" crossorigin="anonymous">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/static/css/custom.css">

    <!-- Mathjax -->
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
    </script>

    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>

</head>

<title>Blog - Eric J. Ma's Personal Site</title>
<body class="body">
    <nav class="navbar navbar-expand-sm navbar-light fixed-top bg-light">
        <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#local-links" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="local-links">
            <ul class="navbar-nav mr-auto">
                
                <li class="nav-item">
                    <a class="nav-link" href="/"><i class="fa fa-home" aria-hidden="true"></i> Home</a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="https://ericmjl.github.io/resume"><i class="fa far fa-file-alt" aria-hidden="true"></i> Resume</a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="/blog"><i class="fa fa-rss" aria-hidden="true"></i> Blog</a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="/open-source"><i class="fa fa-code" aria-hidden="true"></i> Open Source</a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="/projects"><i class="fa fa-briefcase" aria-hidden="true"></i> Projects</a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="/talks"><i class="fa fa-microphone" aria-hidden="true"></i> Talks</a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="/teaching"><i class="fa fa-university" aria-hidden="true"></i> Teaching</a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="/bio"><i class="fa fa-user" aria-hidden="true"></i> Bio</a>
                </li>
                
            </ul>
        </div>
    </nav>
    <div class="container">
        
  
    
  <!-- Import stylesheet -->
  <link rel="stylesheet" href="../../../static/pygments.css">
  <div>

  <!-- Set title style -->
  
    <h1><a href="../../../blog/2018/11/12/thoughts-on-black/">Thoughts on Black</a></h1>
  

  <!-- Append author -->
  <p class="meta">
    written by
    
      <a href="https://twitter.com/ericmjl">Eric J. Ma</a>
    
    on 2018-11-12
  
  </p>
  <!-- Append tags after author -->
  <p>
      <i class="fas fa-tag"></i>
      <a href="../../../blog/tag/python/">
          python
      </a>
      <i class="fas fa-tag"></i>
      <a href="../../../blog/tag/code style/">
          code style
      </a>
      
  </p>
  

  <!-- Put post body -->
  <p>Having used Black for quite a while now, I have a hunch that it will continue to surpass its current popularity amongst projects.</p>
<p>It's one thing to be opinionated about things that matter for a project, but don't matter personally. Like code style. It's another thing to actually build a tool that, with one command, realizes those opinions in (milli)seconds. That's exactly what Black does.</p>
<p>At the end of the day, it was, and still is, a tool that has a very good human API - that of convenience.</p>
<p>By being opinionated about what code <em>ought</em> to look like, <code>black</code> has very few configurable parameters. Its interface is very simple. <em>Convenient.</em></p>
<p>By automagically formatting <em>every</em> Python file in subdirectories (if not otherwise configured so), it makes code formatting quick and easy. <em>Convenient.</em></p>
<p>In particular, by being opinionated about conforming to community standards for code style with Python, <code>black</code> ensures that formatted code is consistently formatted and thus easy to read. <em>Convenient!</em></p>
<p>Because of this, I highly recommend the use of <code>black</code> for code formatting.</p>
<pre><code>pip install black
</code></pre>

  </div>

    <p><i>Did you enjoy this blog post? <a href="../../../blog/2018/11/12/thoughts-on-black/#disqus">Let's discuss more</a>!</i></p>
    <hr class="fancy">
  
    
  <!-- Import stylesheet -->
  <link rel="stylesheet" href="../../../static/pygments.css">
  <div>

  <!-- Set title style -->
  
    <h1><a href="../../../blog/2018/11/7/bayesian-modelling-is-hard-work/">Bayesian Modelling is Hard Work!</a></h1>
  

  <!-- Append author -->
  <p class="meta">
    written by
    
      <a href="https://twitter.com/ericmjl">Eric J. Ma</a>
    
    on 2018-11-07
  
  </p>
  <!-- Append tags after author -->
  <p>
      <i class="fas fa-tag"></i>
      <a href="../../../blog/tag/bayesian/">
          bayesian
      </a>
      <i class="fas fa-tag"></i>
      <a href="../../../blog/tag/data science/">
          data science
      </a>
      <i class="fas fa-tag"></i>
      <a href="../../../blog/tag/statistics/">
          statistics
      </a>
      
  </p>
  

  <!-- Put post body -->
  <p>It’s definitely not easy work; anybody trying to tell you that you can "just apply this 
model and just be done with it" is probably wrong.</p>
<h2 id="simple-models">Simple Models</h2><p>Let me clarify: I agree that doing the first half of the statement, "just apply this model", 
is a good starting point, but I disagree with the latter half, "and just be done with it". I 
have found that writing and fitting a very naive Bayesian model to the data I have is a very 
simple thing. But doing the right thing is not. Let’s not be confused: I don’t mean a Naive 
Bayes model, I mean naively writing down a Bayesian model that is structured very simply 
with the simplest of priors that you can think of.</p>
<p>Write down the model, including any transformations that you may need on the variables, and 
then lazily put in a bunch of priors. For example, you might just start with Gaussians 
everywhere a parameter could take on negative to positive infinity values, or a bounded Half 
Gaussian if it can only take values above (or below) a certain value. You might assume 
Gaussian-distributed noise in the output.</p>
<p>Let’s still not be confused: Obviously this would not apply to a beta-bernoulli/binomial 
model!</p>
<p>Doing the right thing, however, is where the tricky parts come in. To butcher and mash-up 
two quotes:</p>
<blockquote class="blockquote">
<p>All models are wrong, but some are useful (Box), yet some models are more wrong than others (modifying from Orwell). 
</blockquote><h2 id="critiquing-models">Critiquing Models</h2><p>When doing modeling, a series of questions comes up:</p>
<ul>
<li>Do my naive assumptions about "Gaussians everywhere" hold? </li>
<li>Given that my output data are continuous, is there a better distribution that can describe 
the likelihood?</li>
<li>Is there are more principled prior for some of the variables?</li>
<li>Does my link function, which joins the input data to the output parameters, properly 
describe their relationship?</li>
<li>Instead of independent priors per group, would a group prior be justifiable?</li>
<li>Does my model yield posterior distributions that are within bounds of reasonable ranges, 
which come from my prior knowledge? If it does not, do I need to bound my priors instead of 
naively assuming the full support for those distributions?</li>
</ul>
<p>I am quite sure that this list is non-exhaustive, and probably only covers the bare minimum 
we have to think about.</p>
<p>Doing these model critiques is not easy. Yet, if we are to work towards truthful and 
actionable conclusions, it is a necessity. We want to know ground truth, so that we can act 
on it accordingly, and hence take appropriate actions.</p>
<h2 id="prior-experience">Prior Experience</h2><p>I have experienced this modeling loop that Mike Betancourt describes (in his <a href="https://github.com/betanalpha/jupyter_case_studies/blob/master/principled_bayesian_workflow/principled_bayesian_workflow.ipynb">Principled 
Bayesian Workflow notebook</a>) more than once. One involved count data, with a data 
scientist from TripAdvisor last year at the SciPy conference; another involved estimating 
cycle time distributions at work, and yet another involved a whole 4-parameter dose-response 
curve. In each scenario, model fitting and critique took hours at the minimum; I’d also note 
that with real world data, I didn’t necessarily get to the "win" was looking for.</p>
<p>With the count data, the TripAdvisor data scientist and I reached a point where after 5 
rounds of tweaking his model, we had a model that fit the data, and described a data 
generating process that mimics closely to what we would expect given his process. It took us 
5 rounds, and 3 hours of staring at his model and data, to get there!</p>
<p>Yet with cycle time distributions from work, a task ostensibly much easier ("just fit a 
distribution to the data"), none of my distribution choices, which reflected what I thought 
would be the data generating process, gave me a "good fit" to the data. I checked by many 
means: K-S tests, visual inspection, etc. I ended up abandoning the fitting procedure, and 
used empirical distributions instead.</p>
<p>With a 4-parameter dose-response curve, it took me 6 hours to go through 6 rounds of 
modeling to get to a point where I felt comfortable with the model. I started with a 
simplifying "Gaussians everywhere" assumption. Later, though, I hesitantly and tentatively 
putting in bound priors because I knew some posterior distributions were completely out of 
range under the naive assumptions of the first model, and were likely a result of 
insufficient range in the concentrations tested. Yet even that model remained unsatisfying: 
I was stuck with some compounds that didn’t change the output regardless of concentration, 
and that data are fundamentally very hard to fit with a dose response curve. Thus I the next 
afternoon,I modeled the dose response relationship using a Gaussian Process instead. Neither 
model is completely satisfying to the degree that the count data model was, but both the GP 
and the dose-response curve are and will be roughly correct modeling choices (with the GP 
probably being more flexible), and importantly, both are actionable by the experimentalists.</p>
<h2 id="thoughts">Thoughts</h2><p>As you probably can see, whenever we either (1) don’t know ground truth, and/or (2) have 
messy, real world data that don’t fit idealized assumptions about the data generating 
process, <strong>getting the model "right" is a very hard thing to do!</strong> Moreover, data are 
insufficient on their own to critique the model; we will always need to bring in prior 
knowledge. Much as all probability is conditional probability (Venn), all modeling involves 
prior knowledge. Sometimes it comes up in non-modellable ways, though as far as possible, 
it’s a good exercise to try incorporating that into the model definition.</p>
<h2 id="canned-models?">Canned Models?</h2><p>Even with that said, I’m still a fan of canned models, such as those provided by 
<code>pymc-learn</code> and <code>scikit-learn</code> - provided we recognize that their "canned" nature and are 
equipped to critique and modify said models. Yes, they provide easy, convenient baselines 
that we can get started with. We can "just apply this model". But we can’t "just be done 
with it": the hard part of getting the model right takes much longer and much more hard 
work. <strong><em>Veritas!</em></strong></p>

  </div>

    <p><i>Did you enjoy this blog post? <a href="../../../blog/2018/11/7/bayesian-modelling-is-hard-work/#disqus">Let's discuss more</a>!</i></p>
    <hr class="fancy">
  
    
  <!-- Import stylesheet -->
  <link rel="stylesheet" href="../../../static/pygments.css">
  <div>

  <!-- Set title style -->
  
    <h1><a href="../../../blog/2018/10/26/more-dask-pre-scattering-data/">More Dask: Pre-Scattering Data</a></h1>
  

  <!-- Append author -->
  <p class="meta">
    written by
    
      <a href="https://twitter.com/ericmjl">Eric J. Ma</a>
    
    on 2018-10-26
  
  </p>
  <!-- Append tags after author -->
  <p>
      <i class="fas fa-tag"></i>
      <a href="../../../blog/tag/dask/">
          dask
      </a>
      <i class="fas fa-tag"></i>
      <a href="../../../blog/tag/parallel/">
          parallel
      </a>
      <i class="fas fa-tag"></i>
      <a href="../../../blog/tag/data science/">
          data science
      </a>
      <i class="fas fa-tag"></i>
      <a href="../../../blog/tag/optimization/">
          optimization
      </a>
      <i class="fas fa-tag"></i>
      <a href="../../../blog/tag/gridengine/">
          gridengine
      </a>
      
  </p>
  

  <!-- Put post body -->
  <p>I learned a new thing about <code>dask</code> yesterday: pre-scattering data properly!</p>
<p>Turns out, you can pre-scatter your data across worker nodes, and have them access that data later when submitting functions to the scheduler.</p>
<h2 id="how-to">How-To</h2><p>To do so, we first call on <code>client.scatter</code>, pass in the data that I want to scatter across all nodes, ensure that broadcasting is turned on (if and only if I am sure that all worker nodes will need it), and finally assign it to a new variable.</p>
<div class="hll"><pre><span></span><span class="kn">from</span> <span class="nn">dask_jobqueue</span> <span class="kn">import</span> <span class="n">SGECluster</span>
<span class="kn">from</span> <span class="nn">dask.distributed</span> <span class="kn">import</span> <span class="n">Client</span>

<span class="n">cluster</span> <span class="o">=</span> <span class="n">SGECluster</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>  <span class="c1"># put parameters in there.</span>
<span class="n">client</span> <span class="o">=</span> <span class="n">Client</span><span class="p">(</span><span class="n">cluster</span><span class="p">)</span>
<span class="n">data_future</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">broadcast</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>
<p>One key thing to remember here is to assign the result of <code>client.scatter</code> to a variable. This becomes a pointer that you pass into other functions that are submitted via the <code>client.submit</code> interface. Because this point is not immediately clear from the <code>client.scatter</code> docs, <a href="https://github.com/dask/distributed/pull/2320">I put in a pull request (PR) to provide some just-in-time documentation</a>, which just got merged this morning. By the way, not every PR has to be code - documentation help is always good!</p>
<p>Once we've scattered the data across our worker nodes and obtained a pointer for the scattered data, we can parallel submit our function across worker nodes.</p>
<p>Let's say we have a function, called <code>func</code>, that takes in the <code>data</code> variable and returns a number. The key characteristic of this function is that it takes anywhere from a few seconds to minutes to run, but I need it run many times (think hundreds to thousands of times).</p>
<p>In serial, I would usually do this as a list comprehension:</p>
<div class="hll"><pre><span></span><span class="n">results</span> <span class="o">=</span> <span class="p">[</span><span class="n">func</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">200</span><span class="p">)]</span>
</pre></div>
<p>If done in parallel, I can now use the <code>client</code> object to submit the function across all worker nodes. For clarity, let me switch to a <code>for-loop</code> instead:</p>
<div class="hll"><pre><span></span><span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">200</span><span class="p">):</span>
    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">client</span><span class="o">.</span><span class="n">submit</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">data_future</span><span class="p">))</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
</pre></div>
<p>Because the <code>client</code> does not have to worry about sending the large <code>data</code> object across the network of cluster nodes, it is very fast to submit the functions to the scheduler, which then dispatches it to the worker nodes, which all know where <code>data_future</code> is on their own "virtual cluster" memory.</p>
<h2 id="advantages">Advantages</h2><p>By pre-scattering, we invest a bit of time pre-allocating memory on worker nodes to hold data that are relatively expensive to transfer. This time investment reaps dividends later when we are working with functions that operate on the data.</p>
<h2 id="cautions">Cautions</h2><p>Not really disadvantages (as I can't think of any), just some things to note:</p>
<ol>
<li>You need to know how much memory my data requires, and have to request for at least that amount of memory first per worker node at the the <code>SGECluster</code> instantiation step.</li>
<li>Pre-scattering sometimes takes a bit of time, but I have not seen it take as much time as having the scheduler handle everything.</li>
</ol>
<h2 id="acknowledgments">Acknowledgments</h2><p>Special thanks goes to <a href="https://matthewrocklin.com">Matt Rocklin</a>, who answered my question on <a href="https://stackoverflow.com/questions/52997229/is-there-an-advantage-to-pre-scattering-data-objects-in-dask">StackOverflow</a>, which in turn inspired this blog post.</p>

  </div>

    <p><i>Did you enjoy this blog post? <a href="../../../blog/2018/10/26/more-dask-pre-scattering-data/#disqus">Let's discuss more</a>!</i></p>
    <hr class="fancy">
  
    
  <!-- Import stylesheet -->
  <link rel="stylesheet" href="../../../static/pygments.css">
  <div>

  <!-- Set title style -->
  
    <h1><a href="../../../blog/2018/10/11/parallel-processing-with-dask-on-gridengine-clusters/">Parallel Processing with Dask on GridEngine Clusters</a></h1>
  

  <!-- Append author -->
  <p class="meta">
    written by
    
      <a href="https://twitter.com/ericmjl">Eric J. Ma</a>
    
    on 2018-10-11
  
  </p>
  <!-- Append tags after author -->
  <p>
      <i class="fas fa-tag"></i>
      <a href="../../../blog/tag/parallel/">
          parallel
      </a>
      <i class="fas fa-tag"></i>
      <a href="../../../blog/tag/dask/">
          dask
      </a>
      <i class="fas fa-tag"></i>
      <a href="../../../blog/tag/gridengine/">
          gridengine
      </a>
      <i class="fas fa-tag"></i>
      <a href="../../../blog/tag/data science/">
          data science
      </a>
      <i class="fas fa-tag"></i>
      <a href="../../../blog/tag/optimization/">
          optimization
      </a>
      
  </p>
  

  <!-- Put post body -->
  <p>I recently just figured out how to get this working... and it's awesome! :D</p>
<h2 id="motivation">Motivation</h2><p>If I'm developing an analysis in the Jupyter notebook, and I have one semi-long-running function (e.g. takes dozens of seconds) that I need to run over tens to hundreds of thousands of similar inputs, it'll take <em>ages</em> for this to complete in serial. For a sense of scale, a function that takes ~20 seconds per call run serially over 10,000 similar inputs would take 200,000 seconds, which is 2 days of run-time (not including any other overhead). That's not feasible for interactive exploration of data. If I could somehow parallelize just the function over 500 compute nodes, we could take the time down to 7 minutes.</p>
<p>GridEngine-based compute clusters are one of many options for parallelizing work. During grad school at MIT, and at work at Novartis, the primary compute cluster environment that I've encountered has been GridEngine-based. However, because they are designed for batch jobs, as a computational scientist, we have to jump out of whatever development environment we're currently using, and move to custom scripts.</p>
<p>In order to do parallelism with traditional GridEngine systems, I would have to jump out of the notebook and start writing job submission scripts, which disrupts my workflow. I would be disrupting my thought process, and lose the interactivity that I might need to prototype my work faster.</p>
<h2 id="enter-dask">Enter Dask</h2><p><code>dask</code>, alongside <code>dask-jobqueue</code> enables computational scientists like myself to take advantage of existing GridEngine setups to do interactive, parallel work. As long as I have a Jupyter notebook server running on a GridEngine-connected compute node, I can submit functions to the GridEngine cluster and collect back those results to do further processing, in a fraction of the time that it would take, thus enabling me to do my science faster than if I did everything single core/single node.</p>
<p><strong>In this blog post, I'd like to share an annotated, minimal setup for using Dask on a GridEngine cluster.</strong> (Because we use Dask, more complicated pipelines are possible as well - I would encourage you to read the Dask docs for more complex examples.) I will assume that you are working in a Jupyter notebook environment, and that the notebook you are working out of is hosted on a GridEngine-connected compute node, from which you are able to <code>qsub</code> tasks. Don't worry, you won't be qsub-ing anything though!</p>
<h2 id="setup">Setup</h2><p>To start, we need a cell that houses the following code block:</p>
<div class="hll"><pre><span></span><span class="kn">from</span> <span class="nn">dask_jobqueue</span> <span class="kn">import</span> <span class="n">SGECluster</span>
<span class="kn">from</span> <span class="nn">dask</span> <span class="kn">import</span> <span class="n">delayed</span><span class="p">,</span> <span class="n">compute</span>

<span class="n">cluster</span> <span class="o">=</span> <span class="n">SGECluster</span><span class="p">(</span><span class="n">queue</span><span class="o">=</span><span class="s1">&#39;default.q&#39;</span><span class="p">,</span>
                     <span class="n">walltime</span><span class="o">=</span><span class="s2">&quot;1500000&quot;</span><span class="p">,</span>
                     <span class="n">processes</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                     <span class="n">memory</span><span class="o">=</span><span class="s1">&#39;1GB&#39;</span><span class="p">,</span>
                     <span class="n">cores</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                     <span class="n">env_extra</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;source /path/to/custom/script.sh&#39;</span><span class="p">,</span>
                                <span class="s1">&#39;export ENV_VARIABLE=&quot;SOMETHING&quot;&#39;</span><span class="p">]</span>
                       <span class="p">)</span>
</pre></div>
<p>Here, we are instantiating an <code>SGECluster</code> object under the variable name <code>cluster</code>. What <code>cluster</code> stores is essentially a configuration for a block of worker nodes that you will be requesting. Under the hood, what <code>dask-jobqueue</code> is doing is submitting jobs to the GridEngine scheduler, which will block off a specified amount of compute resources (e.g. number of cores, amount of RAM, whether you want GPUs or not, etc.) for a pre-specified amount of time, on which Dask then starts a worker process to communicate with the head process coordinating tasks amongst workers.</p>
<p>As such, you do need to know two pieces of information:</p>
<ol>
<li><code>queue</code>: The queue that jobs are to be submitted to. Usually, it is named something like <code>default.q</code>, but you will need to obtain this through GridEngine. If you have the ability to view all jobs that are running, you can call <code>qstat</code> at the command line to see what queues are being used. Otherwise, you might have to ping your system administrator for this information.</li>
<li><code>walltime</code>: You will also need to pre-estimate the wall clock time, in seconds, that you want the worker node to be alive for. It should be significantly longer than the expected time you think you will need, so that your function call doesn't timeout unexpectedly. I have defaulted to 1.5 million seconds, which is about 18 days of continual runtime. In practice, I usually kill those worker processes after just a few hours.</li>
</ol>
<p>Besides that, you also need to specify the resources that you need per worker process. In my example above, I'm asking for each worker process to use only 1GB of RAM, 1 core, and to use only 1 process per worker (i.e. no multiprocessing, I think).</p>
<p>Finally, I can also specify extra environment setups that I will need. Because each worker process is a new process that has no knowledge of the parent process' environment, you might have to source some bash script, or activate a Python environment, or export some environment variable. This can be done under the <code>env_extra</code> keyword, which accepts a list of strings.</p>
<h2 id="request-for-worker-compute-nodes">Request for worker compute "nodes"</h2><p>I put "nodes" in quotation marks, because they are effectively logical nodes, rather than actual compute nodes. (Technically, I think a compute node minimally means one physical hardware unit with CPUs and RAM).</p>
<p>In order to request for worker nodes to run your jobs, you need the next line of code:</p>
<div class="hll"><pre><span></span><span class="n">cluster</span><span class="o">.</span><span class="n">scale</span><span class="p">(</span><span class="mi">500</span><span class="p">)</span>
</pre></div>
<p>With this line, under the hood, <code>dask-jobqueue</code> will start submitting 500 jobs, each requesting 1GB of RAM and 1 core, populating my compute environment according to the instructions I provided under <code>env_extra</code>.</p>
<p>At the end of this, I effectively have a 500-node cluster on the larger GridEngine cluster (let's call this a "virtual cluster"), each with 1GB of RAM and 1 core available to it, on which I can submit functions to run.</p>
<h2 id="start-a-client-process">Start a client process</h2><p>In order to submit jobs to my virtual cluster, I have to instantiate a client that is connected to the cluster, and is responsible for sending functions there.</p>
<div class="hll"><pre><span></span><span class="kn">from</span> <span class="nn">dask.distributed</span> <span class="kn">import</span> <span class="n">Client</span>

<span class="n">client</span> <span class="o">=</span> <span class="n">Client</span><span class="p">(</span><span class="n">cluster</span><span class="p">)</span>
</pre></div>
<h2 id="compute">Compute!</h2><p>With this setup complete (I have it stored as a TextExpander snippets), we can now start submitting functions to the virtual cluster!</p>
<p>To simulate this, let's define a square-rooting function that takes 2-3 seconds to run each time it is called, and returns the square of its inputs. This simulates a function call that is computationally semi-expensive to run a few times, but because call on this hundreds of thousands of time, the total running time to run it serially would be too much.</p>
<div class="hll"><pre><span></span><span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">sleep</span>
<span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">sqrt</span>
<span class="kn">from</span> <span class="nn">random</span> <span class="kn">import</span> <span class="n">random</span>
<span class="k">def</span> <span class="nf">slow_sqrt</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Simulates the run time needed for a semi-expensive function call.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="c1"># define sleeping time in seconds, between 2-3 seconds.</span>
    <span class="n">t</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">random</span><span class="p">()</span>
    <span class="n">sleep</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
<h3 id="serial-execution">Serial Execution</h3><p>In a naive, serial setting, we would call on the function in a for-loop:</p>
<div class="hll"><pre><span></span><span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>
    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">slow_sqrt</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
</pre></div>
<p>This would take us anywhere between 20,000 to 30,000 seconds (approximately 8 hours, basically).</p>
<h3 id="parallel-execution">Parallel Execution</h3><p>In order to execute this in parallel instead, we could do one of the following three ways:</p>
<h4 id="map">map</h4><div class="hll"><pre><span></span><span class="n">sq_roots</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">slow_sqrt</span><span class="p">,</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">))</span>
<span class="n">sq_roots</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">sq_roots</span><span class="p">)</span>
</pre></div>
<h4 id="for-loop">for-loop</h4><div class="hll"><pre><span></span><span class="n">sq_roots</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>
    <span class="n">sq_roots</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">client</span><span class="o">.</span><span class="n">submit</span><span class="p">(</span><span class="n">slow_sqrt</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">restart</span><span class="o">=</span><span class="mi">20</span><span class="p">))</span>  <span class="c1"># submit the function as first argument, then rest of arguments</span>
<span class="n">sq_roots</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">sq_roots</span><span class="p">)</span>
</pre></div>
<h4 id="delayed">delayed</h4><div class="hll"><pre><span></span><span class="n">sq_roots</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>
    <span class="n">sq_roots</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">delayed</span><span class="p">(</span><span class="n">slow_sqrt</span><span class="p">)(</span><span class="n">i</span><span class="p">))</span>
<span class="n">sq_roots</span> <span class="o">=</span> <span class="n">compute</span><span class="p">(</span><span class="o">*</span><span class="n">sq_roots</span><span class="p">)</span>
</pre></div>
<p>I have some comments on each of the three methods, each of which I have used.</p>
<p>First off, <strong>each of them do require us to change the code that we would have written in serial</strong>. This little bit of overhead is the only tradeoff we really need to make in order to gain parallelism.</p>
<p>In terms of <strong>readability</strong>, all of them are quite readable, though in my case, I tend to favour the for-loop with <code>client.submit</code>. Here is why.</p>
<p>For readability, the for-loop explicitly indicates that we are looping over something. It's probably more easy for novices to approach my code that way.</p>
<p>For <strong>debuggability</strong>, <code>client.submit</code> returns a Futures object (same goes for <code>client.map</code>). A "Futures" object might be confusing at first, so let me start by demystifying that. A Futures object promises that the result that is computed from <code>slow_sqrt</code> will exist, and actually contains a ton of diagnostic information, including the <code>type</code> of the object (which can be useful for diagnosing whether my function actually ran correctly). In addition to that, I can call on <code>Futures.result()</code> to inspect the actual result (in this case, <code>sq_roots[0].result()</code>). This is good for debugging the function call, in case there are issues when scaling up. (At work, I was pinging a database in parallel, and sometimes the ping would fail; debugging led me to include some failsafe code, including retries and sleeps with random lengths to stagger out database calls.)</p>
<p>Finally, <strong>the Futures interface is non-blocking</strong> on my Jupyter notebook session. Once I've submitted the jobs, I can continue with other development work in my notebook in later cells, and check back when the Dask dashboard indicates that the jobs are done.</p>
<p>That said, I like the <code>delayed</code> interface as well. Once I was done debugging and confident that my own data pipeline at work wouldn't encounter the failure modes I was seeing, I switched over to the <code>delayed</code> interface and scaled up my analysis. I was willing to trade in the interactivity using the <code>Futures</code> interface for the automation provided by the <code>delayed</code> interface. (I also first used Dask on a single node through the delayed interface as well).</p>
<p>Of course, there's something also to be said for the simplicity of two lines of code for parallelism (with the <code>client.map</code> example).</p>
<p>The final line in each of the code blocks allows us to "gather" the results back into my coordinator node's memory, thus completing the function call and giving us the result we needed.</p>
<h2 id="conclusions">Conclusions</h2><p>That concludes it! The two key ideas illustrated in this blog post were:</p>
<ol>
<li>To set up a virtual cluster on a GridEngine system, we essentially harness the existing job submission system to generate workers that listen for tasks.</li>
<li>A useful programming pattern is to <code>submit</code> functions using the <code>client</code> object using <code>client.submit(func, *args, **kwargs)</code>. This requires minimal changes from serial code.</li>
</ol>
<h2 id="practical-tips">Practical Tips</h2><p>Here's some tips for doing parallel processing, which I've learned over the years.</p>
<p>Firstly, never prematurely parallelize. It's as bad as prematurely optimizing code. If your code is running slowly, check first to make sure that there aren't algorithmic complexity issues, or bandwidths being clogged up (e.g. I/O bound). As the Dask docs state, it is easier to achieve those gains first before doing parallelization.</p>
<p>Secondly, when developing parallel workflows, make sure to test the pipeline on subsets of input data first, and slowly scale up. It is during this period that you can also profile memory usage to check to see if you need to request for more RAM per worker.</p>
<p>Thirdly, for GridEngine clusters, it is usually easier to request for many small worker nodes that consume few cores and small amounts of RAM. If your job is trivially parallelizable, this may be a good thing.</p>
<p>Fourthly, it's useful to have realistic expectations on the kinds of speed-ups you can expect to gain. At work, through some ad-hoc profiling, I quickly came to the realization that concurrent database pings were the most likely bottleneck in my code's speed, and that nothing apart from increasing the number of concurrent database pings allowed would make my parallel code go faster.</p>
<p>Finally, on a shared cluster, be respectful of others' usage. Don't request for unreasonable amounts of compute time. And when you're confirmed done with your analysis work, remember to shut down the virtual cluster! :)</p>

  </div>

    <p><i>Did you enjoy this blog post? <a href="../../../blog/2018/10/11/parallel-processing-with-dask-on-gridengine-clusters/#disqus">Let's discuss more</a>!</i></p>
    <hr class="fancy">
  
    
  <!-- Import stylesheet -->
  <link rel="stylesheet" href="../../../static/pygments.css">
  <div>

  <!-- Set title style -->
  
    <h1><a href="../../../blog/2018/9/4/optimizing-block-sparse-matrix-creation-with-python/">Optimizing Block Sparse Matrix Creation with Python</a></h1>
  

  <!-- Append author -->
  <p class="meta">
    written by
    
      <a href="https://twitter.com/ericmjl">Eric J. Ma</a>
    
    on 2018-09-04
  
  </p>
  <!-- Append tags after author -->
  <p>
      <i class="fas fa-tag"></i>
      <a href="../../../blog/tag/graph/">
          graph
      </a>
      <i class="fas fa-tag"></i>
      <a href="../../../blog/tag/optimization/">
          optimization
      </a>
      <i class="fas fa-tag"></i>
      <a href="../../../blog/tag/numba/">
          numba
      </a>
      <i class="fas fa-tag"></i>
      <a href="../../../blog/tag/python/">
          python
      </a>
      <i class="fas fa-tag"></i>
      <a href="../../../blog/tag/data science/">
          data science
      </a>
      <i class="fas fa-tag"></i>
      <a href="../../../blog/tag/sparse matrix/">
          sparse matrix
      </a>
      
  </p>
  

  <!-- Put post body -->
  <div class="hll"><pre><span></span><span class="kn">import</span> <span class="nn">networkx</span> <span class="kn">as</span> <span class="nn">nx</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">scipy.sparse</span> <span class="kn">as</span> <span class="nn">sp</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="kn">as</span> <span class="nn">sns</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span>
<span class="kn">from</span> <span class="nn">numba</span> <span class="kn">import</span> <span class="n">jit</span>

<span class="n">sns</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="s1">&#39;talk&#39;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_style</span><span class="p">(</span><span class="s1">&#39;white&#39;</span><span class="p">)</span>

<span class="o">%</span><span class="n">load_ext</span> <span class="n">autoreload</span>
<span class="o">%</span><span class="n">autoreload</span> <span class="mi">2</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="o">%</span><span class="n">config</span> <span class="n">InlineBackend</span><span class="o">.</span><span class="n">figure_format</span> <span class="o">=</span> <span class="s1">&#39;retina&#39;</span>
</pre></div>
<h1 id="introduction">Introduction</h1><p>At work, I recently encountered a neat problem. I'd like to share it with you all.</p>
<p>One of my projects involves graphs; specifically, it involves taking individual graphs and turning them into one big graph. If you've taken my Network Analysis Made Simple workshops before, you'll have learned that graphs can be represented as a matrix, such as the one below:</p>
<div class="hll"><pre><span></span><span class="n">G</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">erdos_renyi_graph</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">to_numpy_array</span><span class="p">(</span><span class="n">G</span><span class="p">)</span>

<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</pre></div>
<p><img src="../../../blog/2018/9/4/optimizing-block-sparse-matrix-creation-with-python/output_2_1.png" alt="png"></p>
<p>Because the matrix is so sparse, we can actually store it as a <strong>sparse matrix</strong>:</p>
<div class="hll"><pre><span></span><span class="n">A_sparse</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">adjacency_matrix</span><span class="p">(</span><span class="n">G</span><span class="p">)</span><span class="o">.</span><span class="n">tocoo</span><span class="p">()</span>
<span class="p">[</span><span class="n">A_sparse</span><span class="o">.</span><span class="n">row</span><span class="p">,</span> <span class="n">A_sparse</span><span class="o">.</span><span class="n">col</span><span class="p">,</span> <span class="n">A_sparse</span><span class="o">.</span><span class="n">data</span><span class="p">]</span>
</pre></div>
<pre><code>[array([0, 0, 1, 2, 3, 3, 3, 4, 4, 5, 5, 5, 5, 6, 7, 7, 8, 8], dtype=int32),
 array([5, 7, 4, 7, 5, 6, 8, 1, 5, 0, 3, 4, 8, 3, 0, 2, 3, 5], dtype=int32),
 array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int64)]
</code></pre>
<p>The most straightforward way of storing sparse matrices is in the COO (COOrdinate) format, which is also known as the "triplet" format, or the "ijv" format. ("i" is row, "j" is col, "v" is value)</p>
<p>If we want to have two or more graphs stored together in a single matrix, which was what my projects required, then one way of representing them is as follows:</p>
<div class="hll"><pre><span></span><span class="n">G2</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">erdos_renyi_graph</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">A2</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">to_numpy_array</span><span class="p">(</span><span class="n">G2</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">sp</span><span class="o">.</span><span class="n">block_diag</span><span class="p">([</span><span class="n">A</span><span class="p">,</span> <span class="n">A2</span><span class="p">])</span><span class="o">.</span><span class="n">todense</span><span class="p">())</span>
</pre></div>
<p><img src="../../../blog/2018/9/4/optimizing-block-sparse-matrix-creation-with-python/output_6_1.png" alt="png"></p>
<p>Now, notice how there's 25 nodes in total (0 to 24), and that they form what we call a "block diagonal" format. In its "dense" form, we have to represent $25^2$ values inside the matrix. That's fine for small amounts of data, but if we have tens of thousands of graphs, that'll be impossible to deal with!</p>
<p>You'll notice I used a function from <code>scipy.sparse</code>, the <code>block_diag</code> function, which will create a block diagonal sparse matrix from an iterable of input matrices.</p>
<p><code>block_diag</code> is the function that I want to talk about in this post.</p>
<h1 id="profiling-block_diag-performance">Profiling <code>block_diag</code> performance</h1><p>I had noticed that when dealing with tens of thousands of graphs, <code>block_diag</code> was not performing up to scratch. Specifically, the time it needed would scale quadratically with the number of matrices provided.</p>
<p>Let's take a look at some simulated data to illustrate this.</p>
<div class="hll"><pre><span></span><span class="o">%%</span><span class="n">time</span>
<span class="n">Gs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">As</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3000</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">30</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="mf">0.2</span>
    <span class="n">G</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">erdos_renyi_graph</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">)</span>
    <span class="n">Gs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">G</span><span class="p">)</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">to_numpy_array</span><span class="p">(</span><span class="n">G</span><span class="p">)</span>
    <span class="n">As</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</pre></div>
<p>Let's now define a function to profile the code.</p>
<div class="hll"><pre><span></span><span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">time</span>
<span class="kn">from</span> <span class="nn">random</span> <span class="kn">import</span> <span class="n">sample</span>

<span class="k">def</span> <span class="nf">profile</span><span class="p">(</span><span class="n">func</span><span class="p">):</span>
    <span class="n">n_graphs</span> <span class="o">=</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">400</span><span class="p">,</span> <span class="mi">600</span><span class="p">,</span> <span class="mi">800</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">1200</span><span class="p">,</span> <span class="mi">1400</span><span class="p">,</span> <span class="mi">1600</span><span class="p">,</span> <span class="mi">1800</span><span class="p">,</span> <span class="mi">2000</span><span class="p">]</span>
    <span class="n">data</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">n_graphs</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>  <span class="c1"># 3 replicates per n</span>
            <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>
            <span class="n">func</span><span class="p">(</span><span class="n">sample</span><span class="p">(</span><span class="n">As</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
            <span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>
            <span class="n">data</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">data</span>

<span class="n">data_sp</span> <span class="o">=</span> <span class="n">profile</span><span class="p">(</span><span class="n">sp</span><span class="o">.</span><span class="n">block_diag</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="o">*</span><span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">data_sp</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;number of graphs&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;time (s)&#39;</span><span class="p">)</span>
</pre></div>
<p><img src="../../../blog/2018/9/4/optimizing-block-sparse-matrix-creation-with-python/output_12_1.png" alt="png"></p>
<p>It is quite clear that the increase in time is super-linear, showing $O(n^2)$ scaling. (Out of impatience, I did not go beyond 50,000 graphs in this post, but at work, I did profile performance up to that many graphs. For reference, it took about 5 minutes to finish creating the scipy sparse matrix for 50K graphs.</p>
<h1 id="optimizing-block_diag-performance">Optimizing <code>block_diag</code> performance</h1><p>I decided to take a stab at creating an optimized version of <code>block_diag</code>. Having profiled my code and discovering that sparse block diagonal matrix creation was a bottleneck, I implemented my own sparse block diagonal matrix creation routine using pure Python.</p>
<div class="hll"><pre><span></span><span class="k">def</span> <span class="nf">_block_diag</span><span class="p">(</span><span class="n">As</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">]):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return the (row, col, data) triplet for a block diagonal matrix.</span>

<span class="sd">    Intended to be put into a coo_matrix. Can be from scipy.sparse, but</span>
<span class="sd">    also can be cupy.sparse, or Torch sparse etc.</span>

<span class="sd">    Example usage:</span>

<span class="sd">    &gt;&gt;&gt; row, col, data = _block_diag(As)</span>
<span class="sd">    &gt;&gt;&gt; coo_matrix((data, (row, col)))</span>

<span class="sd">    :param As: A list of numpy arrays to create a block diagonal matrix.</span>
<span class="sd">    :returns: (row, col, data), each as lists.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">row</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">col</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">data</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">start_idx</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">A</span> <span class="ow">in</span> <span class="n">As</span><span class="p">:</span>
        <span class="n">nrows</span><span class="p">,</span> <span class="n">ncols</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span>
        <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nrows</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ncols</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">A</span><span class="p">[</span><span class="n">r</span><span class="p">,</span> <span class="n">c</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">row</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">r</span> <span class="o">+</span> <span class="n">start_idx</span><span class="p">)</span>
                    <span class="n">col</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">c</span> <span class="o">+</span> <span class="n">start_idx</span><span class="p">)</span>
                    <span class="n">data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">r</span><span class="p">,</span> <span class="n">c</span><span class="p">])</span>
        <span class="n">start_idx</span> <span class="o">=</span> <span class="n">start_idx</span> <span class="o">+</span> <span class="n">nrows</span>
    <span class="k">return</span> <span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">,</span> <span class="n">data</span>
</pre></div>
<p>Running it through the same profiling routine:</p>
<div class="hll"><pre><span></span><span class="n">data_custom</span> <span class="o">=</span> <span class="n">profile</span><span class="p">(</span><span class="n">_block_diag</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="o">*</span><span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">data_custom</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;custom&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="o">*</span><span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">data_sp</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;scipy.sparse&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;number of graphs&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;time (s)&#39;</span><span class="p">)</span>
</pre></div>
<p><img src="../../../blog/2018/9/4/optimizing-block-sparse-matrix-creation-with-python/output_18_1.png" alt="png"></p>
<p>I also happened to have listened in on a <a href="https://www.youtube.com/watch?v=6oXedk2tGfk">talk by Siu Kwan Lam</a> during lunch, on <code>numba</code>, the JIT optimizer that he has been developing for the past 5 years now. Seeing as how the code I had written in <code>_block_diag</code> was all numeric code, which is exactly what <code>numba</code> was designed for, I decided to try optimizing it with JIT.</p>
<div class="hll"><pre><span></span><span class="kn">from</span> <span class="nn">numba</span> <span class="kn">import</span> <span class="n">jit</span>

<span class="n">data_jit</span> <span class="o">=</span> <span class="n">profile</span><span class="p">(</span><span class="n">jit</span><span class="p">(</span><span class="n">_block_diag</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="o">*</span><span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">data_custom</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;custom&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="o">*</span><span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">data_sp</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;scipy.sparse&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="o">*</span><span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">data_jit</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;jit&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;number of graphs&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;time (s)&#39;</span><span class="p">)</span>
</pre></div>
<p><img src="../../../blog/2018/9/4/optimizing-block-sparse-matrix-creation-with-python/output_21_1.png" alt="png"></p>
<p>Notice the speed-up that JIT-ing the code provided! (Granted, that first run was a "warm-up" run; once JIT-compiled, everything is really fast!)</p>
<p>My custom implementation only returns the (row, col, data) triplet. This is an intentional design choice - having profiled the code with and without calling a COO matrix creation routine, I found the JIT-optimized performance to be significantly better without creating the COO matrix routine. As I still have to create a sparse matrix, I ended up with the following design:</p>
<div class="hll"><pre><span></span><span class="k">def</span> <span class="nf">block_diag</span><span class="p">(</span><span class="n">As</span><span class="p">):</span>
    <span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">jit</span><span class="p">(</span><span class="n">_block_diag</span><span class="p">)(</span><span class="n">As</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">sp</span><span class="o">.</span><span class="n">coo_matrix</span><span class="p">((</span><span class="n">data</span><span class="p">,</span> <span class="p">(</span><span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">)))</span>

<span class="n">data_wrap</span> <span class="o">=</span> <span class="n">profile</span><span class="p">(</span><span class="n">block_diag</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="o">*</span><span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">data_custom</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;custom&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="o">*</span><span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">data_sp</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;scipy.sparse&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="o">*</span><span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">data_jit</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;jit&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="o">*</span><span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">data_wrap</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;wrapped&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;number of graphs&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;time (s)&#39;</span><span class="p">)</span>
</pre></div>
<p><img src="../../../blog/2018/9/4/optimizing-block-sparse-matrix-creation-with-python/output_24_1.png" alt="png"></p>
<p>You'll notice that the array creation step induces a consistent overhead on top of the sparse matrix triplet creation routine, but stays flat and trends the "jit" dots quite consistently. It intersects the "custom" dots at about $10^3$ graphs. Given the problem that I've been tackling, which involves $10^4$ to $10^6$ graphs at a time, it is an absolutely worthwhile improvement to JIT-compile the <code>_block_diag</code> function.</p>
<h1 id="conclusion">Conclusion</h1><p>This was simultaneously a fun and useful exercise in optimizing my code!</p>
<p>A few things I would take away from this:</p>
<ul>
<li>Profiling code for bottlenecks can be really handy, and can be especially useful if we have a hypothesis on how to optimize it.</li>
<li><code>numba</code> can really speed up array-oriented Python computation. It lives up to the claims on its documentation.</li>
</ul>
<p>I hope you learned something new, and I hope you also enjoyed reading this post as much as I enjoyed writing it!</p>

  </div>

    <p><i>Did you enjoy this blog post? <a href="../../../blog/2018/9/4/optimizing-block-sparse-matrix-creation-with-python/#disqus">Let's discuss more</a>!</i></p>
    <hr class="fancy">
  

  
  <div class="pagination">
    
      <a href="../../../blog/page/3/">&laquo; Previous</a>
    
    | 4 |
    
      <a href="../../../blog/page/5/">Next &raquo;</a>
    
  </div>


    </div>

    <nav class="navbar navbar-expand-sm navbar-light fixed-bottom bg-light">
        <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#external-links" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="external-links">
            <ul class="navbar-nav mr-auto">
                
                <li class="nav-item">
                    <a class="nav-link" href="https://www.linkedin.com/in/ericmjl"><i class="fab fa-linkedin" aria-hidden="true"></i> LinkedIn</a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="http://twitter.com/ericmjl"><i class="fab fa-twitter" aria-hidden="true"></i> Twitter</a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="http://github.com/ericmjl"><i class="fab fa-github" aria-hidden="true"></i> GitHub</a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="http://stackoverflow.com/users/1274908/ericmjl"><i class="fab fa-stack-overflow" aria-hidden="true"></i> Stack Overflow</a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="http://shortwhale.com/ericmjl"><i class="far fa-envelope" aria-hidden="true"></i> Contact Me</a>
                </li>
                
            </ul>
        </div>
    </nav>
    <!-- Boostrap JS imports -->
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.1/js/bootstrap.min.js" integrity="sha384-smHYKdLADwkXOn1EmN1qk/HfnUcbVRZyYmZ4qpPea6sjB/pTJ0euyQp0Mk8ck+5T" crossorigin="anonymous"></script>
</body>
