

<!doctype html>

<head>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta charset="utf-8">
    <!-- Bootstrap v4beta Imports -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">
    <style media="screen">
        body {
            padding-top: 70px;
            padding-bottom: 70px;
        }
    </style>

    
    

    <!-- Google Analytics -->
    <script>
        (function(i, s, o, g, r, a, m) {
            i['GoogleAnalyticsObject'] = r;
            i[r] = i[r] || function() {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date();
            a = s.createElement(o),
                m = s.getElementsByTagName(o)[0];
            a.async = 1;
            a.src = g;
            m.parentNode.insertBefore(a, m)
        })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');

        ga('create', 'UA-12498603-2', 'auto');
        ga('send', 'pageview');
    </script>

    <!-- ClustrMaps Tracking -->
    <!-- <script type="text/javascript" id="clstr_globe" src="//cdn.clustrmaps.com/globe.js?d=nhKoDpoTjWz4pC6CwI-fSy4hPoJ1uXwTLCfMCT3OK_8"></script> -->

    <!-- FontAwesome embed -->
    <!-- <script src="https://use.fontawesome.com/cb9dbe8e41.js"></script> -->
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.1.1/css/all.css" integrity="sha384-O8whS3fhG2OnA5Kas0Y9l3cfpmYjapjI0E4theH4iuMD+pLhbf6JI0jIMfYcK3yZ" crossorigin="anonymous">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/static/css/custom.css">

    <!-- Mathjax -->
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
    </script>

    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>

</head>

<title>Blog - Eric J. Ma's Personal Site</title>
<body class="body">
    <nav class="navbar navbar-expand-sm navbar-dark fixed-top bg-dark">
        <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#local-links" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="local-links">
            <ul class="navbar-nav mr-auto">
                
                <li class="nav-item">
                    <a class="nav-link" href="/"><i class="fa fa-home" aria-hidden="true"></i> Home</a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="https://ericmjl.github.io/resume"><i class="fa far fa-file-alt" aria-hidden="true"></i> Resume</a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="/blog"><i class="fa fa-rss" aria-hidden="true"></i> Blog</a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="/open-source"><i class="fa fa-code" aria-hidden="true"></i> Open Source</a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="/projects"><i class="fa fa-briefcase" aria-hidden="true"></i> Projects</a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="/talks"><i class="fa fa-microphone" aria-hidden="true"></i> Talks</a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="/teaching"><i class="fa fa-university" aria-hidden="true"></i> Teaching</a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="/bio"><i class="fa fa-user" aria-hidden="true"></i> Bio</a>
                </li>
                
            </ul>
        </div>
    </nav>
    <div class="container">
        
  
    
  <!-- Import stylesheet -->
  <link rel="stylesheet" href="../../../static/pygments.css">
  <div>

  <!-- Set title style -->
  
    <h1><a href="../../../blog/2018/1/8/bayesian-uncertainty-a-more-nuanced-view/">Bayesian Uncertainty: A More Nuanced View</a></h1>
  

  <!-- Append author -->
  <p class="meta">
    written by
    
      <a href="https://twitter.com/ericmjl">Eric J. Ma</a>
    
    on 2018-01-08
  

  <!-- Put post body -->
  <p>The following thought hit my mind just last night.</p>
<p>Bayesian inference requires the computation of uncertainty. Computing that uncertainty is computationally expensive compared to simply computing point estimates/summary statistics. But when exactly is uncertainty useful, and more importantly, actionable? That's something I've not really appreciated in the past. It's probably not productive to be dogmatic about always computing uncertainty if that uncertainty is not actionable.</p>

  </div>

    <p><i>Did you enjoy this blog post? <a href="../../../blog/2018/1/8/bayesian-uncertainty-a-more-nuanced-view/#disqus">Let's discuss more</a>!</i></p>
    <hr class="fancy">
  
    
  <!-- Import stylesheet -->
  <link rel="stylesheet" href="../../../static/pygments.css">
  <div>

  <!-- Set title style -->
  
    <h1><a href="../../../blog/2017/12/13/visual-studio-code-a-new-microsoft/">Visual Studio Code: A New Microsoft?</a></h1>
  

  <!-- Append author -->
  <p class="meta">
    written by
    
      <a href="https://twitter.com/ericmjl">Eric J. Ma</a>
    
    on 2017-12-13
  

  <!-- Put post body -->
  <p>During my week attending PyData NYC 2017, which was effectively a mini-mini-sabbatical from work, I got a chance to try out Visual Studio Code. Part of it was curiosity, having seen so many PyData participants using it; part of it was because of Steve Dowell, a core CPython contributor who works at Microsoft, who mentioned about the Python-friendly tools they added into VSCode.</p>
<p>I think VSCode is representative of a new Microsoft.</p>
<p>But first, let me describe what using it is like.</p>
<h2 id="user-interface">User Interface</h2><p>First off, the UI is beautiful. It's impossible to repeat enough how important the UI is. With minimal configuration, I made it basically match Atom's UI, which I had grown used to. It has an integrated terminal, and the colours are... wow. That shade of green, blue and red are amazing, ever just so slightly muted compared to the Terminal or iTerm. The background shade of black matches well with the rest of VSCode, and the colour scheme is changeable to match that of Atom's. The design feels... just right. Wow!</p>
<h2 id="git-integration">Git Integration</h2><p>Secondly, the integration with Git rivals Atom; in fact, there's a one-click "sync" button! It also has nice <code>git commit -am</code> analog where I can add and commit all of the files simultaneously.</p>
<h2 id="intellisense">Intellisense</h2><p>Thirdly, IntelliSense is just amazing! I like how I can use it to look up a function signature just by mousing over the function name.</p>
<h2 id="open-source">Open Source</h2><p>Finally, it’s fully open source and back able, in the same vein as Atom, minus the bloat that comes from building on top of electron. Impressive stuff!</p>
<h2 id="other-thoughts">Other Thoughts</h2><p>Now, on the new Microsoft.</p>
<p>Only at the recent PyData NYC did I learn that Microsoft has hired almost half of the core CPython developers! Not only that, they are encouraged to continue their contributions into the CPython code base. In my view, that’s a pretty awesome development! It means the Python programming language will continue to have a strong corporate backing while also enjoying community support. Its a sign of a healthy ecosystem, IMO, and also a sign of Microsoft’s support for Open Source Software!</p>
<p>I’m more and more impressed by what Microsoft is doing for the Open Source community. I’m hoping they’ll continue up with this!!</p>

  </div>

    <p><i>Did you enjoy this blog post? <a href="../../../blog/2017/12/13/visual-studio-code-a-new-microsoft/#disqus">Let's discuss more</a>!</i></p>
    <hr class="fancy">
  
    
  <!-- Import stylesheet -->
  <link rel="stylesheet" href="../../../static/pygments.css">
  <div>

  <!-- Set title style -->
  
    <h1><a href="../../../blog/2017/11/30/pydata-nyc-2017-recap/">PyData NYC 2017 Recap</a></h1>
  

  <!-- Append author -->
  <p class="meta">
    written by
    
      <a href="https://twitter.com/ericmjl">Eric J. Ma</a>
    
    on 2017-11-30
  
  </p>
  <!-- Append tags after author -->
  <p>
      <i class="fas fa-tag"></i>
      <a href="../../../blog/tag/pydata/">
          pydata
      </a>
      <i class="fas fa-tag"></i>
      <a href="../../../blog/tag/conferences/">
          conferences
      </a>
      <i class="fas fa-tag"></i>
      <a href="../../../blog/tag/python/">
          python
      </a>
      
  </p>
  

  <!-- Put post body -->
  <p>With that, we’ve finished PyData NYC! Here's some of my highlights of the conference.</p>
<h2 id="keynotes">Keynotes</h2><p>There were three keynotes, one each by Kerstin Kleese van Dam, Thomas Sargent, and Andrew Gelman. Interestingly enough, they didn't do what I would expect most academics to do -- give talks highlighting the accomplishments of their research groups. Rather, Kerstin gave a talk that highlighted the use of PyData tools at Brookhaven National Labs. Thomas Sargent gave a philosophical talk on what economic models really are (they're "games", in a mathematical sense), and I took back the importance of being able to implement models, otherwise, "you're just bull*****ing".</p>
<p>Andrew Gelman surprised me the most - he gave a wide-ranging talk about the problems we have in statistical analysis workflows. He emphasized that "robustness checks" are basically scams, because they're basically methods whose purpose is reassurance. He had a really cool example that highlighted that we need to understand our models by modifying our models, perhaps even using a graph of models to identify perturbations to our model that will help us understand our model. He also peppered his talk with anecdotes about how he made mistakes in his analysis workflows. I took home a different philosophy of data analysis: when we evaluate how "good" a model is, the operative question is, "compared against what?"</p>
<h2 id="talks">Talks</h2><p>The talks were, for me, the highlight of the conference. A lot of good learning material around. Here's the talks from which I learned actionable new material.</p>
<h3 id="analyzing-nba-foul-calls-using-python">Analyzing NBA Foul Calls using Python</h3><p>This talk by the prolific PyMC blogger Austin Rochford is one that I really enjoyed. The take-home that I got from him was towards the end of his talk, in which I picked up three ways to diagnose probabilistic programming models.</p>
<p>The first was the use of residuals - which I now know can be used for classification problems as well as regression problems.</p>
<p>The second was the use of the energy plots in PyMC3, where if the "energy transition" and "marginal energy distribution" plots match up (especially in the tails), then we know that the NUTS sampler did a great job.</p>
<p>The third was the use of the Gelman-Rubin statistic to measure the in-chain vs. between-chain variation; measures close to 1 are generally considered good.</p>
<p>Check out the talk slides <a href="http://austinrochford.com/resources/talks/nba-fouls-pydata-nyc-2017.slides.html#/">here</a>.</p>
<h3 id="scikit-learn-compatible-model-stacking"><code>scikit-learn</code>-compatible model stacking</h3><p>This talk was a great one because it shows how to use model stacking (also known as "ensembling", a technique commonly used in Kaggle competitions) to enable better predictions.</p>
<p>Conceptually, model stacking works like this: I train a set of model individually on a problem, and use the predictions from those models as features for a meta-model. The meta-model should perform, at worst, on par with the best model inside the ensemble, but may also perform better. This idea was first explored in Polley and van der Laan's work, available <a href="http://biostats.bepress.com/ucbbiostat/paper266">online</a>.</p>
<p>Civis Analytics has released their implementation of model stacking in their <a href="https://github.com/civisanalytics/civisml-extensions">GitHub repository</a>, and it's available on PyPI. The best part of it? They didn't try inventing a new API, they kept a <code>scikit-learn</code>-compatible API. Kudos to them!</p>
<p>Check out the repository <a href="https://github.com/civisanalytics/civisml-extensions">here</a>.</p>
<h3 id="stream-processing-with-dask">Stream Processing with Dask</h3><p>Matthew Rocklin, as usual, gave an entertaining and informative talk on the use of Streamz, a lightweight library he built, to explore the use of Dask for streaming applications. The examples he gave were amazing showcases of the library's capabilities. Given the right project, I'd love to try this out!</p>
<p>Check out his slides <a href="http://matthewrocklin.com/slides/pydata-nyc-2017.html#/">here</a>.</p>
<h3 id="asynchronous-python:-a-gentle-introduction">Asynchronous Python: A Gentle Introduction</h3><p>This talk, delivered by James Cropcho, defined what asynchronous programming was all about. For me, things finally clicked at the end when I asked him for an example of how asynchronous programming would be done in data analytics workflows -- to which he responded, "If you're querying for data and then performing a calculation, then async is a good idea."</p>
<p>The idea behind this is as such: web queries written serially are often "blocking", meaning we can't do anything while we wait for the web query to return. If we want to do a calculation on the returned data point, we have to wait for it to return first. On the other hand, if written asynchronously, we could potentially do a calculation on the previous data point while waiting for the current result to return, shaving the total time off potentially by some considerable fraction.</p>
<h3 id="turning-pymc3-into-scikit-learn">Turning PyMC3 into <code>scikit-learn</code></h3><p>This talk was by Nicole Carlson, and she did a tremendously great job delivering this talk. In it, she walked through how to wrap a PyMC3 model inside a <code>scikit-learn</code> estimator, including details on how to implement the <code>.fit()</code>, <code>.predict()</code>, and <code>.predict_proba()</code> methods. The code in <a href="https://github.com/parsing-science/ps-toolkit/blob/master/ps_toolkit/pymc3_models/HLR.py">her repository</a> provides a great base example to copy from.</p>
<p>One thing I can't emphasize enough is that from a user experience standpoint, it's super important to follow idioms that people are used to. What Nicole did in this talk is to show how we can provide such idioms to end-users, rather than inventing a slightly modified wheel. Props to her for that!</p>
<p>Her slides are online <a href="../../../blog/2017/11/30/pydata-nyc-2017-recap/">here</a>.</p>
<h3 id="an-attempt-at-demystifying-bayesian-deep-learning">An Attempt at Demystifying Bayesian Deep Learning</h3><p>This talk was my own, put at the end of the 2nd day. The title definitely contributed to the hype. I popped into the room early, but then left for the restroom. When I got back, there was a lineup in the front door and in the back door. Totally unexpected. That said, big credit to the Boston Bayesians organizers Jordi and Colin, who let me do the talk as a rehearsal for PyData, so I felt very grounded.</p>
<p>The talk went mostly smoothly. I think I was channeling my colleague, Brant Peterson, with his sense of humour during that time. There was one really hilarious hiccup - right after mentioning that I wouldn't overdo the "math" and "equations", I accidentally opened an adjacent tab with an alternate version of the slides... with, surprise surprise, a math equation on it! During the Q&amp;A, when I shared the point of not needing to do train/test splits in Bayesian analysis, I could sense the jaws dropping and eyes widening in disbelief; more than just a handful of people came up and asked for the reference later on.</p>
<p>Having done the talk, I now realize how much people will appreciate a lighthearted and lightweight introduction to a topic that's very dense and filled with jargon. Conference speakers, we need to do more of this!</p>
<p>From an emotional standpoint, many people brought me joy with their positive comments on the visuals and structure of the talk. Others put out positive comments on Twitter, which I collected together in a <a href="https://twitter.com/i/moments/924970479869448193?ref_src=twsrc%5Etfw">Twitter Moment</a>. It was very encouraging, especially on this deep learning journey that I'm on right now.</p>
<h2 id="tutorials">Tutorials</h2><h3 id="interactive-matplotlib-figures">Interactive <code>matplotlib</code> Figures</h3><p>This was something I totally didn't realize was possible before - we can create interactive <code>matplotlib</code> figures very easily! I have cloned the repository, and I think it'll be neat to hack on some projects at work to use this.</p>
<p>The tutorial repository can be found <a href="https://github.com/tacaswell/interactive_mpl_tutorial">here</a>.</p>
<h3 id="linear-regression-three-ways">Linear Regression Three Ways</h3><p>This one was by Colin Carroll, a software engineer at the MIT Media Lab (previously at Kensho). I sat in and learned a good deal of math from him. One thing new I learned was how we can specify a model without requiring the use of observed variables. Any sampling we do will take into account the hierarchical and mathematical relationships we've done. This makes it neat to implement Bayesian nets to test how things will look under different scenarios!</p>
<p>His tutorial repository can be found <a href="https://github.com/ColCarroll/pydata_nyc2017">here</a>.</p>
<h3 id="top-to-bottom-line-by-line">Top-To-Bottom, Line-By-Line</h3><p>This one was led by the ever-entertaining, ever-surprising James Powell. I wish the tutorial was recorded, because even though this is a "novice" tutorial, it nonetheless was still an eye-opening talk. Anybody who thinks they know Python should go listen to James' talks, whenever he gives them live - it's bound to be entertaining and eye-opening!</p>
<h2 id="overall-thoughts">Overall Thoughts</h2><p>I'm glad I made it to PyData NYC 2017 this year. Made many new friends and connections, and caught up with old friends in the PyData community. As always, learned a ton as well!</p>

  </div>

    <p><i>Did you enjoy this blog post? <a href="../../../blog/2017/11/30/pydata-nyc-2017-recap/#disqus">Let's discuss more</a>!</i></p>
    <hr class="fancy">
  
    
  <!-- Import stylesheet -->
  <link rel="stylesheet" href="../../../static/pygments.css">
  <div>

  <!-- Set title style -->
  
    <h1><a href="../../../blog/2017/11/16/bayesian-learning-and-overfitting/">Bayesian Learning and Overfitting</a></h1>
  

  <!-- Append author -->
  <p class="meta">
    written by
    
      <a href="https://twitter.com/ericmjl">Eric J. Ma</a>
    
    on 2017-11-16
  

  <!-- Put post body -->
  <p>Yesterday, after I did my Boston Bayesians dry run talk, there was a point raised that I had only heard of once before: Bayesian learning methods don't overfit. Which means we're allowed to use all the data on hand. The point holds for simple Bayesian networks, and for more complicated deep neural nets.</p>
<p>Though I believe it, I wasn't 100% convinced of this myself, so I decided to check it up. I managed to get my hands on Radford Neal's book, Bayesian Learning for Neural Networks, and found the following quotable paragraphs:</p>
<blockquote class="blockquote blockquote-success">
<p>It is a common belief, however, that restricting the complexity of the models used for such tasks is a good thing, not just because of the obvious computational savings from using a simple model, but also because it is felt that too complex a model will overfit the training data, and perform poorly when applied to new cases. This belief is certainly justified if the model parameters are estimated by maximum likelihood. I will argue here that concern about overfitting is not a good reason to limit complexity in a Bayesian context.</p>
</blockquote><p>A few paragraphs later, after explaining the frequentist procedure:</p>
<blockquote class="blockquote">
<p>From a Bayesian perspective, adjusting the complexity of the model based on the amount of training data makes no sense. A Bayesian defines a model, selects a prior, collects data, computes the posterior, and then makes predictions. There is no provision in the Bayesian framework for changing the model or the prior depending on how much data was collected. If the model and prior are correct for a thousand observations, they are correct for ten observations as well (though the impact of using an incorrect prior might be more serious with fewer observations). In practice, we might sometimes switch to a simpler model if it turns out that we have little data, and we feel that we will consequently derive little benefit from using a complex, computationally expensive model, but this would be a concession to practicality, rather than a theoretically desirable procedure.</p>
</blockquote><p>Finally, in the following section after describing how neural networks are built:</p>
<blockquote class="blockquote">
<p>In a Bayesian model of this type, the role of the hyperparameters controlling the priors for weights is roughly analogous to the role of a weight decay constant in conventional training. With Bayesian training, values for these hyperparameters (more precisely, a distribution of values) can be found without the need for a validation set.</p>
</blockquote><p>This seems to dovetail well with the following convoluted intuition that I've had: <strong>if I fit a Bayesian model on the "training" set of the data, then update it with the "test" set, it's equivalent to just training with the whole dataset. With wide priors, if I fit with a smaller dataset, my posterior distribution will be wider than if I fit with the entire dataset. So... where possible, just train with the entire dataset.</strong> That said, I've not had sufficient grounding in Bayesian stats (after all, still a newcomer) to justify this.</p>
<p>I certainly have more reading/learning to do here. Looks like something neat to explore in the short-term.</p>

  </div>

    <p><i>Did you enjoy this blog post? <a href="../../../blog/2017/11/16/bayesian-learning-and-overfitting/#disqus">Let's discuss more</a>!</i></p>
    <hr class="fancy">
  
    
  <!-- Import stylesheet -->
  <link rel="stylesheet" href="../../../static/pygments.css">
  <div>

  <!-- Set title style -->
  
    <h1><a href="../../../blog/2017/11/14/the-value-of-thinking-simply/">The Value of Thinking Simply</a></h1>
  

  <!-- Append author -->
  <p class="meta">
    written by
    
      <a href="https://twitter.com/ericmjl">Eric J. Ma</a>
    
    on 2017-11-14
  

  <!-- Put post body -->
  <p>Einstein has a famous quote that most people don't hear about.</p>
<blockquote class="blockquote">
<p class="mb-0">It can scarcely be denied that the supreme goal of all theory is to make the irreducible basic elements as simple and as few as possible without having to surrender the adequate representation of a single datum of experience.</p>
<footer class="blockquote-footer">Albert Einstein</footer>
</blockquote><p>It instead, most people hear the misquote:</p>
<blockquote class="blockquote text">
<p>Everything should be made as simple as possible, but no simpler. </p>
<footer class="blockquote-footer">Misquoted version</footer>
</blockquote><p>Though a misquote, it's still a fair (though lopsided -- missing a sufficient translation of the latter half) simplification of the original.</p>
<p>In my work, I'm reminded of this point. I can choose to go for the complex fancy thing, but if I don't start from first principles, or start with simplistic approximations, I will struggle to have a sufficiently firm grasp on a problem to start tackling it. And therein lies the key, I think, in making progress on creative, intellectual work.</p>
<p>The past week, I've noticed myself not wasting time on mindless coding (which usually amounts to re-running code with tweaks), and instead devoting more time to strategic thinking. As an activity, strategic thinking isn't just sitting there and thinking. For me, it involves writing and re-writing what I'm thinking, drawing and re-drawing what I'm seeing, and arranging and composing the pieces that are floating in my mind. During that time of writing, drawing, arranging and composing, I'm questioning myself, "What if I didn't have this piece?". Soon enough, the "simplest complex version" (SCV) of whatever I'm working on begins to emerge -- but it never really is the final version! I go back and prototype it in code, and then get stuck on something, and realize I left something out in that SCV, and re-draw the entire SCV from scratch.</p>
<p>Here's my misquote, then, offered up:</p>
<blockquote class="blockquote">
<p>Sufficiently simple, and only necessarily complex.</p>
<footer class="blockquote-footer">A further mutated version.</footer>
</blockquote>
  </div>

    <p><i>Did you enjoy this blog post? <a href="../../../blog/2017/11/14/the-value-of-thinking-simply/#disqus">Let's discuss more</a>!</i></p>
    <hr class="fancy">
  

  
  <div class="pagination">
    
      <a href="../../../blog/page/13/">&laquo; Previous</a>
    
    | 14 |
    
      <a href="../../../blog/page/15/">Next &raquo;</a>
    
  </div>


    </div>

    <nav class="navbar navbar-expand-sm navbar-dark fixed-bottom bg-dark">
        <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#external-links" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="external-links">
            <ul class="navbar-nav mr-auto">
                
                <li class="nav-item">
                    <a class="nav-link" href="https://www.linkedin.com/in/ericmjl"><i class="fab fa-linkedin" aria-hidden="true"></i> LinkedIn</a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="http://twitter.com/ericmjl"><i class="fab fa-twitter" aria-hidden="true"></i> Twitter</a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="http://octodon.social/ericmjl"><i class="fab fa-mastodon" aria-hidden="true"></i> Octodon</a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="http://github.com/ericmjl"><i class="fab fa-github" aria-hidden="true"></i> GitHub</a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="http://stackoverflow.com/users/1274908/ericmjl"><i class="fab fa-stack-overflow" aria-hidden="true"></i> Stack Overflow</a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="http://shortwhale.com/ericmjl"><i class="far fa-envelope" aria-hidden="true"></i> Contact Me</a>
                </li>
                
            </ul>
        </div>
    </nav>
    <!-- Boostrap JS imports -->
    <script src="https://code.jquery.com/jquery-3.4.1.slim.min.js" integrity="sha384-J6qa4849blE2+poT4WnyKhv5vZF5SrPo0iEjwBvKU7imGFAV0wwj1yYfoRSJoZ+n" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>
</body>
