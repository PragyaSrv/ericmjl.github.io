<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title type="text">Eric Ma's Blog</title>
  <id>urn:uuid:8e5496e4-8606-3632-a35c-1d9694b4313d</id>
  <updated>2019-03-24T00:00:00Z</updated>
  <link href="http://www.ericmjl.com/blog/" />
  <link href="http://www.ericmjl.com/blog.xml" rel="self" />
  <author>
    <name></name>
  </author>
  <generator uri="https://github.com/ajdavis/lektor-atom" version="0.3">Lektor Atom Plugin</generator>
  <entry xml:base="http://www.ericmjl.com/blog/2019/3/24/variance-explained/">
    <title type="text">Variance Explained</title>
    <id>urn:uuid:0732917e-8efd-3932-8fad-e75d4d69dd52</id>
    <updated>2019-03-24T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/3/24/variance-explained/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;Variance explained, as a regression quality metric, is one that I have begun to like a lot, especially when used in place of a metric like the correlation coefficient (r&lt;sup&gt;2&lt;/sup&gt;).&lt;/p&gt;
&lt;p&gt;Here's variance explained defined:&lt;/p&gt;
&lt;p&gt;
$$1 - \frac{var(y_{true} - y_{pred})}{var(y_{true})}$$
&lt;/p&gt;&lt;p&gt;Why do I like it? It’s because this metric gives us a measure of the scale of the error in predictions relative to the scale of the data.&lt;/p&gt;
&lt;p&gt;The numerator in the fraction calculates the variance in the errors, in other words, the &lt;em&gt;scale of the errors&lt;/em&gt;. The denominator in the fraction calculates the variance in the data, in other words, the &lt;em&gt;scale of the data&lt;/em&gt;. By subtracting the fraction from 1, we get a number upper-bounded at 1 (best case), and unbounded towards negative infinity.&lt;/p&gt;
&lt;p&gt;Here's a few interesting scenarios.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If the scale of the errors is small relative to the scale of the data, then variance explained will be close to 1.&lt;/li&gt;
&lt;li&gt;If the scale of the errors is about the same scale as the data, then the variance explained will be around 0. This essentially says our model is garbage.&lt;/li&gt;
&lt;li&gt;If the scale of the errors is greater than the scale of the data, then the variance explained will be negative. This is also an indication of a garbage model.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A thing that is &lt;em&gt;really nice&lt;/em&gt; about variance explained is that it can be used to compare related machine learning tasks that have different unit scales, for which we want to compare how good one model performs across all of the tasks. Mean squared error makes this an apples-to-oranges comparison, because the unit scales of each machine learning task is different. On the other hand, variance explained is unit-less.&lt;/p&gt;
&lt;p&gt;Now, we know that single metrics can have failure points, as does the coefficient of correlation r^2^, as shown in Ansecombe's quartet and the Datasaurus Dozen:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://d2f99xq7vri1nk.cloudfront.net/Anscombe_1_0_0.png&quot; alt=&quot;Ansecombe&amp;#39;s quartet, taken from Autodesk Research&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Fig. 1: Ansecombe's quartet, taken from Autodesk Research&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://blog.revolutionanalytics.com/downloads/DataSaurus%20Dozen.gif&quot; alt=&quot;Datasaurus Dozen, taken from Revolution Analytics&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Fig. 2: Datasaurus Dozen, taken from Revolution Analytics&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;One place where the variance explained can fail is if the predictions are systematically shifted off from the true values. Let's say prediction was shifted off by 2 units.&lt;/p&gt;
&lt;p&gt;
$$var(y_{true} - y_{pred}) = var([2, 2, ..., 2]) = 0$$
&lt;/p&gt;&lt;p&gt;There's no variance in errors, even though they are systematically shifted off from the true prediction. Like r&lt;sup&gt;2&lt;/sup&gt;, variance explained will fail here.&lt;/p&gt;
&lt;p&gt;As usual, &lt;a href=&quot;https://en.wikipedia.org/wiki/Anscombe%27s_quartet&quot;&gt;Ansecombe's quartet&lt;/a&gt;, as does &lt;a href=&quot;https://www.autodeskresearch.com/publications/samestats&quot;&gt;The Datasaurus Dozen&lt;/a&gt;, gives us a pertinent reminder that visually inspecting your model predictions is always a good thing!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/3/22/functools-partial/">
    <title type="text">Functools Partial</title>
    <id>urn:uuid:7059f3fb-181e-305d-840a-bf1c34b42e8b</id>
    <updated>2019-03-22T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/3/22/functools-partial/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;If you’ve done Python programming for a while, I think it pays off to know some little tricks that can improve the readability of your code and decrease the amount of repetition that goes on.&lt;/p&gt;
&lt;p&gt;One such tool is &lt;code&gt;functools.partial&lt;/code&gt;. It took me a few years after my first introduction to &lt;code&gt;partial&lt;/code&gt; before I finally understood why it was such a powerful tool.&lt;/p&gt;
&lt;p&gt;Essentially, what &lt;code&gt;partial&lt;/code&gt; does is it wraps a function and sets a keyword argument to a constant. That’s it. What do we mean?&lt;/p&gt;
&lt;p&gt;Here’s a minimal example. Let’s say we have a function &lt;code&gt;f&lt;/code&gt;, not written by me, but provided by someone else.&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# do something with a and b.&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In my code, let’s say that I know that the value that &lt;code&gt;b&lt;/code&gt; takes on in my app is always the tuple &lt;code&gt;(1, 'A')&lt;/code&gt;. I now have a few options. The most obvious is assign the tuple &lt;code&gt;(1, 'A')&lt;/code&gt; to a variable, and pass that in on every function call:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;A&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# do some stuff.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# do more stuff.&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# ad nauseum&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# set value of N&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;resultN&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The other way I could do it is use &lt;code&gt;functools.partial&lt;/code&gt; and just set the keyword argument &lt;code&gt;b&lt;/code&gt; to equal to the tuple directly.&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;functools&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;partial&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;f_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;partial&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;A&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, I can repeat the code above, but now only worrying about the keyword argument &lt;code&gt;a&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;result1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# do some stuff.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# do more stuff.&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# ad nauseum&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# set value of N&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;resultN&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And there you go, that’s basically how &lt;code&gt;functools.partial&lt;/code&gt; works in a nutshell.&lt;/p&gt;
&lt;p&gt;Now, where have I used this in real life?&lt;/p&gt;
&lt;p&gt;The most common place I have used it is in Flask. I have built Flask apps where I need to dynamically keep my Bokeh version synced up between the Python and JS libraries that get called. To ensure that my HTML templates have a consistent Bokeh version, I use the following pattern:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;bokeh&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__version__&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bkversion&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;flask&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;render_template&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Flask&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;functools&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;partial&lt;/span&gt; 

&lt;span class=&quot;n&quot;&gt;render_template&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;partial&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;render_template&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bkversion&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bkversion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Flask app boilerplate&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;app&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Flask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;vm&quot;&gt;__name__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;nd&quot;&gt;@app.route&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;/&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;home&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;render_template&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;index.html.j2&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, because I always have &lt;code&gt;bkversion&lt;/code&gt; pre-specified in &lt;code&gt;render_template&lt;/code&gt;, I never have to repeat it over every &lt;code&gt;render_template&lt;/code&gt; function call.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/3/20/how-i-work/">
    <title type="text">How I Work</title>
    <id>urn:uuid:ecb1119f-e770-31bd-bddd-4da7130485e2</id>
    <updated>2019-03-20T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/3/20/how-i-work/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;I was inspired to write this because of Will Wolf’s interview with DeepLearning.AI, in which I found a ton of similarities between how both of us work. As such, I thought I’d write down what I use at work to get things done.&lt;/p&gt;
&lt;h2 id=&quot;tooling&quot;&gt;Tooling&lt;/h2&gt;&lt;p&gt;For a data scientist, I think tooling is of very high importance: mastery over our tools keeps us productive. Here’s a sampling of what I use at work:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Compute: I have my own MacBook, but I prefer freeloading off my colleague’s workstation, which is connected to our HPC compute cluster, allowing me to do parallelization with Dask!&lt;/li&gt;
&lt;li&gt;Editors/IDEs: VSCode + Jupyter Lab (JLab). Lots of plugins for VSCode!&lt;/li&gt;
&lt;li&gt;Terminal: iTerm, with my &lt;a href=&quot;https://github.com/ericmjl/dotfiles&quot;&gt;&lt;code&gt;dotfiles&lt;/code&gt;&lt;/a&gt; providing a high degree of customization. I also use the VSCode and JLab terminals where convenient.&lt;/li&gt;
&lt;li&gt;General Purpose: Python, Dask, git&lt;/li&gt;
&lt;li&gt;ML/Stats: &lt;code&gt;scikit-learn&lt;/code&gt;, &lt;code&gt;jax&lt;/code&gt;, &lt;code&gt;pymc3&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Data wrangling: &lt;code&gt;pandas&lt;/code&gt;, &lt;code&gt;pyjanitor&lt;/code&gt; (a package I wrote to provide convenience APIs for data cleaning)&lt;/li&gt;
&lt;li&gt;Data visualization: &lt;code&gt;matplotlib&lt;/code&gt;, &lt;code&gt;seaborn&lt;/code&gt;, &lt;code&gt;holoviews&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;App development: &lt;code&gt;flask&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As you probably can see, I’m a very Python-centric person!&lt;/p&gt;
&lt;h2 id=&quot;daily/weekly-routines&quot;&gt;Daily/Weekly Routines&lt;/h2&gt;&lt;p&gt;Most of my work necessitates long stretches of thinking and hacking time. Without that, I’m unable to get into “the zone” to do anything productive. Hence, I have a habit of packing meetings onto Mondays (a.k.a. “Meeting Mondays”). Backup times for meetings, which I prefer to not do, are 11 am and 1 pm, bookending lunch time so that I don’t end up with a fragmented morning/afternoon. The only exceptions I make are for my two high-priority team meetings, for which I defer to the rest of the team. I’m glad that my managers understand the need for long stretches of hacking time, and have stuck to Monday one-on-one meetings.&lt;/p&gt;
&lt;p&gt;Hence, almost every day from Tuesday through to Friday, I have long stretches of pre-allocated time for hacking. It’s data science scheduling bliss! It also means I turn down a lot of “can I meet you to chat” invites - unless we can pack them on Monday!&lt;/p&gt;
&lt;p&gt;On Friday, I make a point to try to work remotely. It helps with sanity, particularly in the winter, when the commute gets harsh and I can’t bike. Fridays also are the days on which I try to do my open source work.&lt;/p&gt;
&lt;h2 id=&quot;pair-coding&quot;&gt;Pair Coding&lt;/h2&gt;&lt;p&gt;Pair coding with others on mutual projects has been a very productive endeavor, &lt;a href=&quot;https://ericmjl.github.io/blog/2019/3/1/pair-coding-why-and-how-for-data-scientists/&quot;&gt;which I have written about before&lt;/a&gt;. Unlike weekly update meetings, I plan for pair coding on an as-needed basis. We have a pre-defined goal for what we want to accomplish, including a conceivably achievable goal and a stretch goal; achieving the easier one keeps us motivated. It follows the “no agenda, no meeting” rule of thumb by which I protect my time.&lt;/p&gt;
&lt;p&gt;I found that a good setup is really necessary for pair coding to be successful. A minimum is a dual-monitor setup, with one extra keyboard + mouse for my coding partner.&lt;/p&gt;
&lt;p&gt;One thing I didn’t mention in my previous blog post was how knowledge transfer happens. Here’s how I think it works. We have one in the “driver’s seat”, and the other in the observer role. Knowledge transfer generally happens from the more experienced person to the less experienced one, and the driver doesn’t necessarily have to be the more experienced one. For example, when pair coding with my intern, I play the role of observer and may dictate code or outline what needs to be done, but I don’t actively take over on my keyboard unless there’s a situation that shows up that is irrelevant to the coding session goals. On the other hand, if there’s a codebase I’ve developed for which I need to play the tour guide role, I will be in the driver’s seat, while the observer will help me catch peripheral errors that I’m making.&lt;/p&gt;
&lt;h2 id=&quot;learning-new-things&quot;&gt;Learning New Things&lt;/h2&gt;&lt;p&gt;Pair coding has been one way I learn new things. For example, with my colleague Zach as the observer, we hacked together a simple dashboard project using Flask, Holoviews and Panel.&lt;/p&gt;
&lt;p&gt;I’m not very mathematically-savvy, in that algebra is difficult for me to follow. (I’m mildly algebra-blind, but getting better now.) Ironically, code, which is algebraic in nature too, but works with plain English names, works much better for me. Implementing algorithms and statistical methods using &lt;code&gt;jax&lt;/code&gt; (for things that involve differential computing) and &lt;code&gt;PyMC3&lt;/code&gt; (for all things Bayesian) has served to be very educational. While implementing, I also impose some software abstractions on the math, and this also forces me to organize my knowledge, which also helps learning. Implementing things on the computer is also the perfect way to learn by teaching: The computer is the ultimately dumb student, as it will execute exactly as you tell it, mistakes included!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/3/1/pair-coding-why-and-how-for-data-scientists/">
    <title type="text">Pair Coding: Why and How for Data Scientists</title>
    <id>urn:uuid:639416f0-072e-3e1a-bb09-6bbec1ce20e0</id>
    <updated>2019-03-01T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/3/1/pair-coding-why-and-how-for-data-scientists/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;&lt;p&gt;While at work, I've been experimenting with pair coding with other data science-oriented colleagues. My experiences tell me that this is something extremely valuable to do. I'd like to share here the &quot;why&quot; and the &quot;how&quot; on pair coding, but focused towards data scientists.&lt;/p&gt;
&lt;h2 id=&quot;what-is-pair-coding?&quot;&gt;What is pair coding?&lt;/h2&gt;&lt;p&gt;Pair coding is a form of programming where two people work together on a single code base together. It usually involves one person on the keyboard and another talking through the problem and observing for issues, such as syntax, logic, or code style. Occasionally, they may swap who is on the keyboard. In other words, one is the &quot;creator&quot;, and the other is the &quot;critic&quot; (but in a positive, constructive fashion).&lt;/p&gt;
&lt;h2 id=&quot;what-s-your-history-with-pair-coding?&quot;&gt;What's your history with pair coding?&lt;/h2&gt;&lt;p&gt;I was inspired by a few places. Firstly, there are a wealth of blog posts detailing the potential benefits and pitfalls of pair coding, in a software developer's context. (A quick Google search will lead you to them.) Secondly, I had, at work, experimented with &quot;pair hacking&quot; sessions, which involved more than coding, including white-boarding a problem to get a feel for its scope, and it turned out to be pretty productive. Thirdly, I was inspired by &lt;a href=&quot;https://www.newyorker.com/magazine/2018/12/10/the-friendship-that-made-google-huge&quot;&gt;a New Yorker article on Jeff and Sanjay&lt;/a&gt;, in which part of it chronicled how they worked as a pair to solve the toughest problems at Google.&lt;/p&gt;
&lt;p&gt;Now, because I'm not a software engineer by training, and because don't have extensive experience beforehand, and because there are no data-science-oriented resources for pair coding that I have read before (I'd love to read them if you know of any!), I've had to be adapt what I read for software development to a data science context.&lt;/p&gt;
&lt;h2 id=&quot;what-are-the-potential-benefits-of-pair-coding?&quot;&gt;What are the potential benefits of pair coding?&lt;/h2&gt;&lt;p&gt;I can see at least the following benefits, if not more that I have yet to discover:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Instant peer review over data science logic and code. Because we are talking through a problem while coding it up, we can instantly check whether our logic is correct against each other.&lt;/li&gt;
&lt;li&gt;Knowledge transfer. In my experience, I've had productive pair-coding sessions with another colleague who has a better grasp of the project than I do. Hence, I contribute &amp;amp; teach the technical component, while I also learn the broader project context better.&lt;/li&gt;
&lt;li&gt;Building trust. We all know that the more closely you work with someone, the more rough corners get rubbed off.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;what-pre-requisites-do-you-see-for-a-productive-pair-programming-session?&quot;&gt;What pre-requisites do you see for a productive pair programming session?&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;A long, continuous, and uninterrupted time slot (at least 2-3 hours in length) to maintain continuity.&lt;/li&gt;
&lt;li&gt;A defined goal or question that we are seeking to answer - keeps us focused on what needs to be done.&lt;/li&gt;
&lt;li&gt;That goal should also be plausibly achievable within the 2-3 hour timeframe. &lt;/li&gt;
&lt;li&gt;Large monitors for both parties to look at, or a code-sharing platform where both can see the code without needing to physically huddle.&lt;/li&gt;
&lt;li&gt;A place where we can talk without feeling hindered.&lt;/li&gt;
&lt;li&gt;No impromptu interruptions from other individuals.&lt;/li&gt;
&lt;li&gt;Complementary and intersecting skillsets.&lt;/li&gt;
&lt;li&gt;Open-minded individuals who are willing to learn. (Ego-free.)&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;where-does-pair-coding-differ-for-data-scientists-vs.-software-engineers?&quot;&gt;Where does pair coding differ for data scientists vs. software engineers?&lt;/h2&gt;&lt;p&gt;I think the differences at best are subtle, not necessarily overt.&lt;/p&gt;
&lt;p&gt;The biggest difference that I can think of might be in clarity. To the best of my knowledge, software engineers work with pretty well-defined requirements. The only hiccups that I can imagine that may occur are in unforeseen logic/code blockers. Data scientists, on the other hand, often are exploring and defining the requirements as things go along. In other words, we are working with more unknowns than a software engineer might.&lt;/p&gt;
&lt;p&gt;An example is a model I built with a colleague at work that involved groups of groups of samples. We weren't able to envision the final model right at the beginning, and code towards it. Rather, we built the model iteratively, starting with highly simplifying assumptions, discussing which ones to refine, and iteratively building the model as we went forward.&lt;/p&gt;
&lt;p&gt;Perhaps a related difference is that as data scientists, because of potentially greater uncertainty surrounding the final product, we may end up talking more about project direction than one would as a software engineer. But that's probably just a minor detail.&lt;/p&gt;
&lt;h2 id=&quot;do-you-have-any-memorable-quotes-from-the-new-yorker-article?&quot;&gt;Do you have any memorable quotes from the New Yorker article?&lt;/h2&gt;&lt;p&gt;Yes, a number of them.&lt;/p&gt;
&lt;p&gt;One on scaling things up.&lt;/p&gt;
&lt;blockquote class=&quot;blockquote&quot;&gt;
&lt;p&gt;Alan Eustace became the head of the engineering team after Rosing left, in 2005. “To solve problems at scale, paradoxically, you have to know the smallest details,” Eustace said.&lt;/p&gt;
&lt;/blockquote&gt;&lt;p&gt;Another on pair programming as an uncommon practice:&lt;/p&gt;
&lt;blockquote class=&quot;blockquote&quot;&gt;
&lt;p&gt;“I don’t know why more people don’t do it,” Sanjay said, of programming with a partner.&lt;/p&gt;
&lt;p&gt;“You need to find someone that you’re gonna pair-program with who’s compatible with your way of thinking, so that the two of you together are a complementary force,” Jeff said.&lt;/p&gt;
&lt;/blockquote&gt;</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/1/28/minimum-viable-products-matter/">
    <title type="text">Minimum Viable Products (MVPs) Matter</title>
    <id>urn:uuid:e7106802-1560-3f7e-a492-9b3e5bd3d957</id>
    <updated>2019-01-28T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/1/28/minimum-viable-products-matter/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;MVPs matter because they afford us at least two things:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Psychological safety&lt;/li&gt;
&lt;li&gt;Credibility&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Psychological safety comes from knowing that we have at least a working prototype that we can deliver to whomever is going to consume our results. We aren't stuck in the land of imaginary ideas without something tangible for others to interact with.&lt;/p&gt;
&lt;p&gt;Credibility comes about because with the MVP on hand, others now can trust on our ability to execute on an idea. Prior to that, all that others have to go off are promises of &quot;a thing&quot;.&lt;/p&gt;
&lt;p&gt;Build your MVPs. They're a good thing!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/1/21/advi-scalable-bayesian-inference/">
    <title type="text">ADVI: Scalable Bayesian Inference</title>
    <id>urn:uuid:c601f17e-84cb-3984-8342-27dd19372045</id>
    <updated>2019-01-21T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/1/21/advi-scalable-bayesian-inference/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;&lt;p&gt;You never know when scalability becomes an issue. Indeed, scalability necessitates a whole different world of tooling.&lt;/p&gt;
&lt;p&gt;While at work, I've been playing with a model - a Bayesian hierarchical 4-parameter dose response model, to be specific. With this model, the overall goal (without going into proprietary specifics) was parameter learning - what's the 50th percentile concentration, what's the max, what's the minimum, etc.; what was also important was quantifying the uncertainty surrounding these parameters.&lt;/p&gt;
&lt;h2 id=&quot;prototype-phase&quot;&gt;Prototype Phase&lt;/h2&gt;&lt;p&gt;Originally, when I prototyped the model, I used just a few thousand samples, which was trivial to fit with NUTS. I also got the model specification (both the group-level and population-level priors) done using those same few thousand.&lt;/p&gt;
&lt;p&gt;At some point, I was qualitatively and quantitatively comfortable with the model specification. Qualitatively - the model structure reflected prior biochemical knowledge. Quantitatively, I saw good convergence when examining the sampling traces, as well as the shrinkage phenomena.&lt;/p&gt;
&lt;h2 id=&quot;scaling-up&quot;&gt;Scaling Up&lt;/h2&gt;&lt;p&gt;Once I reached that point, I decided to scale up to the entire dataset: 400K+ samples, 3000+ groups.&lt;/p&gt;
&lt;p&gt;Fitting this model with NUTS with the full dataset would have taken a week, with no stopping time guarantees at the end of an overnight run - when I left the day before, I was still hoping for 5 days. However, switching over to ADVI (automatic differentiation variational inference) was a key enabler for this model: I was able to finish fitting the model with ADVI in just 2.5 hours, with similar uncertainty estimates (it'll never end up being identical, given random sampling).&lt;/p&gt;
&lt;h2 id=&quot;thoughts&quot;&gt;Thoughts&lt;/h2&gt;&lt;p&gt;I used to not appreciate that ADVI could be useful for simpler models; in the past, I used to think that ADVI was mainly useful in Bayesian neural network applications - in other words, with large parameter and large data models.&lt;/p&gt;
&lt;p&gt;With this example, I'm definitely more informed about what &quot;scale&quot; can mean: both in terms of number of parameters in a model, and in terms of number of samples that the model is fitted on. In this particular example, the model is simple, but the number of samples is so large that ADVI becomes a feasible alternative to NUTS MCMC sampling.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2018/12/25/conda-hacks-for-data-science-efficiency/">
    <title type="text">Conda hacks for data science efficiency</title>
    <id>urn:uuid:251beb77-b7ce-3834-b046-ca9701afdffb</id>
    <updated>2018-12-25T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2018/12/25/conda-hacks-for-data-science-efficiency/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;The &lt;code&gt;conda&lt;/code&gt; package manager has, over the years, become an integral part of my workflow. I use it to manage project environments, and have built a bunch of very simple hacks around it that you can adopt too. I'd like to share them with you, alongside the rationale for using them.&lt;/p&gt;
&lt;h2 id=&quot;hack-1:-set-up-your-.condarc&quot;&gt;Hack #1: Set up your &lt;code&gt;.condarc&lt;/code&gt;&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Why?&lt;/strong&gt; It will save you a few keystrokes each time you want to do something with &lt;code&gt;conda&lt;/code&gt;. For example, in my &lt;code&gt;.condarc&lt;/code&gt;, I have the following:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Set the channels that the `conda install` command will &lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# automatically search through.&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;defaults&lt;/span&gt;
  &lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;conda-forge&lt;/span&gt;
  &lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;ericmjl&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Always accept installation. Is convenient, but always &lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# double-check!&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;always_yes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;true&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For more information on how to configure your &lt;code&gt;.condarc&lt;/code&gt;, check the &lt;a href=&quot;https://conda.io/docs/user-guide/configuration/use-condarc.html&quot;&gt;online documentation&lt;/a&gt;!&lt;/p&gt;
&lt;h2 id=&quot;hack-2:-use-one-environment-spec-file-per-project&quot;&gt;Hack #2: Use one environment spec file per project&lt;/h2&gt;&lt;p&gt;This assumes that you have the habit of putting all files related to one project inside one folder, using subdirectories for finer-grained organization.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Why?&lt;/strong&gt; It will ensure that you have one version-controlled, authoritative specification for the packages that are associated with the project. This is good for (1) reproducibility, as you can send it to a colleague and have them reproduce the environment, and (2) will enable Hack #3, which I will showcase afterwards.&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# file name: environment.yml&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Give your project an informative name&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;project-name&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Specify the conda channels that you wish to grab packages from, in order of priority.&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;defaults&lt;/span&gt;
&lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;conda-forge&lt;/span&gt;
&lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;ericmjl&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Specify the packages that you would like to install inside your environment. Version numbers are allowed, and conda will automatically use its dependency solver to ensure that all packages work with one another.&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;dependencies&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;python=3.7&lt;/span&gt;
&lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;conda&lt;/span&gt;
&lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;jupyterlab&lt;/span&gt;
&lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;scipy&lt;/span&gt;
&lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;numpy&lt;/span&gt;
&lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;pandas&lt;/span&gt;
&lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;pyjanitor&lt;/span&gt;
&lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;pandas&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# There are some packages which are not conda-installable. You can put the pip dependencies here instead.&lt;/span&gt;
&lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;pip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;tqdm&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# for example only, tqdm is actually available by conda.&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;A hack that I have related to this is that I use TextExpander shortcut to populate a starting environment spec file.&lt;/p&gt;
&lt;p&gt;Additionally, if I want to install a new package, rather than simply typing &lt;code&gt;conda install &amp;lt;packagename&amp;gt;&lt;/code&gt;, I will add the package to the environment spec file, and then type &lt;code&gt;conda env update -f environment.yml&lt;/code&gt;, as more often than not, my default is to continue using the package I added.&lt;/p&gt;
&lt;p&gt;For more details on what the environment spec file is all about, &lt;a href=&quot;https://conda.io/docs/user-guide/tasks/manage-environments.html#create-env-file-manually&quot;&gt;read the online docs&lt;/a&gt;!&lt;/p&gt;
&lt;h2 id=&quot;hack-3:-use-conda-auto-env&quot;&gt;Hack 3: use &lt;code&gt;conda-auto-env&lt;/code&gt;&lt;/h2&gt;&lt;p&gt;Written by &lt;a href=&quot;https://chdoig.github.io&quot;&gt;Christine Doig&lt;/a&gt;, &lt;a href=&quot;https://github.com/chdoig/conda-auto-env&quot;&gt;&lt;code&gt;conda-auto-env&lt;/code&gt;&lt;/a&gt; is a bash hack that enables you to automatically activate an environment once you enter into a project directory, &lt;em&gt;as long as an &lt;code&gt;environment.yml&lt;/code&gt; file already exists in the directory&lt;/em&gt;. If the environment does not already exist, then &lt;code&gt;conda-auto-env&lt;/code&gt; will automatically create one based on the &lt;code&gt;environment.yml&lt;/code&gt; file in your project directory.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Why?&lt;/strong&gt; If you have many projects that you are working on, then it will greatly reduce the amount of effort used to remember which project to activate.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;conda-auto-env&lt;/code&gt; looks like this:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# File: .conda-auto-env&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#!/bin/bash&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;function&lt;/span&gt; conda_auto_env&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt; -e &lt;span class=&quot;s2&quot;&gt;&amp;quot;environment.yml&amp;quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;then&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# echo &amp;quot;environment.yml file found&amp;quot;&lt;/span&gt;
    &lt;span class=&quot;nv&quot;&gt;ENV&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;$(&lt;/span&gt;head -n &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt; environment.yml &lt;span class=&quot;p&quot;&gt;|&lt;/span&gt; cut -f2 -d &lt;span class=&quot;s1&quot;&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Check if you are already in the environment&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;[[&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$PATH&lt;/span&gt; !&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; *&lt;span class=&quot;nv&quot;&gt;$ENV&lt;/span&gt;* &lt;span class=&quot;o&quot;&gt;]]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;then&lt;/span&gt;
      &lt;span class=&quot;c1&quot;&gt;# Check if the environment exists&lt;/span&gt;
      conda activate &lt;span class=&quot;nv&quot;&gt;$ENV&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$?&lt;/span&gt; -eq &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;then&lt;/span&gt;
        :
      &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# Create the environment and activate&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;Conda env &amp;#39;&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ENV&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;#39; doesn&amp;#39;t exist.&amp;quot;&lt;/span&gt;
        conda env create -q
        conda activate &lt;span class=&quot;nv&quot;&gt;$ENV&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;fi&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;fi&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;fi&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;export&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;PROMPT_COMMAND&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;conda_auto_env
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To use it, you have two options. You can either copy/paste the whole original script into your &lt;code&gt;.bashrc&lt;/code&gt;, or you can put it in a file called &lt;code&gt;.conda-auto-env&lt;/code&gt;, and source it from your &lt;code&gt;.bashrc&lt;/code&gt;. I recommend the latter, as it makes managing your &lt;code&gt;.bashrc&lt;/code&gt; easier:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# File: .bashrc&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;source&lt;/span&gt; /path/to/.conda-auto-env
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&quot;hack-4:-hijack-bash-aliases-for-conda-commands&quot;&gt;Hack 4: hijack bash aliases for &lt;code&gt;conda&lt;/code&gt; commands&lt;/h2&gt;&lt;p&gt;I use aliases to save myself a few keystrokes whenever I'm at the terminal. This is a generalizable bash hack, but here it is as applied to &lt;code&gt;conda&lt;/code&gt; commands.&lt;/p&gt;
&lt;p&gt;Anyways, these are the commands that I use most often, which I have found it useful to alias:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# File: .aliases&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;alias&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;ceu&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;conda env update&amp;quot;&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;alias&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;cl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;conda list&amp;quot;&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;alias&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;ci&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;conda install&amp;quot;&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;alias&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;cr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;conda remove&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Make sure your aliases don't clash with existing commands that you use!&lt;/p&gt;
&lt;p&gt;Then, source &lt;code&gt;.aliases&lt;/code&gt; in your &lt;code&gt;. bashrc&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# File: .bashrc&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;source&lt;/span&gt; /path/to/.aliases
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, all of your defined aliases will be available in your bash shell.&lt;/p&gt;
&lt;p&gt;The idea/pattern, as I mentioned earlier, is generalizable beyond just bash commands. (I have &lt;code&gt;ls&lt;/code&gt; aliased for &lt;code&gt;exa&lt;/code&gt;, and &lt;code&gt;l&lt;/code&gt; aliased for &lt;code&gt;ls&lt;/code&gt; - the epitome of laziness!)&lt;/p&gt;
&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;&lt;p&gt;I hope you found these bash and &lt;code&gt;conda&lt;/code&gt; hacks to be useful. Hopefully they will help you become more productive and efficient!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2018/12/16/gaussian-process-notes/">
    <title type="text">Gaussian Process Notes</title>
    <id>urn:uuid:66321576-e328-3d24-b3a2-c90b3fa831fe</id>
    <updated>2018-12-16T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2018/12/16/gaussian-process-notes/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;I first learned GPs about two years back, and have been fascinated by the idea. I learned it through a video by David MacKay, and managed to grok it enough that I could put it to use in simple settings. That was reflected in my &lt;a href=&quot;https://fluforecaster.herokuapp.com&quot;&gt;Flu Forecaster project&lt;/a&gt;, in which my GPs were trained only on individual latent spaces.&lt;/p&gt;
&lt;p&gt;Recently, though, I decided to seriously sit down and try to grok the math behind GPs (and other machine learning models). To do so, I worked through &lt;a href=&quot;https://youtu.be/4vGiHC35j9s&quot;&gt;Nando de Freitas' YouTube videos on GPs&lt;/a&gt;. (Super thankful that he has opted to put these videos up online!)&lt;/p&gt;
&lt;p&gt;The product of this learning is two-fold. Firstly, I have added a &lt;a href=&quot;https://github.com/ericmjl/bayesian-analysis-recipes/blob/master/notebooks/gp-test.ipynb&quot;&gt;GP notebook&lt;/a&gt; to my &lt;a href=&quot;https://github.com/ericmjl/bayesian-analysis-recipes&quot;&gt;Bayesian analysis recipes&lt;/a&gt; repository.&lt;/p&gt;
&lt;p&gt;Secondly, I have also put together some hand-written notes on GPs. (For those who are curious, I first hand-wrote them on paper, then copied them into my iPad mini using a Wacom stylus. We don't have the budget at the moment for an iPad Pro!) They can be downloaded &lt;a href=&quot;../../../../../blog/2018/12/16/gaussian-process-notes/gaussian-processes.pdf&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Some lessons learned:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Algebra is indeed a technology of sorts (to quote Jeremy Kun's book). Being less sloppy than I used to be gives me the opportunity to connect ideas on the page to ideas in my head, and express them more succinctly.&lt;/li&gt;
&lt;li&gt;Grokking the math behind GPs at the minimum requires one thing: remembering, or else knowing how to derive, the formula for how to get the distribution parameters of a multivariate Gaussian conditioned on some of of its variables.&lt;/li&gt;
&lt;li&gt;Once I grokked the math, implementing a GP using only NumPy was trivial; also, extending it to higher dimensions was similarly trivial!&lt;/li&gt;
&lt;/ul&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2018/12/9/mathematical-intuition/">
    <title type="text">Mathematical Intuition</title>
    <id>urn:uuid:4ae0c241-c33a-3013-8d4c-baf759793236</id>
    <updated>2018-12-09T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2018/12/9/mathematical-intuition/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;Last week, I picked up Jeremy Kun's book, &quot;A Programmer's Introduction to Mathematics&quot;. In it, I finally found an explanation for my frustrations when reading math papers:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;What programmers would consider “sloppy” notation is one symptom of the problem, but there there are other expectations on the reader that, for better or worse, decelerate the pace of reading. Unfortunately I have no solution here. Part of the power and expressiveness of mathematics is the ability for its practitioners to overload, redefine, and omit in a suggestive manner. Mathematicians also have thousands of years of “legacy” math that require backward compatibility. Enforcing a single specification for all of mathematics—a suggestion I frequently hear from software engineers—would be horrendously counterproductive.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Reading just &lt;em&gt;that&lt;/em&gt; paragraph explained, in such a lucid manner, how my frustrations reading mathematically-oriented papers, stemmed from mismatched expectations. I come into a paper thinking like a software engineer. Descriptive variable names (as encouraged by Python), which are standardized as well, with structured abstractions providing a hierarchy of logic between chunks of code... No, mathematicians are more like Shakespeare - or perhaps linguists - in that they will take a symbol and imbibe it with a subtly new meaning or interpretation inspired by a new field. That &quot;L&quot; you see in one field of math doesn't always exactly mean the same thing in another field.&lt;/p&gt;
&lt;h2 id=&quot;biology-vs.-math?&quot;&gt;Biology vs. Math?&lt;/h2&gt;&lt;p&gt;The contrast is stark when compared against reading a biology paper. With a biology paper, if you know the key wet-bench experiment types (and there's not that many), you can essentially get the gist of a paper by reading the abstract and dissecting the figures, which, granted, are described and labelled with field-specific jargon, but are at least descriptive names. With a math-oriented paper, the equations are the star, and one has to really grok each element of the equations to know what they mean. It means taking the time to dissect each equation and ask what each symbol is, what each group of symbols means, and how those underlying ideas connect with one another and with other ideas. It's not unlike a biology paper, but requiring a different kind of patience, one that I wasn't trained in.&lt;/p&gt;
&lt;h2 id=&quot;learning-to-learn-by-teaching&quot;&gt;Learning to Learn by Teaching&lt;/h2&gt;&lt;p&gt;As Jeremy Kun wrote in his book, programmers do have some sort of a leg-up when it comes to reading and understanding math. It's a bit more than what Kun wrote, I think - yes, many programming ideas have deep mathematical connections. But I think there's more.&lt;/p&gt;
&lt;p&gt;One thing we know from research into how people learn is that teaching someone something is an incredible way to learn that something. From my prior experience, the less background a student has in a material, the more demands are placed on the teacher's understanding of the material, as we work out how the multiple representations in our head to try to communicate it to them.&lt;/p&gt;
&lt;p&gt;As it turns out, we programmers have the ultimate dumb &quot;student&quot; available at our fingertips: Our computers! By implementing mathematical ideas in code, we are essentially &quot;teaching&quot; the computer to do something mathematical. Computers are not smart; they are programmed to do exactly what we input to them. If we get an idea wrong, our implementation of the math will likely be wrong. That fundamental law of computing shows up again: Garbage in, garbage out.&lt;/p&gt;
&lt;h2 id=&quot;hierarchy-of-ideas&quot;&gt;Hierarchy of Ideas&lt;/h2&gt;&lt;p&gt;More than just that, when we programmers implement a mathematical idea in code, we can start putting our &quot;good software engineering&quot; ideas into place! It helps the math become stickier when we can see, through code, the hierarchy of concepts that are involved.&lt;/p&gt;
&lt;p&gt;An example, for me, comes from the deep learning world. I had an attempt dissecting two math-y deep learning papers last week. Skimming through the papers didn't do much good for my understanding of the paper. Neither did trying to read the paper like I do a biology paper. Sure, I could perhaps just read the ideas that the authors were describing in prose, but I had no intuition on which to base a proper critique of the idea's usefulness. It took implementing those papers in Python code, writing tests for them, and using abstractions that I had previously written, to come to a place where I felt like the ideas in the paper were a flexibly wieldable tool in my toolkit.&lt;/p&gt;
&lt;p&gt;Reinventing the wheel, such that we can learn the wheel, can in fact help us decompose the wheel so that we can do other new things with it. Human creativity is such a wonderful thing!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2018/11/13/solving-problems-actionably/">
    <title type="text">Solving Problems Actionably</title>
    <id>urn:uuid:47287890-e99d-3c82-b19f-7d2ddd0735ac</id>
    <updated>2018-11-13T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2018/11/13/solving-problems-actionably/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;There's a quote by John Tukey that has been a recurrent theme at work.&lt;/p&gt;
&lt;blockquote class=&quot;blockquote&quot;&gt;
&lt;p&gt;It's better to solve the right problem approximately than to solve the wrong problem exactly.&lt;/p&gt;
&lt;/blockquote&gt;&lt;p&gt;Continuing on the theme of quoting two Georges:&lt;/p&gt;
&lt;blockquote class=&quot;blockquote&quot;&gt; 
&lt;p&gt;All models are wrong, but some are more useful than others.&lt;/p&gt;
&lt;/blockquote&gt;&lt;p&gt;H/T Allen Downey for pointing out that our minds think alike.&lt;/p&gt;
&lt;p&gt;I have been working on a modelling effort for colleagues at work. There were two curves involved, and the second depended on the first one.&lt;/p&gt;
&lt;p&gt;In both cases, I started with a simple model, and made judgment calls along the way as to whether to continue improving the model, or to stop there because the current iteration of the model was sufficient enough to act on. With first curve, the first model was actionable for me. With the second curve, the first model I wrote clearly wasn't good enough to be actionable, so I spent lots more rounds of iteration on it.&lt;/p&gt;
&lt;p&gt;But wait, how does one determine &quot;actionability&quot;?&lt;/p&gt;
&lt;h2 id=&quot;actionability&quot;&gt;Actionability&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;For myself&lt;/strong&gt;, it has generally meant that I'm confident enough in the results to take the next modelling step. My second curves depended on the first curves, and after double-checking multiple ways, I thought the first curve fits, though not perfect, were good enough when applied across a large number of samples that I could instead move on to the second curves.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;For others&lt;/strong&gt;, particularly at my workplace, it generally means a scientist can make a decision about what next experiment to run.&lt;/p&gt;
&lt;h2 id=&quot;insight-s-mvp-influence&quot;&gt;Insight's MVP Influence&lt;/h2&gt;&lt;p&gt;Going through Insight Data Science drilled into us an instinct for developing an MVP for our problem before going on to perfect it. I think that general model works well. My project's final modelling results will be the result of chains of modelling assumptions at every step. Documenting those steps clearly, and then being willing to revisit those assumptions, is going always a good thing.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2018/11/12/thoughts-on-black/">
    <title type="text">Thoughts on Black</title>
    <id>urn:uuid:08f2ca8d-df54-3740-8d8b-b93a5bd7e728</id>
    <updated>2018-11-12T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2018/11/12/thoughts-on-black/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;Having used Black for quite a while now, I have a hunch that it will continue to surpass its current popularity amongst projects.&lt;/p&gt;
&lt;p&gt;It's one thing to be opinionated about things that matter for a project, but don't matter personally. Like code style. It's another thing to actually build a tool that, with one command, realizes those opinions in (milli)seconds. That's exactly what Black does.&lt;/p&gt;
&lt;p&gt;At the end of the day, it was, and still is, a tool that has a very good human API - that of convenience.&lt;/p&gt;
&lt;p&gt;By being opinionated about what code &lt;em&gt;ought&lt;/em&gt; to look like, &lt;code&gt;black&lt;/code&gt; has very few configurable parameters. Its interface is very simple. &lt;em&gt;Convenient.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;By automagically formatting &lt;em&gt;every&lt;/em&gt; Python file in subdirectories (if not otherwise configured so), it makes code formatting quick and easy. &lt;em&gt;Convenient.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In particular, by being opinionated about conforming to community standards for code style with Python, &lt;code&gt;black&lt;/code&gt; ensures that formatted code is consistently formatted and thus easy to read. &lt;em&gt;Convenient!&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Because of this, I highly recommend the use of &lt;code&gt;black&lt;/code&gt; for code formatting.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install black
&lt;/code&gt;&lt;/pre&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2018/11/7/bayesian-modelling-is-hard-work/">
    <title type="text">Bayesian Modelling is Hard Work!</title>
    <id>urn:uuid:b544d44a-b92e-3f93-80b0-83a5f38ea17b</id>
    <updated>2018-11-07T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2018/11/7/bayesian-modelling-is-hard-work/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;It’s definitely not easy work; anybody trying to tell you that you can &quot;just apply this 
model and just be done with it&quot; is probably wrong.&lt;/p&gt;
&lt;h2 id=&quot;simple-models&quot;&gt;Simple Models&lt;/h2&gt;&lt;p&gt;Let me clarify: I agree that doing the first half of the statement, &quot;just apply this model&quot;, 
is a good starting point, but I disagree with the latter half, &quot;and just be done with it&quot;. I 
have found that writing and fitting a very naive Bayesian model to the data I have is a very 
simple thing. But doing the right thing is not. Let’s not be confused: I don’t mean a Naive 
Bayes model, I mean naively writing down a Bayesian model that is structured very simply 
with the simplest of priors that you can think of.&lt;/p&gt;
&lt;p&gt;Write down the model, including any transformations that you may need on the variables, and 
then lazily put in a bunch of priors. For example, you might just start with Gaussians 
everywhere a parameter could take on negative to positive infinity values, or a bounded Half 
Gaussian if it can only take values above (or below) a certain value. You might assume 
Gaussian-distributed noise in the output.&lt;/p&gt;
&lt;p&gt;Let’s still not be confused: Obviously this would not apply to a beta-bernoulli/binomial 
model!&lt;/p&gt;
&lt;p&gt;Doing the right thing, however, is where the tricky parts come in. To butcher and mash-up 
two quotes:&lt;/p&gt;
&lt;blockquote class=&quot;blockquote&quot;&gt;
&lt;p&gt;All models are wrong, but some are useful (Box), yet some models are more wrong than others (modifying from Orwell). 
&lt;/blockquote&gt;&lt;h2 id=&quot;critiquing-models&quot;&gt;Critiquing Models&lt;/h2&gt;&lt;p&gt;When doing modeling, a series of questions comes up:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Do my naive assumptions about &quot;Gaussians everywhere&quot; hold? &lt;/li&gt;
&lt;li&gt;Given that my output data are continuous, is there a better distribution that can describe 
the likelihood?&lt;/li&gt;
&lt;li&gt;Is there are more principled prior for some of the variables?&lt;/li&gt;
&lt;li&gt;Does my link function, which joins the input data to the output parameters, properly 
describe their relationship?&lt;/li&gt;
&lt;li&gt;Instead of independent priors per group, would a group prior be justifiable?&lt;/li&gt;
&lt;li&gt;Does my model yield posterior distributions that are within bounds of reasonable ranges, 
which come from my prior knowledge? If it does not, do I need to bound my priors instead of 
naively assuming the full support for those distributions?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I am quite sure that this list is non-exhaustive, and probably only covers the bare minimum 
we have to think about.&lt;/p&gt;
&lt;p&gt;Doing these model critiques is not easy. Yet, if we are to work towards truthful and 
actionable conclusions, it is a necessity. We want to know ground truth, so that we can act 
on it accordingly, and hence take appropriate actions.&lt;/p&gt;
&lt;h2 id=&quot;prior-experience&quot;&gt;Prior Experience&lt;/h2&gt;&lt;p&gt;I have experienced this modeling loop that Mike Betancourt describes (in his &lt;a href=&quot;https://github.com/betanalpha/jupyter_case_studies/blob/master/principled_bayesian_workflow/principled_bayesian_workflow.ipynb&quot;&gt;Principled 
Bayesian Workflow notebook&lt;/a&gt;) more than once. One involved count data, with a data 
scientist from TripAdvisor last year at the SciPy conference; another involved estimating 
cycle time distributions at work, and yet another involved a whole 4-parameter dose-response 
curve. In each scenario, model fitting and critique took hours at the minimum; I’d also note 
that with real world data, I didn’t necessarily get to the &quot;win&quot; was looking for.&lt;/p&gt;
&lt;p&gt;With the count data, the TripAdvisor data scientist and I reached a point where after 5 
rounds of tweaking his model, we had a model that fit the data, and described a data 
generating process that mimics closely to what we would expect given his process. It took us 
5 rounds, and 3 hours of staring at his model and data, to get there!&lt;/p&gt;
&lt;p&gt;Yet with cycle time distributions from work, a task ostensibly much easier (&quot;just fit a 
distribution to the data&quot;), none of my distribution choices, which reflected what I thought 
would be the data generating process, gave me a &quot;good fit&quot; to the data. I checked by many 
means: K-S tests, visual inspection, etc. I ended up abandoning the fitting procedure, and 
used empirical distributions instead.&lt;/p&gt;
&lt;p&gt;With a 4-parameter dose-response curve, it took me 6 hours to go through 6 rounds of 
modeling to get to a point where I felt comfortable with the model. I started with a 
simplifying &quot;Gaussians everywhere&quot; assumption. Later, though, I hesitantly and tentatively 
putting in bound priors because I knew some posterior distributions were completely out of 
range under the naive assumptions of the first model, and were likely a result of 
insufficient range in the concentrations tested. Yet even that model remained unsatisfying: 
I was stuck with some compounds that didn’t change the output regardless of concentration, 
and that data are fundamentally very hard to fit with a dose response curve. Thus I the next 
afternoon,I modeled the dose response relationship using a Gaussian Process instead. Neither 
model is completely satisfying to the degree that the count data model was, but both the GP 
and the dose-response curve are and will be roughly correct modeling choices (with the GP 
probably being more flexible), and importantly, both are actionable by the experimentalists.&lt;/p&gt;
&lt;h2 id=&quot;thoughts&quot;&gt;Thoughts&lt;/h2&gt;&lt;p&gt;As you probably can see, whenever we either (1) don’t know ground truth, and/or (2) have 
messy, real world data that don’t fit idealized assumptions about the data generating 
process, &lt;strong&gt;getting the model &quot;right&quot; is a very hard thing to do!&lt;/strong&gt; Moreover, data are 
insufficient on their own to critique the model; we will always need to bring in prior 
knowledge. Much as all probability is conditional probability (Venn), all modeling involves 
prior knowledge. Sometimes it comes up in non-modellable ways, though as far as possible, 
it’s a good exercise to try incorporating that into the model definition.&lt;/p&gt;
&lt;h2 id=&quot;canned-models?&quot;&gt;Canned Models?&lt;/h2&gt;&lt;p&gt;Even with that said, I’m still a fan of canned models, such as those provided by 
&lt;code&gt;pymc-learn&lt;/code&gt; and &lt;code&gt;scikit-learn&lt;/code&gt; - provided we recognize that their &quot;canned&quot; nature and are 
equipped to critique and modify said models. Yes, they provide easy, convenient baselines 
that we can get started with. We can &quot;just apply this model&quot;. But we can’t &quot;just be done 
with it&quot;: the hard part of getting the model right takes much longer and much more hard 
work. &lt;strong&gt;&lt;em&gt;Veritas!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2018/10/26/more-dask-pre-scattering-data/">
    <title type="text">More Dask: Pre-Scattering Data</title>
    <id>urn:uuid:8b67f328-eb2a-3367-a47c-537108e7c883</id>
    <updated>2018-10-26T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2018/10/26/more-dask-pre-scattering-data/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;I learned a new thing about &lt;code&gt;dask&lt;/code&gt; yesterday: pre-scattering data properly!&lt;/p&gt;
&lt;p&gt;Turns out, you can pre-scatter your data across worker nodes, and have them access that data later when submitting functions to the scheduler.&lt;/p&gt;
&lt;h2 id=&quot;how-to&quot;&gt;How-To&lt;/h2&gt;&lt;p&gt;To do so, we first call on &lt;code&gt;client.scatter&lt;/code&gt;, pass in the data that I want to scatter across all nodes, ensure that broadcasting is turned on (if and only if I am sure that all worker nodes will need it), and finally assign it to a new variable.&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;dask_jobqueue&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SGECluster&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;dask.distributed&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Client&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;cluster&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SGECluster&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# put parameters in there.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;client&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Client&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cluster&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;data_future&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;client&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;broadcast&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;One key thing to remember here is to assign the result of &lt;code&gt;client.scatter&lt;/code&gt; to a variable. This becomes a pointer that you pass into other functions that are submitted via the &lt;code&gt;client.submit&lt;/code&gt; interface. Because this point is not immediately clear from the &lt;code&gt;client.scatter&lt;/code&gt; docs, &lt;a href=&quot;https://github.com/dask/distributed/pull/2320&quot;&gt;I put in a pull request (PR) to provide some just-in-time documentation&lt;/a&gt;, which just got merged this morning. By the way, not every PR has to be code - documentation help is always good!&lt;/p&gt;
&lt;p&gt;Once we've scattered the data across our worker nodes and obtained a pointer for the scattered data, we can parallel submit our function across worker nodes.&lt;/p&gt;
&lt;p&gt;Let's say we have a function, called &lt;code&gt;func&lt;/code&gt;, that takes in the &lt;code&gt;data&lt;/code&gt; variable and returns a number. The key characteristic of this function is that it takes anywhere from a few seconds to minutes to run, but I need it run many times (think hundreds to thousands of times).&lt;/p&gt;
&lt;p&gt;In serial, I would usually do this as a list comprehension:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;results&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If done in parallel, I can now use the &lt;code&gt;client&lt;/code&gt; object to submit the function across all worker nodes. For clarity, let me switch to a &lt;code&gt;for-loop&lt;/code&gt; instead:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;results&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;results&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;client&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;submit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data_future&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;results&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;client&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gather&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;results&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Because the &lt;code&gt;client&lt;/code&gt; does not have to worry about sending the large &lt;code&gt;data&lt;/code&gt; object across the network of cluster nodes, it is very fast to submit the functions to the scheduler, which then dispatches it to the worker nodes, which all know where &lt;code&gt;data_future&lt;/code&gt; is on their own &quot;virtual cluster&quot; memory.&lt;/p&gt;
&lt;h2 id=&quot;advantages&quot;&gt;Advantages&lt;/h2&gt;&lt;p&gt;By pre-scattering, we invest a bit of time pre-allocating memory on worker nodes to hold data that are relatively expensive to transfer. This time investment reaps dividends later when we are working with functions that operate on the data.&lt;/p&gt;
&lt;h2 id=&quot;cautions&quot;&gt;Cautions&lt;/h2&gt;&lt;p&gt;Not really disadvantages (as I can't think of any), just some things to note:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;You need to know how much memory my data requires, and have to request for at least that amount of memory first per worker node at the the &lt;code&gt;SGECluster&lt;/code&gt; instantiation step.&lt;/li&gt;
&lt;li&gt;Pre-scattering sometimes takes a bit of time, but I have not seen it take as much time as having the scheduler handle everything.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;acknowledgments&quot;&gt;Acknowledgments&lt;/h2&gt;&lt;p&gt;Special thanks goes to &lt;a href=&quot;https://matthewrocklin.com&quot;&gt;Matt Rocklin&lt;/a&gt;, who answered my question on &lt;a href=&quot;https://stackoverflow.com/questions/52997229/is-there-an-advantage-to-pre-scattering-data-objects-in-dask&quot;&gt;StackOverflow&lt;/a&gt;, which in turn inspired this blog post.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2018/10/11/parallel-processing-with-dask-on-gridengine-clusters/">
    <title type="text">Parallel Processing with Dask on GridEngine Clusters</title>
    <id>urn:uuid:18e951d8-6df5-3751-bf52-1bb750d90d1a</id>
    <updated>2018-10-11T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2018/10/11/parallel-processing-with-dask-on-gridengine-clusters/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;I recently just figured out how to get this working... and it's awesome! :D&lt;/p&gt;
&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;&lt;p&gt;If I'm developing an analysis in the Jupyter notebook, and I have one semi-long-running function (e.g. takes dozens of seconds) that I need to run over tens to hundreds of thousands of similar inputs, it'll take &lt;em&gt;ages&lt;/em&gt; for this to complete in serial. For a sense of scale, a function that takes ~20 seconds per call run serially over 10,000 similar inputs would take 200,000 seconds, which is 2 days of run-time (not including any other overhead). That's not feasible for interactive exploration of data. If I could somehow parallelize just the function over 500 compute nodes, we could take the time down to 7 minutes.&lt;/p&gt;
&lt;p&gt;GridEngine-based compute clusters are one of many options for parallelizing work. During grad school at MIT, and at work at Novartis, the primary compute cluster environment that I've encountered has been GridEngine-based. However, because they are designed for batch jobs, as a computational scientist, we have to jump out of whatever development environment we're currently using, and move to custom scripts.&lt;/p&gt;
&lt;p&gt;In order to do parallelism with traditional GridEngine systems, I would have to jump out of the notebook and start writing job submission scripts, which disrupts my workflow. I would be disrupting my thought process, and lose the interactivity that I might need to prototype my work faster.&lt;/p&gt;
&lt;h2 id=&quot;enter-dask&quot;&gt;Enter Dask&lt;/h2&gt;&lt;p&gt;&lt;code&gt;dask&lt;/code&gt;, alongside &lt;code&gt;dask-jobqueue&lt;/code&gt; enables computational scientists like myself to take advantage of existing GridEngine setups to do interactive, parallel work. As long as I have a Jupyter notebook server running on a GridEngine-connected compute node, I can submit functions to the GridEngine cluster and collect back those results to do further processing, in a fraction of the time that it would take, thus enabling me to do my science faster than if I did everything single core/single node.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;In this blog post, I'd like to share an annotated, minimal setup for using Dask on a GridEngine cluster.&lt;/strong&gt; (Because we use Dask, more complicated pipelines are possible as well - I would encourage you to read the Dask docs for more complex examples.) I will assume that you are working in a Jupyter notebook environment, and that the notebook you are working out of is hosted on a GridEngine-connected compute node, from which you are able to &lt;code&gt;qsub&lt;/code&gt; tasks. Don't worry, you won't be qsub-ing anything though!&lt;/p&gt;
&lt;h2 id=&quot;setup&quot;&gt;Setup&lt;/h2&gt;&lt;p&gt;To start, we need a cell that houses the following code block:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;dask_jobqueue&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SGECluster&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;dask&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;delayed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;compute&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;cluster&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SGECluster&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;queue&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;default.q&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                     &lt;span class=&quot;n&quot;&gt;walltime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;1500000&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                     &lt;span class=&quot;n&quot;&gt;processes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                     &lt;span class=&quot;n&quot;&gt;memory&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;1GB&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                     &lt;span class=&quot;n&quot;&gt;cores&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                     &lt;span class=&quot;n&quot;&gt;env_extra&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;source /path/to/custom/script.sh&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                &lt;span class=&quot;s1&quot;&gt;&amp;#39;export ENV_VARIABLE=&amp;quot;SOMETHING&amp;quot;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
                       &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Here, we are instantiating an &lt;code&gt;SGECluster&lt;/code&gt; object under the variable name &lt;code&gt;cluster&lt;/code&gt;. What &lt;code&gt;cluster&lt;/code&gt; stores is essentially a configuration for a block of worker nodes that you will be requesting. Under the hood, what &lt;code&gt;dask-jobqueue&lt;/code&gt; is doing is submitting jobs to the GridEngine scheduler, which will block off a specified amount of compute resources (e.g. number of cores, amount of RAM, whether you want GPUs or not, etc.) for a pre-specified amount of time, on which Dask then starts a worker process to communicate with the head process coordinating tasks amongst workers.&lt;/p&gt;
&lt;p&gt;As such, you do need to know two pieces of information:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;queue&lt;/code&gt;: The queue that jobs are to be submitted to. Usually, it is named something like &lt;code&gt;default.q&lt;/code&gt;, but you will need to obtain this through GridEngine. If you have the ability to view all jobs that are running, you can call &lt;code&gt;qstat&lt;/code&gt; at the command line to see what queues are being used. Otherwise, you might have to ping your system administrator for this information.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;walltime&lt;/code&gt;: You will also need to pre-estimate the wall clock time, in seconds, that you want the worker node to be alive for. It should be significantly longer than the expected time you think you will need, so that your function call doesn't timeout unexpectedly. I have defaulted to 1.5 million seconds, which is about 18 days of continual runtime. In practice, I usually kill those worker processes after just a few hours.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Besides that, you also need to specify the resources that you need per worker process. In my example above, I'm asking for each worker process to use only 1GB of RAM, 1 core, and to use only 1 process per worker (i.e. no multiprocessing, I think).&lt;/p&gt;
&lt;p&gt;Finally, I can also specify extra environment setups that I will need. Because each worker process is a new process that has no knowledge of the parent process' environment, you might have to source some bash script, or activate a Python environment, or export some environment variable. This can be done under the &lt;code&gt;env_extra&lt;/code&gt; keyword, which accepts a list of strings.&lt;/p&gt;
&lt;h2 id=&quot;request-for-worker-compute-nodes&quot;&gt;Request for worker compute &quot;nodes&quot;&lt;/h2&gt;&lt;p&gt;I put &quot;nodes&quot; in quotation marks, because they are effectively logical nodes, rather than actual compute nodes. (Technically, I think a compute node minimally means one physical hardware unit with CPUs and RAM).&lt;/p&gt;
&lt;p&gt;In order to request for worker nodes to run your jobs, you need the next line of code:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cluster&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scale&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;With this line, under the hood, &lt;code&gt;dask-jobqueue&lt;/code&gt; will start submitting 500 jobs, each requesting 1GB of RAM and 1 core, populating my compute environment according to the instructions I provided under &lt;code&gt;env_extra&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;At the end of this, I effectively have a 500-node cluster on the larger GridEngine cluster (let's call this a &quot;virtual cluster&quot;), each with 1GB of RAM and 1 core available to it, on which I can submit functions to run.&lt;/p&gt;
&lt;h2 id=&quot;start-a-client-process&quot;&gt;Start a client process&lt;/h2&gt;&lt;p&gt;In order to submit jobs to my virtual cluster, I have to instantiate a client that is connected to the cluster, and is responsible for sending functions there.&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;dask.distributed&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Client&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;client&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Client&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cluster&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&quot;compute&quot;&gt;Compute!&lt;/h2&gt;&lt;p&gt;With this setup complete (I have it stored as a TextExpander snippets), we can now start submitting functions to the virtual cluster!&lt;/p&gt;
&lt;p&gt;To simulate this, let's define a square-rooting function that takes 2-3 seconds to run each time it is called, and returns the square of its inputs. This simulates a function call that is computationally semi-expensive to run a few times, but because call on this hundreds of thousands of time, the total running time to run it serially would be too much.&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;time&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sleep&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;math&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;random&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;slow_sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    Simulates the run time needed for a semi-expensive function call.&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;assert&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# define sleeping time in seconds, between 2-3 seconds.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sleep&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id=&quot;serial-execution&quot;&gt;Serial Execution&lt;/h3&gt;&lt;p&gt;In a naive, serial setting, we would call on the function in a for-loop:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;results&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;results&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;slow_sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This would take us anywhere between 20,000 to 30,000 seconds (approximately 8 hours, basically).&lt;/p&gt;
&lt;h3 id=&quot;parallel-execution&quot;&gt;Parallel Execution&lt;/h3&gt;&lt;p&gt;In order to execute this in parallel instead, we could do one of the following three ways:&lt;/p&gt;
&lt;h4 id=&quot;map&quot;&gt;map&lt;/h4&gt;&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sq_roots&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;client&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;slow_sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sq_roots&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;client&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gather&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sq_roots&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h4 id=&quot;for-loop&quot;&gt;for-loop&lt;/h4&gt;&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sq_roots&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sq_roots&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;client&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;submit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;slow_sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;restart&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# submit the function as first argument, then rest of arguments&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sq_roots&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;client&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gather&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sq_roots&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h4 id=&quot;delayed&quot;&gt;delayed&lt;/h4&gt;&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sq_roots&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sq_roots&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;delayed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;slow_sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sq_roots&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;compute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sq_roots&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;I have some comments on each of the three methods, each of which I have used.&lt;/p&gt;
&lt;p&gt;First off, &lt;strong&gt;each of them do require us to change the code that we would have written in serial&lt;/strong&gt;. This little bit of overhead is the only tradeoff we really need to make in order to gain parallelism.&lt;/p&gt;
&lt;p&gt;In terms of &lt;strong&gt;readability&lt;/strong&gt;, all of them are quite readable, though in my case, I tend to favour the for-loop with &lt;code&gt;client.submit&lt;/code&gt;. Here is why.&lt;/p&gt;
&lt;p&gt;For readability, the for-loop explicitly indicates that we are looping over something. It's probably more easy for novices to approach my code that way.&lt;/p&gt;
&lt;p&gt;For &lt;strong&gt;debuggability&lt;/strong&gt;, &lt;code&gt;client.submit&lt;/code&gt; returns a Futures object (same goes for &lt;code&gt;client.map&lt;/code&gt;). A &quot;Futures&quot; object might be confusing at first, so let me start by demystifying that. A Futures object promises that the result that is computed from &lt;code&gt;slow_sqrt&lt;/code&gt; will exist, and actually contains a ton of diagnostic information, including the &lt;code&gt;type&lt;/code&gt; of the object (which can be useful for diagnosing whether my function actually ran correctly). In addition to that, I can call on &lt;code&gt;Futures.result()&lt;/code&gt; to inspect the actual result (in this case, &lt;code&gt;sq_roots[0].result()&lt;/code&gt;). This is good for debugging the function call, in case there are issues when scaling up. (At work, I was pinging a database in parallel, and sometimes the ping would fail; debugging led me to include some failsafe code, including retries and sleeps with random lengths to stagger out database calls.)&lt;/p&gt;
&lt;p&gt;Finally, &lt;strong&gt;the Futures interface is non-blocking&lt;/strong&gt; on my Jupyter notebook session. Once I've submitted the jobs, I can continue with other development work in my notebook in later cells, and check back when the Dask dashboard indicates that the jobs are done.&lt;/p&gt;
&lt;p&gt;That said, I like the &lt;code&gt;delayed&lt;/code&gt; interface as well. Once I was done debugging and confident that my own data pipeline at work wouldn't encounter the failure modes I was seeing, I switched over to the &lt;code&gt;delayed&lt;/code&gt; interface and scaled up my analysis. I was willing to trade in the interactivity using the &lt;code&gt;Futures&lt;/code&gt; interface for the automation provided by the &lt;code&gt;delayed&lt;/code&gt; interface. (I also first used Dask on a single node through the delayed interface as well).&lt;/p&gt;
&lt;p&gt;Of course, there's something also to be said for the simplicity of two lines of code for parallelism (with the &lt;code&gt;client.map&lt;/code&gt; example).&lt;/p&gt;
&lt;p&gt;The final line in each of the code blocks allows us to &quot;gather&quot; the results back into my coordinator node's memory, thus completing the function call and giving us the result we needed.&lt;/p&gt;
&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;&lt;p&gt;That concludes it! The two key ideas illustrated in this blog post were:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;To set up a virtual cluster on a GridEngine system, we essentially harness the existing job submission system to generate workers that listen for tasks.&lt;/li&gt;
&lt;li&gt;A useful programming pattern is to &lt;code&gt;submit&lt;/code&gt; functions using the &lt;code&gt;client&lt;/code&gt; object using &lt;code&gt;client.submit(func, *args, **kwargs)&lt;/code&gt;. This requires minimal changes from serial code.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;practical-tips&quot;&gt;Practical Tips&lt;/h2&gt;&lt;p&gt;Here's some tips for doing parallel processing, which I've learned over the years.&lt;/p&gt;
&lt;p&gt;Firstly, never prematurely parallelize. It's as bad as prematurely optimizing code. If your code is running slowly, check first to make sure that there aren't algorithmic complexity issues, or bandwidths being clogged up (e.g. I/O bound). As the Dask docs state, it is easier to achieve those gains first before doing parallelization.&lt;/p&gt;
&lt;p&gt;Secondly, when developing parallel workflows, make sure to test the pipeline on subsets of input data first, and slowly scale up. It is during this period that you can also profile memory usage to check to see if you need to request for more RAM per worker.&lt;/p&gt;
&lt;p&gt;Thirdly, for GridEngine clusters, it is usually easier to request for many small worker nodes that consume few cores and small amounts of RAM. If your job is trivially parallelizable, this may be a good thing.&lt;/p&gt;
&lt;p&gt;Fourthly, it's useful to have realistic expectations on the kinds of speed-ups you can expect to gain. At work, through some ad-hoc profiling, I quickly came to the realization that concurrent database pings were the most likely bottleneck in my code's speed, and that nothing apart from increasing the number of concurrent database pings allowed would make my parallel code go faster.&lt;/p&gt;
&lt;p&gt;Finally, on a shared cluster, be respectful of others' usage. Don't request for unreasonable amounts of compute time. And when you're confirmed done with your analysis work, remember to shut down the virtual cluster! :)&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2018/9/4/optimizing-block-sparse-matrix-creation-with-python/">
    <title type="text">Optimizing Block Sparse Matrix Creation with Python</title>
    <id>urn:uuid:77088e4f-4e42-31ba-8d87-ad8dca59c91a</id>
    <updated>2018-09-04T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2018/9/4/optimizing-block-sparse-matrix-creation-with-python/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;networkx&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;nx&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;scipy.sparse&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sp&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;seaborn&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sns&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tqdm&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tqdm&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;typing&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;List&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numba&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;jit&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;sns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_context&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;talk&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_style&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;white&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load_ext&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;autoreload&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;autoreload&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matplotlib&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inline&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;InlineBackend&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure_format&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;retina&amp;#39;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;&lt;p&gt;At work, I recently encountered a neat problem. I'd like to share it with you all.&lt;/p&gt;
&lt;p&gt;One of my projects involves graphs; specifically, it involves taking individual graphs and turning them into one big graph. If you've taken my Network Analysis Made Simple workshops before, you'll have learned that graphs can be represented as a matrix, such as the one below:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;G&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;erdos_renyi_graph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_numpy_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;G&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;sns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;heatmap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;../../../../../blog/2018/9/4/optimizing-block-sparse-matrix-creation-with-python/output_2_1.png&quot; alt=&quot;png&quot;&gt;&lt;/p&gt;
&lt;p&gt;Because the matrix is so sparse, we can actually store it as a &lt;strong&gt;sparse matrix&lt;/strong&gt;:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A_sparse&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;adjacency_matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;G&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tocoo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A_sparse&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;row&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A_sparse&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A_sparse&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;[array([0, 0, 1, 2, 3, 3, 3, 4, 4, 5, 5, 5, 5, 6, 7, 7, 8, 8], dtype=int32),
 array([5, 7, 4, 7, 5, 6, 8, 1, 5, 0, 3, 4, 8, 3, 0, 2, 3, 5], dtype=int32),
 array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int64)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The most straightforward way of storing sparse matrices is in the COO (COOrdinate) format, which is also known as the &quot;triplet&quot; format, or the &quot;ijv&quot; format. (&quot;i&quot; is row, &quot;j&quot; is col, &quot;v&quot; is value)&lt;/p&gt;
&lt;p&gt;If we want to have two or more graphs stored together in a single matrix, which was what my projects required, then one way of representing them is as follows:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;G2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;erdos_renyi_graph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;A2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_numpy_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;G2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;heatmap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;block_diag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;todense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;../../../../../blog/2018/9/4/optimizing-block-sparse-matrix-creation-with-python/output_6_1.png&quot; alt=&quot;png&quot;&gt;&lt;/p&gt;
&lt;p&gt;Now, notice how there's 25 nodes in total (0 to 24), and that they form what we call a &quot;block diagonal&quot; format. In its &quot;dense&quot; form, we have to represent $25^2$ values inside the matrix. That's fine for small amounts of data, but if we have tens of thousands of graphs, that'll be impossible to deal with!&lt;/p&gt;
&lt;p&gt;You'll notice I used a function from &lt;code&gt;scipy.sparse&lt;/code&gt;, the &lt;code&gt;block_diag&lt;/code&gt; function, which will create a block diagonal sparse matrix from an iterable of input matrices.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;block_diag&lt;/code&gt; is the function that I want to talk about in this post.&lt;/p&gt;
&lt;h1 id=&quot;profiling-block_diag-performance&quot;&gt;Profiling &lt;code&gt;block_diag&lt;/code&gt; performance&lt;/h1&gt;&lt;p&gt;I had noticed that when dealing with tens of thousands of graphs, &lt;code&gt;block_diag&lt;/code&gt; was not performing up to scratch. Specifically, the time it needed would scale quadratically with the number of matrices provided.&lt;/p&gt;
&lt;p&gt;Let's take a look at some simulated data to illustrate this.&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Gs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;As&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;30&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;G&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;erdos_renyi_graph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Gs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;G&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_numpy_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;G&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;As&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Let's now define a function to profile the code.&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;time&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;random&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;profile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n_graphs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;400&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;600&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;800&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1400&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1600&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1800&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tqdm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_graphs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 3 replicates per n&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;start&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;As&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;data_sp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;profile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;block_diag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_sp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;number of graphs&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;time (s)&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;../../../../../blog/2018/9/4/optimizing-block-sparse-matrix-creation-with-python/output_12_1.png&quot; alt=&quot;png&quot;&gt;&lt;/p&gt;
&lt;p&gt;It is quite clear that the increase in time is super-linear, showing $O(n^2)$ scaling. (Out of impatience, I did not go beyond 50,000 graphs in this post, but at work, I did profile performance up to that many graphs. For reference, it took about 5 minutes to finish creating the scipy sparse matrix for 50K graphs.&lt;/p&gt;
&lt;h1 id=&quot;optimizing-block_diag-performance&quot;&gt;Optimizing &lt;code&gt;block_diag&lt;/code&gt; performance&lt;/h1&gt;&lt;p&gt;I decided to take a stab at creating an optimized version of &lt;code&gt;block_diag&lt;/code&gt;. Having profiled my code and discovering that sparse block diagonal matrix creation was a bottleneck, I implemented my own sparse block diagonal matrix creation routine using pure Python.&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_block_diag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;As&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;List&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]):&lt;/span&gt;
    &lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    Return the (row, col, data) triplet for a block diagonal matrix.&lt;/span&gt;

&lt;span class=&quot;sd&quot;&gt;    Intended to be put into a coo_matrix. Can be from scipy.sparse, but&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    also can be cupy.sparse, or Torch sparse etc.&lt;/span&gt;

&lt;span class=&quot;sd&quot;&gt;    Example usage:&lt;/span&gt;

&lt;span class=&quot;sd&quot;&gt;    &amp;gt;&amp;gt;&amp;gt; row, col, data = _block_diag(As)&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    &amp;gt;&amp;gt;&amp;gt; coo_matrix((data, (row, col)))&lt;/span&gt;

&lt;span class=&quot;sd&quot;&gt;    :param As: A list of numpy arrays to create a block diagonal matrix.&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    :returns: (row, col, data), each as lists.&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;row&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;col&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;start_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;As&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;nrows&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ncols&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nrows&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ncols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;row&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;start_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nrows&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;row&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Running it through the same profiling routine:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_custom&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;profile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_block_diag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_custom&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;custom&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_sp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;scipy.sparse&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;number of graphs&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;time (s)&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;../../../../../blog/2018/9/4/optimizing-block-sparse-matrix-creation-with-python/output_18_1.png&quot; alt=&quot;png&quot;&gt;&lt;/p&gt;
&lt;p&gt;I also happened to have listened in on a &lt;a href=&quot;https://www.youtube.com/watch?v=6oXedk2tGfk&quot;&gt;talk by Siu Kwan Lam&lt;/a&gt; during lunch, on &lt;code&gt;numba&lt;/code&gt;, the JIT optimizer that he has been developing for the past 5 years now. Seeing as how the code I had written in &lt;code&gt;_block_diag&lt;/code&gt; was all numeric code, which is exactly what &lt;code&gt;numba&lt;/code&gt; was designed for, I decided to try optimizing it with JIT.&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numba&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;jit&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;data_jit&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;profile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_block_diag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_custom&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;custom&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_sp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;scipy.sparse&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_jit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;jit&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;number of graphs&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;time (s)&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;../../../../../blog/2018/9/4/optimizing-block-sparse-matrix-creation-with-python/output_21_1.png&quot; alt=&quot;png&quot;&gt;&lt;/p&gt;
&lt;p&gt;Notice the speed-up that JIT-ing the code provided! (Granted, that first run was a &quot;warm-up&quot; run; once JIT-compiled, everything is really fast!)&lt;/p&gt;
&lt;p&gt;My custom implementation only returns the (row, col, data) triplet. This is an intentional design choice - having profiled the code with and without calling a COO matrix creation routine, I found the JIT-optimized performance to be significantly better without creating the COO matrix routine. As I still have to create a sparse matrix, I ended up with the following design:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;block_diag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;As&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;row&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;jit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_block_diag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;As&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;coo_matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;row&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;data_wrap&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;profile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;block_diag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_custom&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;custom&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_sp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;scipy.sparse&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_jit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;jit&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_wrap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;wrapped&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;number of graphs&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;time (s)&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;../../../../../blog/2018/9/4/optimizing-block-sparse-matrix-creation-with-python/output_24_1.png&quot; alt=&quot;png&quot;&gt;&lt;/p&gt;
&lt;p&gt;You'll notice that the array creation step induces a consistent overhead on top of the sparse matrix triplet creation routine, but stays flat and trends the &quot;jit&quot; dots quite consistently. It intersects the &quot;custom&quot; dots at about $10^3$ graphs. Given the problem that I've been tackling, which involves $10^4$ to $10^6$ graphs at a time, it is an absolutely worthwhile improvement to JIT-compile the &lt;code&gt;_block_diag&lt;/code&gt; function.&lt;/p&gt;
&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;&lt;p&gt;This was simultaneously a fun and useful exercise in optimizing my code!&lt;/p&gt;
&lt;p&gt;A few things I would take away from this:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Profiling code for bottlenecks can be really handy, and can be especially useful if we have a hypothesis on how to optimize it.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;numba&lt;/code&gt; can really speed up array-oriented Python computation. It lives up to the claims on its documentation.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I hope you learned something new, and I hope you also enjoyed reading this post as much as I enjoyed writing it!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2018/8/7/joint-conditional-and-marginal-probability-distributions/">
    <title type="text">Joint, conditional, and marginal probability distributions</title>
    <id>urn:uuid:cf34ea6b-fc4e-3a7f-aa18-5b12a4d0b0d9</id>
    <updated>2018-08-07T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2018/8/7/joint-conditional-and-marginal-probability-distributions/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;Joint probability, conditional probability, and marginal probability... These are three central terms when learning about probability, and they show up in Bayesian statistics as well. However... I never really could remember what they were, especially since we were usually taught them using formulas, rather than pictures.&lt;/p&gt;
&lt;p&gt;Well, for those who learn a bit better using pictures... if you know what a probability distribution is, then hopefully these help with remembering what these terms mean. (Clicking on the image will bring you to the original, hosted on GitHub.)&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/ericmjl/distributions/raw/master/joint-conditional-marginal.pdf&quot;&gt;&lt;img src=&quot;../../../../../blog/2018/8/7/joint-conditional-and-marginal-probability-distributions/joint-conditional-marginal.png&quot; alt=&quot;&quot;&gt;&lt;/a&gt;&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2018/8/6/d-separation-in-causal-inference/">
    <title type="text">d-separation in causal inference</title>
    <id>urn:uuid:2f272b7e-2bfd-3f09-9d5d-2b782ce072bb</id>
    <updated>2018-08-06T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2018/8/6/d-separation-in-causal-inference/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;Yesterday evening, I had an empty block of time during which I finally did a worked example of finding whether two nodes are &quot;d-separated&quot; in a causal graph. It was pretty instructive to implement the algorithm. It also reminded me yet again: there's this weird thing about me where I need programming to learn math!&lt;/p&gt;
&lt;p&gt;Anyways, if you're interested in seeing the implementation, it's available at &lt;a href=&quot;https://github.com/ericmjl/causality&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2018/8/1/nxviz-05-released/">
    <title type="text">nxviz 0.5 released!</title>
    <id>urn:uuid:04350bd2-49a7-32f6-b61c-ccc1c573974c</id>
    <updated>2018-08-01T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2018/8/1/nxviz-05-released/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;A new version of nxviz is released!&lt;/p&gt;
&lt;p&gt;In this update, I have added a declarative interface for visualizing geographically-constrained graphs. Here, nodes in a graph have their placement constrained by longitude and latitude.&lt;/p&gt;
&lt;p&gt;An example of how to use it is below:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;../../../../../blog/2018/8/1/nxviz-05-released/carbon-original.png&quot;&gt;&lt;img src=&quot;../../../../../blog/2018/8/1/nxviz-05-released/carbon-small.png&quot; alt=&quot;nxviz geoplots&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In the GeoPlot constructor API, the keyword arguments &lt;code&gt;node_lat&lt;/code&gt; and &lt;code&gt;node_lon&lt;/code&gt; specify which node metadata are to be used to place nodes on the x- and y- axes.&lt;/p&gt;
&lt;p&gt;By no means do I intend for GeoPlot to replace more sophisticated analysis methods; like &lt;code&gt;seaborn&lt;/code&gt;, the interface is declarative; for me, the intent is to provide a very quick-and-dirty way for an end user to visualize graphs with spatially constrained nodes.&lt;/p&gt;
&lt;p&gt;Please enjoy!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2018/7/27/pyjanitor-0-3-released/">
    <title type="text">pyjanitor 0.3 released!</title>
    <id>urn:uuid:ca97ad1f-1402-3a65-a15b-1653138a01d4</id>
    <updated>2018-07-27T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2018/7/27/pyjanitor-0-3-released/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;A new release of &lt;a href=&quot;https://github.com/ericmjl/pyjanitor&quot;&gt;&lt;code&gt;pyjanitor&lt;/code&gt;&lt;/a&gt; is out!&lt;/p&gt;
&lt;p&gt;Two new features that I have added in include:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Concatenating column names into a single column, such that each item is separated by a delimiter.&lt;/li&gt;
&lt;li&gt;Deconcatenating a column into multiple columns, separating on the basis of a delimiter.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Both of these tasks come up frequently in data preparation.&lt;/p&gt;
&lt;p&gt;For example, concatenating a few columns together oftentimes lets us create an unique index based sample properties.&lt;/p&gt;
&lt;p&gt;On the other hand, deconcatenating columns into multiple columns can be useful when our index is used to store metadata. (This really shouldn't be happening, but... sometimes that's just how the world works right now...)&lt;/p&gt;
&lt;p&gt;Here's an example of how it works:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;../../../../../blog/2018/7/27/pyjanitor-0-3-released/carbon-original.png&quot;&gt;&lt;img src=&quot;../../../../../blog/2018/7/27/pyjanitor-0-3-released/carbon-small.png&quot; alt=&quot;&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;To install &lt;code&gt;pyjanitor&lt;/code&gt;, grab it from PyPI:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;$&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pip&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pyjanitor&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The conda-forge build will be coming soon!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2018/7/26/scipy-2018/">
    <title type="text">SciPy 2018</title>
    <id>urn:uuid:d1504284-578a-32b3-b988-3bac646e8b8e</id>
    <updated>2018-07-26T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2018/7/26/scipy-2018/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;It's been about two weeks since SciPy 2018 ended, and I've finally found some breathing room to write about it.&lt;/p&gt;
&lt;p&gt;SciPy 2018 is the 4th year I've made it to the conference, my first one being SciPy 2015 (not 2014, as I had originally incorrectly remembered). The conference has grown over the years that I've attended it!&lt;/p&gt;
&lt;h2 id=&quot;organizing&quot;&gt;Organizing&lt;/h2&gt;&lt;p&gt;This year, I served again as the Financial Aid Co-Chair with Scott Collis and Celia Cintas. It brings me joy to be able to help bring others to the conference, much as I was a few years back when I was still a graduate student.&lt;/p&gt;
&lt;p&gt;Building upon last year's application process, where we implemented blinded reviews, this year, we improved the review process so that it was much less tedious and more user-friendly for reviewers, i.e. myself, Celia, Scott and our two committee reviewers, Kasia and Patrick.&lt;/p&gt;
&lt;p&gt;The review process can always be improved; we still have some work to do. One would be making the application less intimidating for under-represented individuals. Two might be reworking how we quantify how good our selections are; rather than some aggregate &quot;total&quot; score, it might be that we ought to optimize for a breadth of worthy contributions to the community. Finally, we definitely want to ensure that our focus is on FinAid's mission: to enable us to bring deserving and new people to the conference who otherwise might not have the resources!&lt;/p&gt;
&lt;h2 id=&quot;tutorials&quot;&gt;Tutorials&lt;/h2&gt;&lt;p&gt;I did two tutorials, one with Hugo and one with Mridul. The one with Mridul was on Network Analysis, and the one with Hugo was on Bayesian statistics. Over the years, I've developed muscle memory on Network Analysis, so it felt very natural to me. Bayesian statistics and probabilistic programming was a new topic for myself and Hugo; as such, I spent proportionally more time preparing for that tutorial instead.&lt;/p&gt;
&lt;p&gt;What was pleasantly surprising for me was that Bayesian statistics was gaining a ton of popularity, and this tutorial just happened to be there at the right time. I had a lot of one-on-one chats with tutorial participants after the tutorial and during the conference days, where we talked about the application of Bayesian methods to problems that they had encountered. There's a lot to do until people can generally communicate about data problems using Bayesian methods, but I think we're at an upwards-inflection point right now!&lt;/p&gt;
&lt;h2 id=&quot;conference&quot;&gt;Conference&lt;/h2&gt;&lt;p&gt;I missed the talks mostly because I was doing the Tejas Room track (sit in the Tejas room and chat with people). I nonetheless had a very fruitful and fun time doing so!&lt;/p&gt;
&lt;p&gt;To make up for lost time, I put together a &lt;a href=&quot;https://www.youtube.com/playlist?list=PLW01hpWnEtbTORVE7o70mZqB7IVVjOr4E&quot;&gt;playlist&lt;/a&gt; of things I'd like to catch up on later.&lt;/p&gt;
&lt;h2 id=&quot;sprints&quot;&gt;Sprints&lt;/h2&gt;&lt;p&gt;For the first time ever, I stayed on to sprint! However, I also simultaneously caught a conference bug, so I was basically knocked out for the second day of sprints. For this year's sprints, I implemented a declarative interface for geographic graph visualizations in nxviz, where node placement is prioritized according by geographic information. The intent here isn't to replace geospatial analysis packages, but rather to provide a quick, &lt;code&gt;seaborn&lt;/code&gt;-like view into a graph's geographical structure. Once a user has a feel for the data, if nothing more is needed, they can use the graph as is; otherwise, they can move onto a different package.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2018/7/16/bayesian-estimation-group-comparison-and-workflow/">
    <title type="text">Bayesian Estimation, Group Comparison, and Workflow</title>
    <id>urn:uuid:58217c0b-67d0-3380-92be-00cbcab54a50</id>
    <updated>2018-07-16T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2018/7/16/bayesian-estimation-group-comparison-and-workflow/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;Over the past year, having learned about Bayesian inference methods, I finally see how estimation, group comparison, and model checking build upon each other into this really elegant framework for data analysis.&lt;/p&gt;
&lt;h2 id=&quot;parameter-estimation&quot;&gt;Parameter Estimation&lt;/h2&gt;&lt;p&gt;The foundation of this is &quot;estimating a parameter&quot;. In a typical situation, we are most concerned with the parameter of interest. It could be a population mean, or a population variance. If there's a mathematical function that links the input variables to the output (a.k.a. &quot;link function&quot;), then the parameters of the model are that function's parameters. The key point here is that the atomic activity of Bayesian analysis is the estimation of a parameter, and its associated uncertainty.&lt;/p&gt;
&lt;h2 id=&quot;comparison-of-groups&quot;&gt;Comparison of Groups&lt;/h2&gt;&lt;p&gt;Building on that, we can then estimate parameters for more than one group of things. As a first pass, we could assume that each of the groups are unrelated, and thus &quot;independently&quot; (I'm trying to avoid overloading this term) estimate parameters per group under this assumption. Alternatively, we could assume that the groups are related to one another, and thus use a &quot;hierarchical&quot; model to estimate parameters for each group.&lt;/p&gt;
&lt;p&gt;Once we've done that, what's left is the comparison of parameters between the groups. The simplest activity is to compare the posterior distributions' 95% highest posterior densities, and check to see if they overlap. Usually this is done for the mean, or for regression parameters, but the variance might also be important to check as well.&lt;/p&gt;
&lt;h2 id=&quot;model-check&quot;&gt;Model Check&lt;/h2&gt;&lt;p&gt;Rounding off this framework is model checking: how do we test that the model is a good one? The bare minimum that we should do is simulate data from the model - it should generate samples whose distribution looks like the actual data itself. If it doesn't, then we have a problem, and need to go back and rework the model structure until it does.&lt;/p&gt;
&lt;p&gt;Could it be that we have a model that only fits the data on hand (overfitting)? Potentially so - and if this is the case, then our best check is to have an &quot;out-of-sample&quot; group. While a train/test/validation split might be useful, the truest test of a model is new data that has been collected.&lt;/p&gt;
&lt;h2 id=&quot;thoughts&quot;&gt;Thoughts&lt;/h2&gt;&lt;p&gt;These three major steps in Bayesian data analysis workflows did not come together until recently; they each seemed disconnected from the others. Perhaps this was just an artefact of how I was learning them. However, I think I've finally come to a unified realization: Estimation is necessary before we can do comparison, and model checking helps us build confidence in the estimation and comparison procedures that we use.&lt;/p&gt;
&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;&lt;p&gt;When doing Bayesian data analysis, the key steps that we're performing are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Estimation&lt;/li&gt;
&lt;li&gt;Comparison&lt;/li&gt;
&lt;li&gt;Model Checking&lt;/li&gt;
&lt;/ol&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2018/7/14/ecdfs/">
    <title type="text">ECDFs</title>
    <id>urn:uuid:dcedf06f-59ec-360f-9711-7e4db71e3ef1</id>
    <updated>2018-07-14T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2018/7/14/ecdfs/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;In my two SciPy 2018 co-taught tutorials, I made the case that ECDFs provide richer information compared to histograms. My main points were:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We can more easily identify central tendency measures, in particular, the median, compared to a histogram.&lt;/li&gt;
&lt;li&gt;We can much more easily identify other percentile values, compared to a histogram.&lt;/li&gt;
&lt;li&gt;We become less susceptible to outliers arising from binning issues.&lt;/li&gt;
&lt;li&gt;It is more difficult to hide multiple modes.&lt;/li&gt;
&lt;li&gt;We can easily identify repeat values.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;What are ECDFs? ECDFs stand for the &quot;empirical cumulative distribution function&quot;, and they map every data point in the dataset to a quantile, which is a number between 0 and 1 that indicates the cumulative fraction of data points smaller than that data point itself.&lt;/p&gt;
&lt;p&gt;To illustrate, let's take a look at the following plots.&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;plt&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Generate a mixture of two normal distributions, but with&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# very few data points.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;mx1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scale&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;mx2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scale&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;mx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;concatenate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mx1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mx2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# one outlier&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;ecdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sort&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax_ecdf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;121&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax_hist&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;122&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax_hist&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;histogram&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;ax_hist&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ecdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax_ecdf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax_ecdf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;ecdf&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class=&quot;output_png output_subarea &quot;&gt;
&lt;img src=&quot;data:image/png;base64,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
&quot;
&gt;
&lt;/div&gt;&lt;p&gt;Let's compare the ECDF and the histogram for this data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Is the central tendency measure easily discoverable?&lt;/strong&gt; We might say that there's some peak at just below the x-axis at just above zero, but is that the mode, median or mean? And what is its exact value? On the other hand, at least the median is easily discoverable on the ECDF: Draw a horizontal line from 0.5 on the y-axis until it crosses a data point, and then drop a line down to the x-axis to get the median value.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Are percentiles easily discoverable?&lt;/strong&gt; It's much clearer that the answer is &quot;yes&quot; for the ECDF, and &quot;no&quot; for the histogram.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What is the value of the potential outlier?&lt;/strong&gt; Difficult to tell on the histogram: it could be anywhere from 4 to 5 (high outlier) and maybe -3 to -4 on the low outlier. On the other hand, just drop a line down from the suspected outliers to the x-axis to read off their values.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Is this a mixture distribution or is this a single Normal distribution?&lt;/strong&gt; If you looked at the histogram, you might be tempted to think that the data are normally distributed with mean 0.5 and standard deviation about 2. However, if you look at the ECDF, it's clear that there are multiple modes, as shown by two or three sigmoidal-like curves. This should give us pause to see if there's a mixture distribution at play here.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Are there repeat values?&lt;/strong&gt; You can't tell in a histogram. However, it's evidently clear on the ECDF scatterplot that there's no repeat values -- they would show up on the plot as vertical stacks of dots. (Repeat-values might be important when working with, say, a zero- or X-inflated distribution.)&lt;/p&gt;
&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;&lt;p&gt;I hope this post showed you why ECDFs contain richer information than histograms. They're taught less commonly than histograms, so people will have a harder time interpreting them at first glance. However, a bit of guidance and orientation will bring out the rich information on the ECDFs.&lt;/p&gt;
&lt;h2 id=&quot;credits&quot;&gt;Credits&lt;/h2&gt;&lt;p&gt;I credit Justin Bois (Caltech) for teaching me about ECDFs, and Hugo Bowne-Anderson (DataCamp) for reinforcing the idea.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2018/6/17/git-tip-apply-a-patch/">
    <title type="text">Git Tip: Apply a Patch</title>
    <id>urn:uuid:91a163e7-561a-3b01-937e-dd71a96974c3</id>
    <updated>2018-06-17T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2018/6/17/git-tip-apply-a-patch/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;I learned a new thing this weekend: we apparently can apply a patch onto a branch/fork using &lt;code&gt;git apply [patchfile]&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;There's a few things to unpack here. First off, what's a &lt;code&gt;patchfile&lt;/code&gt;?&lt;/p&gt;
&lt;p&gt;The long story cut short is that a &lt;code&gt;patchfile&lt;/code&gt; is nothing more than a plain text file that contains all information about &lt;code&gt;diffs&lt;/code&gt; between one commit and another. If you've ever used the &lt;code&gt;git diff&lt;/code&gt; command, you'll know that it will output a &lt;code&gt;diff&lt;/code&gt; between the current state of a repository, and the last committed state. Let's take a look at an example.&lt;/p&gt;
&lt;p&gt;Say we have a file, called &lt;code&gt;my_file.txt&lt;/code&gt;. In a real world example, this would be parallel to, say, a &lt;code&gt;.py&lt;/code&gt; module that you've written. After a bunch of commits, I have a directory structure that looks like this:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ ls
total &lt;span class=&quot;m&quot;&gt;8&lt;/span&gt;
drwxr-xr-x   &lt;span class=&quot;m&quot;&gt;4&lt;/span&gt; ericmjl  staff   128B Jun &lt;span class=&quot;m&quot;&gt;17&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;:26 ./
drwx------@ &lt;span class=&quot;m&quot;&gt;19&lt;/span&gt; ericmjl  staff   608B Jun &lt;span class=&quot;m&quot;&gt;17&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;:26 ../
drwxr-xr-x  &lt;span class=&quot;m&quot;&gt;12&lt;/span&gt; ericmjl  staff   384B Jun &lt;span class=&quot;m&quot;&gt;17&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;:27 .git/
-rw-r--r--   &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt; ericmjl  staff    68B Jun &lt;span class=&quot;m&quot;&gt;17&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;:26 my_file.txt
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The contents of &lt;code&gt;my_file.txt&lt;/code&gt; are as follows:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ cat my_file.txt
Hello! This is a text file.

I have some text written inside here.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, let's say I edit the text file by adding a new line and removing one line.&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ cat my_file.txt
Hello! This is a text file.

I have some text written inside here.

This is a new line!
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If I looked at the &quot;diff&quot; between the current state of the file and the previous committed state of the file:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ git diff my_file.txt
&lt;span class=&quot;gh&quot;&gt;diff --git a/my_file.txt b/my_file.txt&lt;/span&gt;
&lt;span class=&quot;gh&quot;&gt;index a594a37..d8602e1 100644&lt;/span&gt;
&lt;span class=&quot;gd&quot;&gt;--- a/my_file.txt&lt;/span&gt;
&lt;span class=&quot;gi&quot;&gt;+++ b/my_file.txt&lt;/span&gt;
&lt;span class=&quot;gu&quot;&gt;@@ -1,4 +1,4 @@&lt;/span&gt;
 Hello! This is a text file.

&lt;span class=&quot;gd&quot;&gt;-I have some text written inside here.&lt;/span&gt;
&lt;span class=&quot;gi&quot;&gt;+This is a new line!&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;While this may look intimidating at first, the key thing that one needs to look at is the &lt;code&gt;+&lt;/code&gt; and &lt;code&gt;-&lt;/code&gt;. The &lt;code&gt;+&lt;/code&gt; signals that there is an addition of one line, and the &lt;code&gt;-&lt;/code&gt; signals the removal of one line.&lt;/p&gt;
&lt;p&gt;Turns out, I can export this as a file.&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ git diff my_file.txt &amp;gt; /tmp/patch1.txt
$ cat /tmp/patch1.txt
diff --git a/my_file.txt b/my_file.txt
index a594a37..d8602e1 &lt;span class=&quot;m&quot;&gt;100644&lt;/span&gt;
--- a/my_file.txt
+++ b/my_file.txt
@@ -1,4 +1,4 @@
 Hello! This is a text file.

-I have some text written inside here.
+This is a new line!
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, let's simulate the scenario where I accidentally discarded those changes in the repository. A real-world analogue happened to me while contributing to CuPy: I had a really weird commit history, and couldn't remember how to rebase, so I exported the patch from my GitHub pull request (more on this later) and applied it following the same conceptual steps below.&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ git checkout -- my_file.txt
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, the repository is in a &quot;cleaned&quot; state -- there are no changes made:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ git status
On branch master
nothing to commit, working tree clean
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Since I have saved the diff as a file, I can apply it onto my project:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ git apply /tmp/patch1.txt
$ git status
On branch master
Changes not staged &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; commit:
  &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;use &lt;span class=&quot;s2&quot;&gt;&amp;quot;git add &amp;lt;file&amp;gt;...&amp;quot;&lt;/span&gt; to update what will be committed&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;use &lt;span class=&quot;s2&quot;&gt;&amp;quot;git checkout -- &amp;lt;file&amp;gt;...&amp;quot;&lt;/span&gt; to discard changes in working directory&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

        modified:   my_file.txt

no changes added to commit &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;use &lt;span class=&quot;s2&quot;&gt;&amp;quot;git add&amp;quot;&lt;/span&gt; and/or &lt;span class=&quot;s2&quot;&gt;&amp;quot;git commit -a&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Looking at the diff again, I've recovered the changes that were lost!&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ git diff
&lt;span class=&quot;gh&quot;&gt;diff --git a/my_file.txt b/my_file.txt&lt;/span&gt;
&lt;span class=&quot;gh&quot;&gt;index a594a37..d8602e1 100644&lt;/span&gt;
&lt;span class=&quot;gd&quot;&gt;--- a/my_file.txt&lt;/span&gt;
&lt;span class=&quot;gi&quot;&gt;+++ b/my_file.txt&lt;/span&gt;
&lt;span class=&quot;gu&quot;&gt;@@ -1,4 +1,4 @@&lt;/span&gt;
 Hello! This is a text file.

&lt;span class=&quot;gd&quot;&gt;-I have some text written inside here.&lt;/span&gt;
&lt;span class=&quot;gi&quot;&gt;+This is a new line!&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Don't forget to commit and push!&lt;/p&gt;
&lt;h2 id=&quot;how-to-export-a-patch-from-github?&quot;&gt;How to export a patch from GitHub?&lt;/h2&gt;&lt;p&gt;I mentioned earlier that I had exported the patch file from GitHub and then applied it on a re-forked repository. How does one do that? It's not as hard as you think.&lt;/p&gt;
&lt;p&gt;Here's the commands below with comments.&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Download the patch from the pull request URL.&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Replace curly-braced elements with the appropriate names.&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Export it to /tmp/patch.txt.&lt;/span&gt;
$ wget https://github.com/&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;repo_owner&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;/&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;repo&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;/pull/&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;pr_number&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;.patch -O /tmp/patch.txt

&lt;span class=&quot;c1&quot;&gt;# Now, apply the patch to your project&lt;/span&gt;
$ git apply /tmp/patch.txt
&lt;/pre&gt;&lt;/div&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2018/6/5/my-latent-dissatisfaction-with-modern-ml/">
    <title type="text">My Latent Dissatisfaction with Modern ML</title>
    <id>urn:uuid:2e5732c7-78fb-320e-a57e-6d2164ced345</id>
    <updated>2018-06-05T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2018/6/5/my-latent-dissatisfaction-with-modern-ml/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;It took reading Judea Pearl's &quot;The Book of Why&quot;, and Jonas Peters' mini-course on causality, for me to finally figure out why I had this lingering dissatisfaction with modern machine learning. It's because modern machine learning (deep learning included) is most commonly used as a tool in the service of finding correlations, and is not concerned with understanding systems.&lt;/p&gt;
&lt;p&gt;Perhaps this is why Pearl writes of modern ML as basically being &quot;curve fitting&quot;. I tend to believe he didn't write those words in a dismissive way, though I might be wrong about it. Regardless, I think there is an element of truth to that statement.&lt;/p&gt;
&lt;p&gt;Linear models seek a linear combination of correlations between input variables and their targets. Tree-based models essentially seek combinations of splits in the data, while deep learning models are just stacked compositions of linear models with nonlinear functions applied to their outputs. As Keras author Francois Chollet wrote, &lt;a href=&quot;https://blog.keras.io/the-limitations-of-deep-learning.html&quot;&gt;deep learning can be thought of as basically geometric transforms of data from one data manifold to another&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;(For convenience, I've divided the ML world into linear models, tree-based models, and deep learning models. Ensembles, like Random Forest, are just that: ensembles composed of these basic models.)&lt;/p&gt;
&lt;p&gt;Granted, curve fitting is actually very useful: much of image deep learning has found pragmatic use: image search, digital pathology, self-driving cars, and more. Yet, in none of these models is the notion of causality important. This is where these models are dissatisfying: they do not provide the tools to help us interrogate these questions in a structured fashion. I think it's reasonable to say that these models are essentially concerned with conditional probabilities. As written by Ferenc Huszár, &lt;a href=&quot;http://www.inference.vc/untitled/&quot;&gt;conditional probabilities are different from interventional probabilities&lt;/a&gt; (ok, I mutilated that term).&lt;/p&gt;
&lt;p&gt;Humans are innately wired to recognize and ask questions about causality; consider it part of our innate makeup. That is, of course, unless that has been drilled out of our minds by our life experiences. (I know of a person who insists that causes do not exist. An extreme Hume-ist, I guess? As I'm not a student of philosophy much, I'm happy to be corrected on this point.) As such, I believe that part of being human involves asking the question, &quot;Why?&quot; (and its natural extension, &quot;How?&quot;). Yet, modern ML is still stuck at the question of, &quot;What?&quot;&lt;/p&gt;
&lt;p&gt;To get at why and how, we test our understanding of a system by perturbing it (i.e. intervening in it), or asking about &quot;what if&quot; scenarios (i.e. thinking about counterfactuals). In the real  world of biological research (which I'm embedded in), we call this &quot;experimentation&quot;. Inherent in a causal view of the world is a causal model. In causal inference, these things are structured and expressed mathematically, and we are given formal procedures for describing an intervention and thinking about counterfactual scenarios. From what I've just learned (baby steps at the moment), these are the basic ingredients, and their mathematical mappings:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Causal model: a directed, acyclic graph&lt;ul&gt;
&lt;li&gt;Variables: nodes in a graph&lt;/li&gt;
&lt;li&gt;Relationships: structured causal model's equations (math transforms of incoming variables with a noise distribution added on top, embedded in each node)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Interventions: removal of edges in a graph (&quot;do-calculus&quot;)&lt;/li&gt;
&lt;li&gt;Counterfactuals: set causal model based on observation, then perform do-calculus.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Having just learned this, I think there's a way out of this latent dissatisfaction that I have with modern ML. A neat thing about ML methods is that we can use them as tools to help us better identify the important latent factors buried inside our (observational) data, which we can use to construct a better model of our data generating process. Better yet, we can express the model in a structured and formal sense, which would expose our assumptions more explicitly for critique and reasoning. Conditioned on that, perhaps we may be able to write better causal models of the world!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2018/5/26/causal-modelling/">
    <title type="text">Causal Modelling</title>
    <id>urn:uuid:0d2239fc-b359-3a37-abfa-d50e0d2a6f01</id>
    <updated>2018-05-26T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2018/5/26/causal-modelling/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;Finally, I have finished Judea Pearl's latest work &lt;a href=&quot;https://www.amazon.com/Book-Why-Science-Cause-Effect-ebook/dp/B075CR9QBJ/ref=mt_kindle?_encoding=UTF8&amp;amp;me=&quot;&gt;&quot;The Book of Why&quot;&lt;/a&gt;! Having read it, I have come to appreciate how much work had to go on in order to formalize the very intuitions that we have for causal reasoning into essentially a modelling language.&lt;/p&gt;
&lt;p&gt;&quot;The Book of Why&quot; is geared towards the layman reader. Thus, unlike a textbook, it does not contain &quot;simplest complex examples&quot; that a reader can walk through and do calculations by hand (or through simulation). Thankfully, there is a &lt;a href=&quot;https://www.youtube.com/playlist?list=PLW01hpWnEtbTcuY0a0jhZyanHX3GPImAy&quot;&gt;lecture series by Jonas Peters&lt;/a&gt;, organized by the Broad Institute and held at MIT, that are available freely online.&lt;/p&gt;
&lt;p&gt;From just viewing the first of the four lectures, I am thoroughly enjoying Jonas' explanations of the core ideas in causal modelling. Indeed, Jonas is a very talented lecturer! He builds up the ideas from simple examples, finally culminating in a &quot;simple complex example&quot; that we can simulate on a computer. Having just freshly read &quot;The Book of Why&quot; also helps immensely; it's also clear to me that people in the world of causal modelling are very much familiar with the same talking points. For those interested in learning more about causal modelling, I highly recommend both the book and the lecture series!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2018/5/6/model-baselines-are-important/">
    <title type="text">Model Baselines Are Important</title>
    <id>urn:uuid:f3d55548-fb27-39e0-bc0d-86413913fe59</id>
    <updated>2018-05-06T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2018/5/6/model-baselines-are-important/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;For any problem that we think is machine learnable, having a sane baseline is really important. It is even more important to establish them early.&lt;/p&gt;
&lt;p&gt;Today at ODSC, I had a chance to meet both Andreas Mueller and Randy Olson. Andreas leads &lt;a href=&quot;http://scikit-learn.org/stable/&quot;&gt;&lt;code&gt;scikit-learn&lt;/code&gt;&lt;/a&gt; development, while Randy was the lead developer of &lt;a href=&quot;https://github.com/EpistasisLab/tpot&quot;&gt;TPOT&lt;/a&gt;, an AutoML tool. To both of them, I told a variation of the following story:&lt;/p&gt;
&lt;p&gt;I had spent about 1.5 months building and testing a graph convolutions neural network model to predict RNA cleavage by an enzyme. I was suffering from a generalization problem - this model class would never generalize beyond the training samples for my problem on hand, even though I saw the same model class perform admirably well for small molecules and proteins.&lt;/p&gt;
&lt;p&gt;Together with an engineer at NIBR, we brainstormed a baseline with some simple features, and threw a random forest model at it. Three minutes later, after implementing everything, we had a model that generalized and outperformed my implementation of graph CNNs. Three days later, we had an AutoML (TPOT) model that beat the random forest. After further discussion, we realize then that the work that we did is sufficiently publishable even without the fancy graph CNNs.&lt;/p&gt;
&lt;p&gt;I think there’s a lesson in establishing baselines and MVPs early on!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2018/3/30/consolidate-your-scripts-using-click/">
    <title type="text">Consolidate your scripts using click</title>
    <id>urn:uuid:beec04f1-f5b9-3932-9d12-92337ec70fbb</id>
    <updated>2018-03-30T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2018/3/30/consolidate-your-scripts-using-click/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;&lt;p&gt;&lt;code&gt;click&lt;/code&gt; is amazing! It's a Python package that allows us to add a command-line interface (CLI) to our Python scripts easily. This blog post is a data scientist-oriented post on how we can use &lt;code&gt;click&lt;/code&gt; to build useful tools for ourselves. In this blog post, I want to focus on how we can better organize our scripts.&lt;/p&gt;
&lt;p&gt;I have found myself sometimes writing custom scripts to deal with custom data transforms. Having them refactored out into a library of modular functions can really help with maintenance. However, I still end up with multiple scripts that might not have a naturally logical organization... except for the fact that they are scripts that I run from time to time! Rather than have them scattered in multiple places, why not have them put together into a single &lt;code&gt;.py&lt;/code&gt; file, with options that are callable from the command line instead?&lt;/p&gt;
&lt;h2 id=&quot;template&quot;&gt;Template&lt;/h2&gt;&lt;p&gt;Here's a template for organizing all those messy scripts using  &lt;code&gt;click&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;click&lt;/span&gt;


&lt;span class=&quot;nd&quot;&gt;@click.group&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;pass&lt;/span&gt;


&lt;span class=&quot;nd&quot;&gt;@main.command&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;script1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    Makes stuff happen.&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# do stuff that was originally in script 1&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;click&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;echo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;script 1 was run!&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# click.echo is recommended by the click authors.&lt;/span&gt;


&lt;span class=&quot;nd&quot;&gt;@main.command&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;script2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;Makes more stuff happen.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# do stuff that was originally in script 2.&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;script 2 was run!&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# we can run print instead of click.echo as well!&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;vm&quot;&gt;__name__&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;__main__&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;cli&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&quot;how-to-use&quot;&gt;How to use&lt;/h2&gt;&lt;p&gt;Let's call this new meta-script &lt;code&gt;jobs.py&lt;/code&gt;, and make it executable.&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ chmod +x jobs.py
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To execute it at the command line, we now a help command for free:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ ./jobs.py --help
Usage: jobs.py &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;OPTIONS&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt; COMMAND &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;ARGS&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;...

Options:
  --help  Show this message and exit.

Commands:
  script1  Makes stuff happen.
  script2  Makes more stuff happen.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We can also use just one script with varying commands to control the execution of what was originally two different &lt;code&gt;.py&lt;/code&gt; files.&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ ./jobs.py script1
script &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt; was run!
$ ./jobs.py script2
script &lt;span class=&quot;m&quot;&gt;2&lt;/span&gt; was run!
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Instead of versioning multiple &lt;code&gt;.py&lt;/code&gt; files, we now only have to keep track of one file where all non-standard custom stuff goes!&lt;/p&gt;
&lt;h2 id=&quot;details&quot;&gt;Details&lt;/h2&gt;&lt;p&gt;Here's what's going on under the hood.&lt;/p&gt;
&lt;p&gt;With the decorator &lt;code&gt;@click.group()&lt;/code&gt;, we have exposed the &lt;code&gt;main()&lt;/code&gt; function from the command line as a &quot;group&quot; of commands that are callable from the command line. What this does is then &quot;wrap&quot; the &lt;code&gt;main()&lt;/code&gt; function (somehow), such that now it can be used to decorate another function (in our case, &lt;code&gt;script1&lt;/code&gt; and &lt;code&gt;script2&lt;/code&gt;) using the decorator syntax &lt;code&gt;@main.command()&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&quot;recap&quot;&gt;Recap&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Consolidate disparate Python scripts into a single &lt;code&gt;.py&lt;/code&gt;  file, wrapping them inside a callable function.&lt;/li&gt;
&lt;li&gt;Use &lt;code&gt;click&lt;/code&gt; to expose them to the end-user (yourself, or others) at the command line.&lt;/li&gt;
&lt;/ul&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2018/2/28/lessons-learned-and-reinforced-from-writing-my-own-deep-learning-package/">
    <title type="text">Lessons learned and reinforced from writing my own deep learning package</title>
    <id>urn:uuid:390270b7-9d7b-306c-9ead-2617aac7af0a</id>
    <updated>2018-02-28T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2018/2/28/lessons-learned-and-reinforced-from-writing-my-own-deep-learning-package/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;At work, I’ve been rolling my own deep learning package to experiment with graph convolutional neural networks. I did this because in graph-centric deep learning, an idea I picked up from this paper, the inputs, convolution kernels, and much more, are being actively developed, and the standard APIs don’t fit with this kind of data.&lt;/p&gt;
&lt;p&gt;Here’s lessons I learned (and reinforced) while doing this.&lt;/p&gt;
&lt;h2 id=&quot;autograd-is-an-amazing-package&quot;&gt;&lt;code&gt;autograd&lt;/code&gt; is an amazing package&lt;/h2&gt;&lt;p&gt;I am using &lt;code&gt;autograd&lt;/code&gt; to write my neural networks. &lt;code&gt;autograd&lt;/code&gt; provides a way to automatically differentiate &lt;code&gt;numpy&lt;/code&gt; code. As long as I write the forward computation up to the loss function, &lt;code&gt;autograd&lt;/code&gt; will be able to differentiate the loss function w.r.t. all of the parameters used, thus providing the direction to move parameters to minimize the loss function.&lt;/p&gt;
&lt;h2 id=&quot;deep-learning-is-nothing-more-than-chaining-elementary-differentiable-functions&quot;&gt;Deep learning is nothing more than chaining elementary differentiable functions&lt;/h2&gt;&lt;p&gt;Linear regression is nothing more than a dot product of features with weights and adding bias terms. Logistic regression just chains the logistic function on top of that. Anything deeper than that is what we might call a neural network.&lt;/p&gt;
&lt;p&gt;One interesting thing that I've begun to ponder is the shape of the loss function, and how it changes when I change model architecture, activation functions, and more. I can't speak intelligently about it right now, but from observing the training performance live (I update a plot of predictions vs. actual values at the end of &lt;code&gt;x&lt;/code&gt; training epochs), different combinations of activation functions seem to cause different behaviours of the outputs, and there's no first-principles reason why that I can think of. All-told, pretty interesting :).&lt;/p&gt;
&lt;h2 id=&quot;defining-a-good-api-is-hard-work&quot;&gt;Defining a good API is hard work&lt;/h2&gt;&lt;p&gt;There are design choices that go into the API design. I first off wanted to build something familiar, so I chose to emulate the functional API of Keras and PyTorch and Chainer. I also wanted composability, in which I can define modules of layers and chain them together, so I opted to use Python objects and to take advantage of their &lt;code&gt;__call__&lt;/code&gt; method to achieve both goals. At the same time, &lt;code&gt;autograd&lt;/code&gt; imposes a constraint in that I need to have functions differentiable with respect to their first argument, an array of parameters. Thus, I had to make sure the weights and biases are made transparently available for &lt;code&gt;autograd&lt;/code&gt; to differentiate. As a positive side effect, it means I can actually inspect the parameters dictionary quite transparently.&lt;/p&gt;
&lt;h2 id=&quot;optimizing-for-speed-is-a-very-hard-thing&quot;&gt;Optimizing for speed is a very hard thing&lt;/h2&gt;&lt;p&gt;Even though I'm doing my best already with matrix math (and hopefully getting better at mastering 3-dimensional and higher matrix algebra), in order to keep my API clean and compatible with &lt;code&gt;autograd&lt;/code&gt; (meaning no sparse arrays), I have opted to use lists of &lt;code&gt;numpy&lt;/code&gt; arrays.&lt;/p&gt;
&lt;h2 id=&quot;graph-convolutions-have-a-connection-to-network-propagation&quot;&gt;Graph convolutions have a connection to network propagation&lt;/h2&gt;&lt;p&gt;I will probably explore this a bit more deeply in another blog post, but yes, as I explore the math involved in doing graph convolutions, I'm noticing that there's a deep connection there. The short story is basically &quot;convolutions propagate information across nodes&quot; in almost exactly the same way as &quot;network propagation methods share information across nodes&quot;, through the use of a kernel defined by the adjacency matrix of a graph.&lt;/p&gt;
&lt;p&gt;Ok, that's a lot of jargon, but I promise I will explore this topic at a later time.&lt;/p&gt;
&lt;h2 id=&quot;open-sourcing&quot;&gt;Open Sourcing&lt;/h2&gt;&lt;p&gt;I'm an avid open source fan. Lots of my work builds on it. However, because this &quot;neural networks on graphs&quot; work is developed on company time and for company use, this will very likely be the first software project that I send to Legal to evaluate whether I can open source/publish it or not -- I'll naturally have to make my strongest case for open sourcing the code base (e.g. ensuring no competitive intelligence is leaked), but eventually will still have to defer to them for a final decision.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2018/2/26/joy-from-teaching/">
    <title type="text">Joy from teaching</title>
    <id>urn:uuid:5c9209f0-4531-3b75-b34a-ceb067bc526c</id>
    <updated>2018-02-26T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2018/2/26/joy-from-teaching/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;It always brings me joy to see others benefit from what I can offer.&lt;/p&gt;
&lt;p&gt;Thanks for &lt;a href=&quot;https://www.linkedin.com/feed/update/urn:li:activity:6373939725656563713&quot;&gt;sharing the fruits of your journey&lt;/a&gt; on LinkedIn, Umar!&lt;/p&gt;
&lt;p&gt;Also a big thanks to the others who have finished the course! I hope you have enjoyed the learning journey, and were able to find problems to apply your newly-gained knowledge!&lt;/p&gt;
&lt;p&gt;With big thanks to &lt;a href=&quot;https://www.datacamp.com/&quot;&gt;DataCamp&lt;/a&gt; as well, for the development of their platform, enabling us to do teaching even outside of the academy! (Special shout-out to Hugo Bowne-Anderson and Yashas Roy, with whom I've personally partnered with to make the content go live.)&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2018/2/25/annotating-code-tests-and-selectively-running-tests/">
    <title type="text">Annotating code tests and selectively running tests</title>
    <id>urn:uuid:58ec5548-622e-3171-a4e9-74776a93d076</id>
    <updated>2018-02-25T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2018/2/25/annotating-code-tests-and-selectively-running-tests/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;I just learned about a neat trick when using &lt;code&gt;pytest&lt;/code&gt; - the ability to &quot;mark&quot; tests with metadata, and the ability to selectively run groups of marked tests.&lt;/p&gt;
&lt;p&gt;Here's an example:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pytest&lt;/span&gt;

&lt;span class=&quot;nd&quot;&gt;@pytest.mark.slow&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# annotate it as a &amp;quot;slow&amp;quot; test&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;test_that_runs_slowly&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;....&lt;/span&gt;

&lt;span class=&quot;nd&quot;&gt;@pytest.mark.slow&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# annotate test as a &amp;quot;slow&amp;quot; test.&lt;/span&gt;
&lt;span class=&quot;nd&quot;&gt;@pytest.mark.integration&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# annotate test as being an &amp;quot;integration&amp;quot; test&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;test_that_does_integration&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;....&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;What's really cool here is that I can selectively run slow tests or selectively run integration tests:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ py.test -m &lt;span class=&quot;s2&quot;&gt;&amp;quot;slow&amp;quot;&lt;/span&gt;   &lt;span class=&quot;c1&quot;&gt;# only runs &amp;quot;slow&amp;quot; tests&lt;/span&gt;
$ py.test -m &lt;span class=&quot;s2&quot;&gt;&amp;quot;integration&amp;quot;&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# only runs &amp;quot;integration&amp;quot; tests&lt;/span&gt;
$ py.test -m &lt;span class=&quot;s2&quot;&gt;&amp;quot;not integration&amp;quot;&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# only runs tests that are not &amp;quot;integration&amp;quot; tests.&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2018/2/21/nxviz-first-pr-merged/">
    <title type="text">nxviz first PR merged!</title>
    <id>urn:uuid:12691a84-54fb-355f-a79b-2f656232a743</id>
    <updated>2018-02-21T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2018/2/21/nxviz-first-pr-merged/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;With thanks to &lt;a href=&quot;https://twitter.com/KassnerNora&quot;&gt;@KassnerNora&lt;/a&gt;, &lt;a href=&quot;https://github.com/ericmjl/nxviz&quot;&gt;&lt;code&gt;nxviz&lt;/code&gt;&lt;/a&gt; has a new feature! Now, it is possible to specify the data to use to colour edges. Previously, this was available in the API but was not working as I had not the time to implement it.&lt;/p&gt;
&lt;p&gt;Now, with &lt;a href=&quot;https://github.com/ericmjl/nxviz/pull/260&quot;&gt;Nora's PR&lt;/a&gt; merged in, users can declare their edges to be coloured by some edge metadata key! In addition to that, Nora included a &lt;code&gt;seaborn&lt;/code&gt; colour palette that may be useful for drawing other edge colours. The API remains a declarative API, giving users an easy way to specify how they want rational graph plots to be made.&lt;/p&gt;
&lt;p&gt;Thank you for the contribution, Nora!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2018/2/20/deep-learning-and-the-importance-of-a-good-teacher/">
    <title type="text">Deep Learning and the Importance of a Good Teacher</title>
    <id>urn:uuid:06ad28dc-a618-3792-a8d5-1cc5645cb01b</id>
    <updated>2018-02-20T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2018/2/20/deep-learning-and-the-importance-of-a-good-teacher/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;I can’t emphasize this enough - someone who teaches well can really open their student’s minds.&lt;/p&gt;
&lt;p&gt;On my journey in to deep learning and now graph convolutions, &lt;a href=&quot;../../../../../blog/2018/2/20/deep-learning-and-the-importance-of-a-good-teacher/www.cs.toronto.edu/~duvenaud/&quot;&gt;David Duvenaud&lt;/a&gt; (currently a professor at the University of Toronto) simultaneously taught me, a newcomer to deep learning, the basics of deep learning and the mechanics behind the graph convolutional neural network he and his colleagues had just published. The key insight he passed on to me was that deep learning was nothing more than chaining differentiable functions together. Many times I’d ask him, “so does this mean I can do that operation?”, and the answer would usually be “yeah, why not?”.&lt;/p&gt;
&lt;p&gt;Knowing this point has made me realize how flexible deep learning really is. Once I got under the hood of what deep learning really was, then I realized that actually, DL is all about chaining together math functions one after another. Best part is, we get to define what those math functions are!&lt;/p&gt;
&lt;p&gt;Knowing this has also helped me when I read new DL papers. It’s now a lot easier to for me to tell when a research group has come up with something very different from the rest of the pack as opposed to advancing existing methods.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2018/2/13/data-scientists-need-to-write-good-apis/">
    <title type="text">Data scientists need to write good APIs</title>
    <id>urn:uuid:53402dcb-b8ee-3f08-923d-b30a8e94a285</id>
    <updated>2018-02-13T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2018/2/13/data-scientists-need-to-write-good-apis/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;I had a breakthrough in my work today. This was not some scientific epiphany, but just breaking through a wall in my progress. Today's breakthrough  was totally enabled by writing my class definitions in a way that made sense, and by writing class methods that enabled me to express my ideas in a literate fashion.&lt;/p&gt;
&lt;p&gt;Logical class definitions and methods, refactored functions... these should be reflexive habits, but unfortunately, this isn't always the case with data science. We get so caught up in writing the code to make that plot that we forget to refactor out so that the block of code isn't brittle. But that brittle code means that my future self will loathe my current self for not writing that code robustly.&lt;/p&gt;
&lt;p&gt;In other words, write good APIs.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2018/2/7/bayesian-inference-and-testing-sets/">
    <title type="text">Bayesian Inference &amp; Testing Sets</title>
    <id>urn:uuid:ef48ae04-9b7a-3b04-a221-bf4e0f46d296</id>
    <updated>2018-02-07T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2018/2/7/bayesian-inference-and-testing-sets/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;This topic recently came up again on the &lt;a href=&quot;https://discourse.pymc.io/t/do-we-need-a-testing-set/759/5&quot;&gt;PyMC3 discourse&lt;/a&gt;. I had an opportunity to further clarify what I was thinking about when I first uttered the train/test split comment &lt;a href=&quot;https://www.youtube.com/watch?v=s0S6HFdPtlA&quot;&gt;at PyData NYC&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;After a little while, my thoughts for a layperson are a bit clearer, and I thought I'd re-iterate them here.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Model specification uncertainty: Did we get the conditional relationships correct? Did we specify enough of the explanatory variables?&lt;/li&gt;
&lt;li&gt;Model parameter uncertainty: Given a model, can we quantify the uncertainty in the parameter values?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These are different uncertainties to deal with. We must be clear: where we are pretty sure about the model spec, Bayesian inference is about quantifying the uncertainty in the parameter values. Under this paradigm, if we use more data, we get narrower posterior distributions, and if we use less data, we get wider posterior distributions. If we split the data, we're just feeding in fewer data points to the model; if we don't, then we're just feeding in more data points.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2018/2/6/pycon-program-committee-review/">
    <title type="text">PyCon Program Committee Review</title>
    <id>urn:uuid:017a77c3-861d-34cd-82ec-12512e56f66f</id>
    <updated>2018-02-06T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2018/2/6/pycon-program-committee-review/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;This year, I participated in the PyCon 2018 program committee (ProgCom). Though it was my second time doing it, it nonetheless was an eye-opening experience. This was because in contrast to last year, when I was writing my thesis and thus couldn't follow through on both rounds of review, this year I was. I’d like to write a bit about my thoughts on the process, with three-fold goals:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;To document my experience reviewing this year's PyCon talks.&lt;/li&gt;
&lt;li&gt;To bring a little bit more transparency to the process. (Proposal authors, understandably, might feel like the process is opaque.)&lt;/li&gt;
&lt;li&gt;To indirectly encourage others to participate in the process by demystifying what goes on in the mind of a reviewer.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Before I go on, though I do want to make a few points clear on what I will not be doing in this blog post.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;I will not be commenting on any particular proposal.&lt;/li&gt;
&lt;li&gt;I will not be mentioning specific reviewers' names and what they commented on specific proposals.&lt;/li&gt;
&lt;li&gt;I will not be offering tips for making your proposal succeed next year - that will depend on what next year’s program committee is looking for.&lt;/li&gt;
&lt;li&gt;I will not be describing the review process in detail, as this will be dealt with by an &quot;official&quot; blog post from the PSF.&lt;/li&gt;
&lt;li&gt;This blog post is definitely not an invitation to review your current round or next round proposal, as with a full-time job, I currently don't have the bandwidth for that.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Ready to read on? Let's go!&lt;/p&gt;
&lt;h2 id=&quot;stage-1:-scoring&quot;&gt;Stage 1: Scoring&lt;/h2&gt;&lt;p&gt;The process for selecting talks is a two-stage process. In the first stage, we are rating the talks qualitatively (ordinally) on six criteria (for which I will defer to the official PSF blog post for details when it comes out). &lt;strong&gt;At this stage, we are blinded to a speaker's name and prior experience&lt;/strong&gt;, as we want to review only the content on its merits. I'd like to give my thoughts on a few of the criteria below.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;On &quot;code of conduct&quot;&lt;/strong&gt;, this criteria ensures that there's no disparaging of sub-communities. I generally ding'ed talks that carried any hints of negativity, as I'd like to see PyCon be a positive force in the community. A few proposals were easily misconstrued as being negative, even though they weren't in substance; we tried our best to communicate this concern to proposal authors. That said, most talks are technical in nature, thus this criteria was not really an issue.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;On &quot;completeness&quot;&lt;/strong&gt;, it's really as about seeing whether an author demonstrated the meticulousness in making it easy for us to review. Timings were super important to me, to gauge whether I thought the talk proposer had over- or under-proposed content. I also sought detail to evaluate the accuracy of content and connect the sub-points into the author's overall message. The additional detail made it easier for me to champion a talk in the later stages.&lt;/p&gt;
&lt;p&gt;A few authors, while in communication with the ProgCom, flat-out refused to add in details after I had messaged them requesting details, citing other conferences' practices. Unfortunately, each conference is going to be slightly different in how they operate, and that ongoing dialogue can indirectly influence reviewers' perception and therefore the proposal's score. We're all human, and if a talk proposer comes off as uncooperative, especially when we provide an ongoing opportunity to engage in dialogue, it just makes it tougher to justify their elevated presence as a speaker at a conference that emphasizes cooperation and community.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;On &quot;coherence&quot;&lt;/strong&gt;, this point somewhat overlaps with &quot;completeness&quot;, in that the accuracy of content can help boost this criteria. A good talk would cover one sufficiently focused topic for 30 minutes (or 45 minutes if request in sufficient depth.&lt;/p&gt;
&lt;p&gt;I think that what constitutes &quot;sufficient&quot; depends on our state of knowledge. For example, in data science, I would view it as not sufficient to speak on &quot;how to do a data analysis&quot;, as it is now quite clear that each analysis is quite different, and the generalizable principles are too vague to be of use for a listener. On the other hand, speaking about solving a particular (data) science problem can be illuminating with solid take-aways for an audience members.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Other non-explicit criteria can affect the perception of a proposal&lt;/strong&gt;. At this stage, we're doing an ongoing dialogue with proposal authors, up till the submission deadline. Thus, as mentioned above, an authors' cooperativeness can affect our perception of the proposal, particularly on the &quot;code of conduct&quot; criteria - it'll affect whether we can trust an author's ability to adhere to the code of conduct. Additionally, having good English grammar can affect how readable the proposal is. Finally, there is something qualitatively different about a proposal by an author who is deeply passionate about their topic and believes they have something important to say, in contrast to a proposal author who is merely trying to fulfill PyCon selection criteria. I want to hear from the former, not the latter.&lt;/p&gt;
&lt;h2 id=&quot;stage-2:-selection&quot;&gt;Stage 2: Selection&lt;/h2&gt;&lt;p&gt;At this stage, we looked at the composition of talks, and proposed &quot;buckets&quot; of topics that the talks covered. This means that the &quot;topics&quot; are defined by what the community puts together. In total, there were &amp;gt;60+ groups of talks.&lt;/p&gt;
&lt;p&gt;In this stage, we are basically looking for one talk to emerge from each group. The unfortunate reality is that there will be some groups that are small (3 talks), and some groups that are large (&amp;gt;15 talks), and many groups in between, meaning not every talk has an &quot;equal&quot; chance of making it through. This is entirely dependent on what the community has submitted, though, so there's no easy way to control for this.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;At this stage, we are also debating what kind of conference we want.&lt;/strong&gt; This is where it gets super interesting, and the idiosyncracies of each ProgCom member will show through. Here's a sampling of questions that went through my mind.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Do we want one in which mature talks are rehashed from the three other conferences it's already been at, or do we want new talks to come to prominence?&lt;/strong&gt; It's not an easy decision - for some topics we favoured new ones, and for others we favoured mature talks.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Do we want experienced speakers or do we want to encourage new ones to come up?&lt;/strong&gt; This one is relatively easy - we hard-limited speakers to one talk; some speakers proposed 5 or 6 of them, but we only take one, to enable other speakers to be present. This gives more room for newcomers to speak.&lt;/p&gt;
&lt;p&gt;For me, there were some experienced speakers giving new talks, and I knew they'd be able to pull it off given their track record; I favoured them on both merits: experience and topical novelty. On the other hand, there were experienced speakers taking a single talk around the conference circuit - for these talks, if they were publicly available online, I didn't favour them, and communicated that to other members of the ProgCom team.&lt;/p&gt;
&lt;p&gt;Amongst new speakers (and relatively unknown speakers) at PyCon, I was looking for slide decks to evaluate their message, and recordings of other talks that they had done. Knowing that there's a catch-22 problem (new speaker approximately means no recordings), I also tried looking in greater detail at the proposal for hints and clues that the speaker knew in great depth what they were speaking about, and were confident about delivering it. (Generally, good amounts of non-jargony detail highlight a speaker's capacity for mastery and communication skills simultaneously.)&lt;/p&gt;
&lt;p&gt;My advocacy for new speakers and new talks, and advocacy against talks that had been given at other conferences, particularly those which had a public recording available online, stems from my desire to see PyCon be complementary to other conferences. We're programmers, and I thInk Don't Repeat Yourself (DRY) is a good general principle to adhere to. Other ProgCom members were also free to disagree with my advocacy, as are you, the reader.&lt;/p&gt;
&lt;p&gt;In deciding on whether to include some evergreen topics at a beginners level or not, I looked to history to help me decide. For example, we hadn't had a beginner-friendly live talk on testing for the past few years, so I advocated in favour of that, even though one other program committee member disagreed and preferred to have more advanced testing talks in the program.&lt;/p&gt;
&lt;p&gt;One topic that I was super torn by when voting was in the Bayesian Statistics category. Two talks, one extremely topical and beginner friendly, the other deeply technical but extremely useful in a variety of domains, both by speakers whom I've learned from in the past. I couldn't bring myself to pick one, so I voted for both and communicated this guidance to others on the team, and let them cast the deciding vote.&lt;/p&gt;
&lt;p&gt;Finally, as a machine learner, I have been frustrated by new libraries that don't respect existing community idioms, however idiosyncratic those idioms are. One particular pet peeve is libraries that reinvent similar-yet-slightly-different APIs. There are a myriad of DataFrame APIs out there, yet I've only seen Dask do its best to explicitly implement the Pandas API. Likewise, there are a myriad of GPU tensor libraries out there, but I've only seen CuPy explicitly implement the NumPy API, which is idiomatic in the Python community. I thus strongly advocated for talks that described projects that explicitly adhered to and built on top of community idioms.&lt;/p&gt;
&lt;h2 id=&quot;gratitude-towards-the-team&quot;&gt;Gratitude towards the team&lt;/h2&gt;&lt;p&gt;I'm not ProgCom's fearless leader (Jason Myers, whose name will be public anyways, is our fearless leader (and spreadsheet maestro) – and yes, I know I mentioned a second name here), but I nonetheless feel a ton of gratiutde towards the team. We worked asynchronously, distributed around globe in a myriad of time zones. We gave incisive insight into topics, and educated each other on our respective areas of expertise. I learned and got excited about new topics in Python. We debated and advocated for a PyCon that we'd all be proud of presenting back to the community.&lt;/p&gt;
&lt;h2 id=&quot;encouragement-for-speakers-accepted-and-turned-down&quot;&gt;Encouragement for speakers, accepted and turned down&lt;/h2&gt;&lt;p&gt;As part of PyCon's program committee in 2018, I'm proud to congratulate speakers accepted to this year's PyCon talk lineup!&lt;/p&gt;
&lt;p&gt;To those who were turned down (myself included), I would like to offer up a picture of the reality we faced: some talks were decided by a single vote; at other times, we had to decide between two proposals submitted independently that paralleled each other; yet at other times, we saw such a big cluster of talks that we knew we wanted to hear, yet could only pick a handful because we didn't have an Education Summit-like dedicated track to accommodate all of them. Tough choices left and right. Don't be discouraged, you have important things to say, and there are many awesome Python-related venues (SciPy &amp;amp; PyData) to present at.&lt;/p&gt;
&lt;p&gt;For those whose data science talks were turned down, ping me on Twitter @ericmjl: I'd love to organize a Data Science Summit with you at PyCon 2019!&lt;/p&gt;
&lt;h2 id=&quot;addressing-potential-lingering-questions&quot;&gt;Addressing potential lingering questions&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Does everybody on the ProgCom have the qualifications to review every talk proposal submitted?&lt;/strong&gt; Definitely not. I have a knowledge bias towards the data science talks, and could handle some of the web talks, but I was completely unqualified to review talks on security and Python internals. Thus, for the data science talks, I offered my guidance to the rest of the ProgCom on what would be useful to speak about, but deferred to the expertise of others for talks I could not intelligently comment on. More than once I found myself re-voting based on other expert opinions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Without a fixed criteria on hand, how can talk proposers maximize their chances of getting their talk accepted?&lt;/strong&gt; This “criteria” (if you want to call it that) develops organically over time. This is intentional, as PyCon is a community conference, not a topical conference. By not setting explicit topical criteria, we can solicit talks from the community that range from timely to evergreen, from specialized to broad, and beyond, allowing the community to speak for itself (pun, ahem, not intended).&lt;/p&gt;
&lt;p&gt;This is not to say that PyCon couldn't become a topical conference, in which the ProgCom solitics proposals in particular pre-defined categories. If this changes, I'd love for next year's ProgCom to be explicit about this change, so that proposal authors have enough time to prepare for it.&lt;/p&gt;
&lt;p&gt;Moreover, new topical areas can be proposed: if I am remembering history correctly, this is how the Education Summit came into being (though I'm happy to be corrected if I'm wrong). If anybody's up for it, I'd love for a topical &quot;Data Science Summit&quot; at PyCon to come to life as well! Let's propose it together next year through the Hatchery Program.&lt;/p&gt;
&lt;p&gt;Back to the question, though: if you're thinking of this question, your proposal is probably not the one I would vote in favour of. I personally would like to hear from speakers who are deeply technical and can inject passion into a room through their technical talk, rather than from someone who was trying to tick checkboxes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If you're on the ProgCom, does this mean you can't propose a talk?&lt;/strong&gt; Of course you can propose a talk! :) Our previous fearless leader, Ned Jackson Lovely, wrote an open source app that hides our own talks from our own review, thus enabling us to remain impartial. For what it's worth, my own talk was not accepted by the ProgCom, but I have no hard feelings about it - it was placed in a category with (I think) 21 talks, making that category super competitive.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2018/1/29/refactor-notebook-code/">
    <title type="text">Refactor Notebook Code</title>
    <id>urn:uuid:d502b49d-f207-3319-a533-01b3b17c9330</id>
    <updated>2018-01-29T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2018/1/29/refactor-notebook-code/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;Jupyter notebooks that are filled with complex analyses can get unwieldy. Refactoring repeated code out into functions placed in modules should be standard practice, but from the sampling of Jupyter notebooks I've seen, I don't think this is standard practice.&lt;/p&gt;
&lt;p&gt;When should code be refactored? As soon as we start copying/pasting it! Making sure I have self-contained functions ensures that lingering state in my notebook doesn't cause unexpected behaviour. (Side note: learning the &quot;functional&quot; programming mindset can be very useful here!)&lt;/p&gt;
&lt;p&gt;But won't this slow down my pace? Isn't it faster to just copy and paste the code, and tweak what I need? Yes, but a small speed hit is going to be traded for a massive bump in rigour. Just today, I saw the effects of &quot;lingering state&quot; in my notebooks causing my plots to display different things before and after refactoring. It's not a good sign for any analysis if this happens.&lt;/p&gt;
&lt;p&gt;In short, refactor your code.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2018/1/18/pymc3-docs-weibull-patches-merged/">
    <title type="text">PyMC3 docs + Weibull patches merged!</title>
    <id>urn:uuid:337ce668-8116-366a-b9dc-903308b761a6</id>
    <updated>2018-01-18T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2018/1/18/pymc3-docs-weibull-patches-merged/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;I recently had a few PRs merged into the PyMC3 codebase. Really happy about it, and just like my previous bug fix, I thought I'd share a bit about how those PRs came about.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&quot;https://github.com/pymc-devs/pymc3/pull/2789&quot;&gt;first PR&lt;/a&gt; was an update to the docs on when to specify precision and when to specify standard deviation. They're related, so only one has to be specified, but I sometimes am sloppy when reading the docs and didn't pick up on that. Thus, I added a few lines to make sure this was crystal clear to sloppy readers like me.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&quot;https://github.com/pymc-devs/pymc3/pull/2799&quot;&gt;next PR&lt;/a&gt; was an update to the Mixture model docs, in which I added an example of the new API for specifying components of mixture models. It previously wasn't clear how to do this, as there were no examples provided, so I put in a documentation PR specifying examples.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&quot;https://github.com/pymc-devs/pymc3/pull/2804&quot;&gt;final PR&lt;/a&gt; was a patch to the Weibull distribution. I wanted to play around with trying mixture Weibulls at work, but mixture Weibulls wouldn't work because it didn't have a mode specified. I checked on Wikipedia, and found that Weibull's mode is conditional on the value of its parameters, and thus put in a PR to make this happen. &lt;a href=&quot;https://github.com/ericmjl/bayesian-analysis-recipes/blob/master/notebooks/mixture-model.ipynb&quot;&gt;Trying it out on some simulated/toy data&lt;/a&gt;, it worked! Thus, the devs allowed it to be merged.&lt;/p&gt;
&lt;p&gt;A few lessons I've learned along the way:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;(1) Docs are an awesome place to start.&lt;/strong&gt; In fact, I made a few formatting mistakes in my first and second PRs that gave an opportunity for another guy to fix! Nothing is too small to be made as a contribution. FWIW, my first contribution to open source software were documentation fixes for &lt;code&gt;matplotlib&lt;/code&gt;, and that was a superb learning journey!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;(2) Friendly maintainers are crucial.&lt;/strong&gt; The PyMC dev team can basically be described as, &quot;generally super nice!&quot; From the online and in-person interactions I've had with them, there's little in the way of egos, they're always learning, always being generally helpful. If they weren't that way, I very likely would have second thoughts trying putting in a PR there.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;(3) Open source lets me fix bugs I find.&lt;/strong&gt; This lets me work at the pace that I need to, without having to wait for commercial vendors to provide update patches. If the patch that I find turns out to be useful for others, then the work I did can possibly save a ton of people's time as well. Win-win scenario!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2018/1/10/offline-time/">
    <title type="text">Offline Time</title>
    <id>urn:uuid:9c643c98-4cfc-3fe2-ac9a-d2ee9f8ad34f</id>
    <updated>2018-01-10T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2018/1/10/offline-time/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;I was locked out of my work computer due to password reasons. (It's not human error - something about corporate management tools locked me out. Okay, well, that's human error too.) That said, I inadvertently gained a full two hours of offline time on Monday.&lt;/p&gt;
&lt;p&gt;Those two hours turned out to be pretty productive. I spent some time sketching out my work projects, trying to make better sense of how the project could fit in a disease area researcher's workflow, and figuring out derivative analyses that could enhance the value to them. This was something I probably wouldn't be able to accomplish if I had the regular distractions of my computer nearby.&lt;/p&gt;
&lt;p&gt;Seems like Cal Newport's &quot;digital distraction de-cluttering&quot; is a good thing to do. I must do more of it.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2018/1/8/bayesian-uncertainty-a-more-nuanced-view/">
    <title type="text">Bayesian Uncertainty: A More Nuanced View</title>
    <id>urn:uuid:c8e9b5e3-9f3c-38d6-9605-cd46842f22ae</id>
    <updated>2018-01-08T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2018/1/8/bayesian-uncertainty-a-more-nuanced-view/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;The following thought hit my mind just last night.&lt;/p&gt;
&lt;p&gt;Bayesian inference requires the computation of uncertainty. Computing that uncertainty is computationally expensive compared to simply computing point estimates/summary statistics. But when exactly is uncertainty useful, and more importantly, actionable? That's something I've not really appreciated in the past. It's probably not productive to be dogmatic about always computing uncertainty if that uncertainty is not actionable.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2017/12/13/visual-studio-code-a-new-microsoft/">
    <title type="text">Visual Studio Code: A New Microsoft?</title>
    <id>urn:uuid:06b625f2-4e15-3653-91b0-2f759200dd68</id>
    <updated>2017-12-13T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2017/12/13/visual-studio-code-a-new-microsoft/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;During my week attending PyData NYC 2017, which was effectively a mini-mini-sabbatical from work, I got a chance to try out Visual Studio Code. Part of it was curiosity, having seen so many PyData participants using it; part of it was because of Steve Dowell, a core CPython contributor who works at Microsoft, who mentioned about the Python-friendly tools they added into VSCode.&lt;/p&gt;
&lt;p&gt;I think VSCode is representative of a new Microsoft.&lt;/p&gt;
&lt;p&gt;But first, let me describe what using it is like.&lt;/p&gt;
&lt;h2 id=&quot;user-interface&quot;&gt;User Interface&lt;/h2&gt;&lt;p&gt;First off, the UI is beautiful. It's impossible to repeat enough how important the UI is. With minimal configuration, I made it basically match Atom's UI, which I had grown used to. It has an integrated terminal, and the colours are... wow. That shade of green, blue and red are amazing, ever just so slightly muted compared to the Terminal or iTerm. The background shade of black matches well with the rest of VSCode, and the colour scheme is changeable to match that of Atom's. The design feels... just right. Wow!&lt;/p&gt;
&lt;h2 id=&quot;git-integration&quot;&gt;Git Integration&lt;/h2&gt;&lt;p&gt;Secondly, the integration with Git rivals Atom; in fact, there's a one-click &quot;sync&quot; button! It also has nice &lt;code&gt;git commit -am&lt;/code&gt; analog where I can add and commit all of the files simultaneously.&lt;/p&gt;
&lt;h2 id=&quot;intellisense&quot;&gt;Intellisense&lt;/h2&gt;&lt;p&gt;Thirdly, IntelliSense is just amazing! I like how I can use it to look up a function signature just by mousing over the function name.&lt;/p&gt;
&lt;h2 id=&quot;open-source&quot;&gt;Open Source&lt;/h2&gt;&lt;p&gt;Finally, it’s fully open source and back able, in the same vein as Atom, minus the bloat that comes from building on top of electron. Impressive stuff!&lt;/p&gt;
&lt;h2 id=&quot;other-thoughts&quot;&gt;Other Thoughts&lt;/h2&gt;&lt;p&gt;Now, on the new Microsoft.&lt;/p&gt;
&lt;p&gt;Only at the recent PyData NYC did I learn that Microsoft has hired almost half of the core CPython developers! Not only that, they are encouraged to continue their contributions into the CPython code base. In my view, that’s a pretty awesome development! It means the Python programming language will continue to have a strong corporate backing while also enjoying community support. Its a sign of a healthy ecosystem, IMO, and also a sign of Microsoft’s support for Open Source Software!&lt;/p&gt;
&lt;p&gt;I’m more and more impressed by what Microsoft is doing for the Open Source community. I’m hoping they’ll continue up with this!!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2017/11/30/pydata-nyc-2017-recap/">
    <title type="text">PyData NYC 2017 Recap</title>
    <id>urn:uuid:69d8bdba-18aa-34f8-961d-a08fcc38cc72</id>
    <updated>2017-11-30T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2017/11/30/pydata-nyc-2017-recap/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;With that, we’ve finished PyData NYC! Here's some of my highlights of the conference.&lt;/p&gt;
&lt;h2 id=&quot;keynotes&quot;&gt;Keynotes&lt;/h2&gt;&lt;p&gt;There were three keynotes, one each by Kerstin Kleese van Dam, Thomas Sargent, and Andrew Gelman. Interestingly enough, they didn't do what I would expect most academics to do -- give talks highlighting the accomplishments of their research groups. Rather, Kerstin gave a talk that highlighted the use of PyData tools at Brookhaven National Labs. Thomas Sargent gave a philosophical talk on what economic models really are (they're &quot;games&quot;, in a mathematical sense), and I took back the importance of being able to implement models, otherwise, &quot;you're just bull*****ing&quot;.&lt;/p&gt;
&lt;p&gt;Andrew Gelman surprised me the most - he gave a wide-ranging talk about the problems we have in statistical analysis workflows. He emphasized that &quot;robustness checks&quot; are basically scams, because they're basically methods whose purpose is reassurance. He had a really cool example that highlighted that we need to understand our models by modifying our models, perhaps even using a graph of models to identify perturbations to our model that will help us understand our model. He also peppered his talk with anecdotes about how he made mistakes in his analysis workflows. I took home a different philosophy of data analysis: when we evaluate how &quot;good&quot; a model is, the operative question is, &quot;compared against what?&quot;&lt;/p&gt;
&lt;h2 id=&quot;talks&quot;&gt;Talks&lt;/h2&gt;&lt;p&gt;The talks were, for me, the highlight of the conference. A lot of good learning material around. Here's the talks from which I learned actionable new material.&lt;/p&gt;
&lt;h3 id=&quot;analyzing-nba-foul-calls-using-python&quot;&gt;Analyzing NBA Foul Calls using Python&lt;/h3&gt;&lt;p&gt;This talk by the prolific PyMC blogger Austin Rochford is one that I really enjoyed. The take-home that I got from him was towards the end of his talk, in which I picked up three ways to diagnose probabilistic programming models.&lt;/p&gt;
&lt;p&gt;The first was the use of residuals - which I now know can be used for classification problems as well as regression problems.&lt;/p&gt;
&lt;p&gt;The second was the use of the energy plots in PyMC3, where if the &quot;energy transition&quot; and &quot;marginal energy distribution&quot; plots match up (especially in the tails), then we know that the NUTS sampler did a great job.&lt;/p&gt;
&lt;p&gt;The third was the use of the Gelman-Rubin statistic to measure the in-chain vs. between-chain variation; measures close to 1 are generally considered good.&lt;/p&gt;
&lt;p&gt;Check out the talk slides &lt;a href=&quot;http://austinrochford.com/resources/talks/nba-fouls-pydata-nyc-2017.slides.html#/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&quot;scikit-learn-compatible-model-stacking&quot;&gt;&lt;code&gt;scikit-learn&lt;/code&gt;-compatible model stacking&lt;/h3&gt;&lt;p&gt;This talk was a great one because it shows how to use model stacking (also known as &quot;ensembling&quot;, a technique commonly used in Kaggle competitions) to enable better predictions.&lt;/p&gt;
&lt;p&gt;Conceptually, model stacking works like this: I train a set of model individually on a problem, and use the predictions from those models as features for a meta-model. The meta-model should perform, at worst, on par with the best model inside the ensemble, but may also perform better. This idea was first explored in Polley and van der Laan's work, available &lt;a href=&quot;http://biostats.bepress.com/ucbbiostat/paper266&quot;&gt;online&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Civis Analytics has released their implementation of model stacking in their &lt;a href=&quot;https://github.com/civisanalytics/civisml-extensions&quot;&gt;GitHub repository&lt;/a&gt;, and it's available on PyPI. The best part of it? They didn't try inventing a new API, they kept a &lt;code&gt;scikit-learn&lt;/code&gt;-compatible API. Kudos to them!&lt;/p&gt;
&lt;p&gt;Check out the repository &lt;a href=&quot;https://github.com/civisanalytics/civisml-extensions&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&quot;stream-processing-with-dask&quot;&gt;Stream Processing with Dask&lt;/h3&gt;&lt;p&gt;Matthew Rocklin, as usual, gave an entertaining and informative talk on the use of Streamz, a lightweight library he built, to explore the use of Dask for streaming applications. The examples he gave were amazing showcases of the library's capabilities. Given the right project, I'd love to try this out!&lt;/p&gt;
&lt;p&gt;Check out his slides &lt;a href=&quot;http://matthewrocklin.com/slides/pydata-nyc-2017.html#/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&quot;asynchronous-python:-a-gentle-introduction&quot;&gt;Asynchronous Python: A Gentle Introduction&lt;/h3&gt;&lt;p&gt;This talk, delivered by James Cropcho, defined what asynchronous programming was all about. For me, things finally clicked at the end when I asked him for an example of how asynchronous programming would be done in data analytics workflows -- to which he responded, &quot;If you're querying for data and then performing a calculation, then async is a good idea.&quot;&lt;/p&gt;
&lt;p&gt;The idea behind this is as such: web queries written serially are often &quot;blocking&quot;, meaning we can't do anything while we wait for the web query to return. If we want to do a calculation on the returned data point, we have to wait for it to return first. On the other hand, if written asynchronously, we could potentially do a calculation on the previous data point while waiting for the current result to return, shaving the total time off potentially by some considerable fraction.&lt;/p&gt;
&lt;h3 id=&quot;turning-pymc3-into-scikit-learn&quot;&gt;Turning PyMC3 into &lt;code&gt;scikit-learn&lt;/code&gt;&lt;/h3&gt;&lt;p&gt;This talk was by Nicole Carlson, and she did a tremendously great job delivering this talk. In it, she walked through how to wrap a PyMC3 model inside a &lt;code&gt;scikit-learn&lt;/code&gt; estimator, including details on how to implement the &lt;code&gt;.fit()&lt;/code&gt;, &lt;code&gt;.predict()&lt;/code&gt;, and &lt;code&gt;.predict_proba()&lt;/code&gt; methods. The code in &lt;a href=&quot;https://github.com/parsing-science/ps-toolkit/blob/master/ps_toolkit/pymc3_models/HLR.py&quot;&gt;her repository&lt;/a&gt; provides a great base example to copy from.&lt;/p&gt;
&lt;p&gt;One thing I can't emphasize enough is that from a user experience standpoint, it's super important to follow idioms that people are used to. What Nicole did in this talk is to show how we can provide such idioms to end-users, rather than inventing a slightly modified wheel. Props to her for that!&lt;/p&gt;
&lt;p&gt;Her slides are online &lt;a href=&quot;../../../../../blog/2017/11/30/pydata-nyc-2017-recap/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&quot;an-attempt-at-demystifying-bayesian-deep-learning&quot;&gt;An Attempt at Demystifying Bayesian Deep Learning&lt;/h3&gt;&lt;p&gt;This talk was my own, put at the end of the 2nd day. The title definitely contributed to the hype. I popped into the room early, but then left for the restroom. When I got back, there was a lineup in the front door and in the back door. Totally unexpected. That said, big credit to the Boston Bayesians organizers Jordi and Colin, who let me do the talk as a rehearsal for PyData, so I felt very grounded.&lt;/p&gt;
&lt;p&gt;The talk went mostly smoothly. I think I was channeling my colleague, Brant Peterson, with his sense of humour during that time. There was one really hilarious hiccup - right after mentioning that I wouldn't overdo the &quot;math&quot; and &quot;equations&quot;, I accidentally opened an adjacent tab with an alternate version of the slides... with, surprise surprise, a math equation on it! During the Q&amp;amp;A, when I shared the point of not needing to do train/test splits in Bayesian analysis, I could sense the jaws dropping and eyes widening in disbelief; more than just a handful of people came up and asked for the reference later on.&lt;/p&gt;
&lt;p&gt;Having done the talk, I now realize how much people will appreciate a lighthearted and lightweight introduction to a topic that's very dense and filled with jargon. Conference speakers, we need to do more of this!&lt;/p&gt;
&lt;p&gt;From an emotional standpoint, many people brought me joy with their positive comments on the visuals and structure of the talk. Others put out positive comments on Twitter, which I collected together in a &lt;a href=&quot;https://twitter.com/i/moments/924970479869448193?ref_src=twsrc%5Etfw&quot;&gt;Twitter Moment&lt;/a&gt;. It was very encouraging, especially on this deep learning journey that I'm on right now.&lt;/p&gt;
&lt;h2 id=&quot;tutorials&quot;&gt;Tutorials&lt;/h2&gt;&lt;h3 id=&quot;interactive-matplotlib-figures&quot;&gt;Interactive &lt;code&gt;matplotlib&lt;/code&gt; Figures&lt;/h3&gt;&lt;p&gt;This was something I totally didn't realize was possible before - we can create interactive &lt;code&gt;matplotlib&lt;/code&gt; figures very easily! I have cloned the repository, and I think it'll be neat to hack on some projects at work to use this.&lt;/p&gt;
&lt;p&gt;The tutorial repository can be found &lt;a href=&quot;https://github.com/tacaswell/interactive_mpl_tutorial&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&quot;linear-regression-three-ways&quot;&gt;Linear Regression Three Ways&lt;/h3&gt;&lt;p&gt;This one was by Colin Carroll, a software engineer at the MIT Media Lab (previously at Kensho). I sat in and learned a good deal of math from him. One thing new I learned was how we can specify a model without requiring the use of observed variables. Any sampling we do will take into account the hierarchical and mathematical relationships we've done. This makes it neat to implement Bayesian nets to test how things will look under different scenarios!&lt;/p&gt;
&lt;p&gt;His tutorial repository can be found &lt;a href=&quot;https://github.com/ColCarroll/pydata_nyc2017&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&quot;top-to-bottom-line-by-line&quot;&gt;Top-To-Bottom, Line-By-Line&lt;/h3&gt;&lt;p&gt;This one was led by the ever-entertaining, ever-surprising James Powell. I wish the tutorial was recorded, because even though this is a &quot;novice&quot; tutorial, it nonetheless was still an eye-opening talk. Anybody who thinks they know Python should go listen to James' talks, whenever he gives them live - it's bound to be entertaining and eye-opening!&lt;/p&gt;
&lt;h2 id=&quot;overall-thoughts&quot;&gt;Overall Thoughts&lt;/h2&gt;&lt;p&gt;I'm glad I made it to PyData NYC 2017 this year. Made many new friends and connections, and caught up with old friends in the PyData community. As always, learned a ton as well!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2017/11/16/bayesian-learning-and-overfitting/">
    <title type="text">Bayesian Learning and Overfitting</title>
    <id>urn:uuid:8ac4ab4c-3233-3ebe-8191-80fadf3a82c5</id>
    <updated>2017-11-16T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2017/11/16/bayesian-learning-and-overfitting/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;Yesterday, after I did my Boston Bayesians dry run talk, there was a point raised that I had only heard of once before: Bayesian learning methods don't overfit. Which means we're allowed to use all the data on hand. The point holds for simple Bayesian networks, and for more complicated deep neural nets.&lt;/p&gt;
&lt;p&gt;Though I believe it, I wasn't 100% convinced of this myself, so I decided to check it up. I managed to get my hands on Radford Neal's book, Bayesian Learning for Neural Networks, and found the following quotable paragraphs:&lt;/p&gt;
&lt;blockquote class=&quot;blockquote blockquote-success&quot;&gt;
&lt;p&gt;It is a common belief, however, that restricting the complexity of the models used for such tasks is a good thing, not just because of the obvious computational savings from using a simple model, but also because it is felt that too complex a model will overfit the training data, and perform poorly when applied to new cases. This belief is certainly justified if the model parameters are estimated by maximum likelihood. I will argue here that concern about overfitting is not a good reason to limit complexity in a Bayesian context.&lt;/p&gt;
&lt;/blockquote&gt;&lt;p&gt;A few paragraphs later, after explaining the frequentist procedure:&lt;/p&gt;
&lt;blockquote class=&quot;blockquote&quot;&gt;
&lt;p&gt;From a Bayesian perspective, adjusting the complexity of the model based on the amount of training data makes no sense. A Bayesian defines a model, selects a prior, collects data, computes the posterior, and then makes predictions. There is no provision in the Bayesian framework for changing the model or the prior depending on how much data was collected. If the model and prior are correct for a thousand observations, they are correct for ten observations as well (though the impact of using an incorrect prior might be more serious with fewer observations). In practice, we might sometimes switch to a simpler model if it turns out that we have little data, and we feel that we will consequently derive little benefit from using a complex, computationally expensive model, but this would be a concession to practicality, rather than a theoretically desirable procedure.&lt;/p&gt;
&lt;/blockquote&gt;&lt;p&gt;Finally, in the following section after describing how neural networks are built:&lt;/p&gt;
&lt;blockquote class=&quot;blockquote&quot;&gt;
&lt;p&gt;In a Bayesian model of this type, the role of the hyperparameters controlling the priors for weights is roughly analogous to the role of a weight decay constant in conventional training. With Bayesian training, values for these hyperparameters (more precisely, a distribution of values) can be found without the need for a validation set.&lt;/p&gt;
&lt;/blockquote&gt;&lt;p&gt;This seems to dovetail well with the following convoluted intuition that I've had: &lt;strong&gt;if I fit a Bayesian model on the &quot;training&quot; set of the data, then update it with the &quot;test&quot; set, it's equivalent to just training with the whole dataset. With wide priors, if I fit with a smaller dataset, my posterior distribution will be wider than if I fit with the entire dataset. So... where possible, just train with the entire dataset.&lt;/strong&gt; That said, I've not had sufficient grounding in Bayesian stats (after all, still a newcomer) to justify this.&lt;/p&gt;
&lt;p&gt;I certainly have more reading/learning to do here. Looks like something neat to explore in the short-term.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2017/11/14/the-value-of-thinking-simply/">
    <title type="text">The Value of Thinking Simply</title>
    <id>urn:uuid:fd45d251-9956-3192-839f-cf23a0455b66</id>
    <updated>2017-11-14T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2017/11/14/the-value-of-thinking-simply/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;Einstein has a famous quote that most people don't hear about.&lt;/p&gt;
&lt;blockquote class=&quot;blockquote&quot;&gt;
&lt;p class=&quot;mb-0&quot;&gt;It can scarcely be denied that the supreme goal of all theory is to make the irreducible basic elements as simple and as few as possible without having to surrender the adequate representation of a single datum of experience.&lt;/p&gt;
&lt;footer class=&quot;blockquote-footer&quot;&gt;Albert Einstein&lt;/footer&gt;
&lt;/blockquote&gt;&lt;p&gt;It instead, most people hear the misquote:&lt;/p&gt;
&lt;blockquote class=&quot;blockquote text&quot;&gt;
&lt;p&gt;Everything should be made as simple as possible, but no simpler. &lt;/p&gt;
&lt;footer class=&quot;blockquote-footer&quot;&gt;Misquoted version&lt;/footer&gt;
&lt;/blockquote&gt;&lt;p&gt;Though a misquote, it's still a fair (though lopsided -- missing a sufficient translation of the latter half) simplification of the original.&lt;/p&gt;
&lt;p&gt;In my work, I'm reminded of this point. I can choose to go for the complex fancy thing, but if I don't start from first principles, or start with simplistic approximations, I will struggle to have a sufficiently firm grasp on a problem to start tackling it. And therein lies the key, I think, in making progress on creative, intellectual work.&lt;/p&gt;
&lt;p&gt;The past week, I've noticed myself not wasting time on mindless coding (which usually amounts to re-running code with tweaks), and instead devoting more time to strategic thinking. As an activity, strategic thinking isn't just sitting there and thinking. For me, it involves writing and re-writing what I'm thinking, drawing and re-drawing what I'm seeing, and arranging and composing the pieces that are floating in my mind. During that time of writing, drawing, arranging and composing, I'm questioning myself, &quot;What if I didn't have this piece?&quot;. Soon enough, the &quot;simplest complex version&quot; (SCV) of whatever I'm working on begins to emerge -- but it never really is the final version! I go back and prototype it in code, and then get stuck on something, and realize I left something out in that SCV, and re-draw the entire SCV from scratch.&lt;/p&gt;
&lt;p&gt;Here's my misquote, then, offered up:&lt;/p&gt;
&lt;blockquote class=&quot;blockquote&quot;&gt;
&lt;p&gt;Sufficiently simple, and only necessarily complex.&lt;/p&gt;
&lt;footer class=&quot;blockquote-footer&quot;&gt;A further mutated version.&lt;/footer&gt;
&lt;/blockquote&gt;</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2017/11/3/boston-bayesians-talk-an-attempt-at-demystifying-bayesian-deep-learning/">
    <title type="text">Boston Bayesians Talk: An Attempt at Demystifying Bayesian Deep Learning</title>
    <id>urn:uuid:16575827-ed23-3dba-8acf-172b01b148f7</id>
    <updated>2017-11-03T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2017/11/3/boston-bayesians-talk-an-attempt-at-demystifying-bayesian-deep-learning/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;It's confirmed! I will be rehearsing my PyData NYC talk &lt;a href=&quot;https://www.meetup.com/Boston-Bayesians/events/244731222&quot;&gt;at Boston Bayesians&lt;/a&gt;, held at McKinsey's office.&lt;/p&gt;
&lt;p&gt;This time round, I've challenged myself with making the slides without using PowerPoint or Keynote, and I think I've successfully done it! Check them out:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/ericmjl/bayesian-deep-learning-demystified&quot;&gt;GitHub repository&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://ericmjl.github.io/bayesian-deep-learning-demystified&quot;&gt;Online slides&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Side note, I'm starting to really love what we can do with the web!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2017/10/31/always-check-your-data/">
    <title type="text">Always Check Your Data</title>
    <id>urn:uuid:5aa6b7fd-5346-3f68-a35b-83bb0dc0b0b6</id>
    <updated>2017-10-31T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2017/10/31/always-check-your-data/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;True story, just happened today. I was trying to fit a Poisson likelihood to estimate event cycle times (in discreet weeks). For certain columns, everything went perfectly fine. Yet for other columns, I was getting negative infinity’s likelihoods, and was banging my head over this problem for over an hour and a half.&lt;/p&gt;
&lt;p&gt;As things turned out, those columns that gave me negative infinity likelihood initializations were doing so because of negative values in the data. Try fitting a Poisson likelihood, which only has positive support, on that!&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://i.imgflip.com/1yl6ki.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;This lost hour and a half was a good lesson in data checking/testing: &lt;strong&gt;always be sure to sanity check basic stats associated with the data - bounds (min/max), central tendency (mean/median/mode) and spread (variance, quartile range) - always check!&lt;/strong&gt;&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2017/10/27/random-forests-a-good-default-model/">
    <title type="text">Random Forests: A Good Default Model?</title>
    <id>urn:uuid:2570438e-960b-3197-8bf0-6a690f53f864</id>
    <updated>2017-10-27T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2017/10/27/random-forests-a-good-default-model/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;I've been giving this some thought, and wanted to go out on a limb to put forth this idea:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;I think Random Forests (RF) are a good &quot;baseline&quot; model to try, after establishing a &quot;random&quot; baseline case.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;(Clarification: I'm using RF as a shorthand for &quot;forest-based ML algorithms&quot;, including XGBoost etc.)&lt;/p&gt;
&lt;p&gt;Before I go on, let me first provide some setup.&lt;/p&gt;
&lt;p&gt;Let's say we have a two-class classification problem. Assume everything is balanced. One &quot;dumb baseline&quot;&quot; case is a coin flip. The other &quot;dumb baseline&quot; is predicting everything to be one class. Once we have these established, we can go to a &quot;baseline&quot; machine learning model.&lt;/p&gt;
&lt;p&gt;Usually, people might say, &quot;go do logistic regression (LR)&quot; as your first baseline model for classification problems. It sure is a principled choice! Logistic regression is geared towards classification problems, makes only linear assumptions about the data, and identifies directional effects as well. From a practical perspective, it's also very fast to train.&lt;/p&gt;
&lt;p&gt;But I've found myself more and more being oriented towards using RFs as my baseline model instead of logistic regression. Here are my reasons:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Practically speaking, any modern computer can train a RF model with ~1000+ trees in not much more time than it would need for an LR model.&lt;/li&gt;
&lt;li&gt;By using RFs, we do not make linearity assumptions about the data.&lt;/li&gt;
&lt;li&gt;Additionally, we don't have to scale the data (one less thing to do).&lt;/li&gt;
&lt;li&gt;RFs will automatically learn non-linear interaction terms in the data, which is not possible without further feature engineering in LR.&lt;/li&gt;
&lt;li&gt;As such, the out-of-the-box performance using large RFs with default settings is often very good, making for a much more intellectually interesting challenge in trying to beat that classifier.&lt;/li&gt;
&lt;li&gt;With &lt;code&gt;scikit-learn&lt;/code&gt;, it's a one-liner change to swap out LR for RF. The API is what matters, and as such, drop-in replacements are easily implemented!&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Just to be clear, I'm not advocating for throwing away logistic regression altogether. There are moments where interpretability is needed, and is more easily done by using LR. In those cases, LR can be the &quot;baseline model&quot;, or even just back-filled in after training the baseline RF model for comparison.&lt;/p&gt;
&lt;p&gt;Random Forests were the darling of the machine learning world before neural networks came along, and even now, remain the tool-of-choice for colleagues in the cheminformatics world. Given how easy they are to use now, why not just start with them?&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2017/10/22/network-propagation/">
    <title type="text">Network Propagation</title>
    <id>urn:uuid:8092c56f-71cb-389a-ab28-5d3168f9e6c9</id>
    <updated>2017-10-22T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2017/10/22/network-propagation/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;&lt;p&gt;I recently read a &lt;a href=&quot;http://www.nature.com/nrg/journal/vaop/ncurrent/abs/nrg.2017.38.html&quot;&gt;review paper on network propagation&lt;/a&gt;. It looks like a very useful thing in data science, particularly where networks are involved, and I wanted to share it with everybody.&lt;/p&gt;
&lt;p&gt;Before I go on, I will start off by assuming we have a dataset modelled as a &quot;network&quot;, or a &quot;graph&quot;, in which the nodes are entities in the graph, and edges are relationships between those entities. In the context of my work, I encounter biological networks of many kinds - gene-gene interactions, protein-protein interactions, protein-RNA interactions, evolutionary networks, and more.&lt;/p&gt;
&lt;p&gt;What kind of problems can network propagation solve? The problem class generally follows this logic: I start with some nodes of interest, and I'm most interested in finding other nodes of interest based on these &quot;seed&quot; nodes. Knowing that path-based methods can often end up over-prioritizing highly connected nodes, we need a different principled method for finding these nodes. One way would be to take a random walk on the graph, and find out how often we land on a particular set of nodes on the graph. This random walk is what we call &quot;network propagation&quot;.&lt;/p&gt;
&lt;h2 id=&quot;how-network-propagation-works&quot;&gt;How network propagation works&lt;/h2&gt;&lt;p&gt;Network propagation follows this intuition: Imagine nodes contain information, and edges dictate which nodes information can be shared with. Network propagation shares the information with other nodes, according to the following rules:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;I can only share by copying to my neighbours in the graph,&lt;/li&gt;
&lt;li&gt;I must share everything that I have with my neighbours, and&lt;/li&gt;
&lt;li&gt;Exchanges in the graph happen simultaneously.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;An animation of how this works is shown below.&lt;/p&gt;
&lt;p&gt;If you take my DataCamp course, you'll learn that networks can be represented as matrices on a computer. The adjacency matrix can be a 1/0 (i.e. binary) matrix that describes how nodes (rows and columns) are connected, and nodes can have their own vector of information (i.e. whether they are &quot;seed&quot; nodes or not). As it turns out, network propagation has a very nice matrix representation:&lt;/p&gt;
&lt;h2 id=&quot;how-to-implement-network-propagation&quot;&gt;How to implement network propagation&lt;/h2&gt;&lt;p&gt;So, how do we do network propagation with NetworkX, &lt;code&gt;numpy&lt;/code&gt; and Python? Let's take a look at code. Firstly, &lt;strong&gt;how do you convert a graph to a &lt;code&gt;numpy&lt;/code&gt; matrix&lt;/strong&gt;?&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Assume we have a graph G. Convert it to a numpy matrix.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;In&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_numpy_matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;G&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;In&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This basically corresponds to the following graph:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;../../../../../blog/2017/10/22/network-propagation/graph.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;Great! Now, let's mark out some &quot;seed&quot; nodes. These are the &quot;nodes&quot; of interest. To represent this, we use a binary vector - each slot in the vector represents one node in the graph. Let's start by highlighting two nodes, the second and third one:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;In&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;M&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The &lt;code&gt;.reshape(A.shape[0], 1)&lt;/code&gt; is done for convenience, so that each axis is aligned appropriately for the following matrix multiplication:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;In&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;M&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;After the end of one round, nodes 2 and 3 have no information held on them, but their neighbours do. Let's do two rounds of propagation, rather than just one.&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;In&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;M&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;After two rounds of information, information is shared back with the original nodes, but not necessarily evenly - it all depends on the connectivity of each node.&lt;/p&gt;
&lt;p&gt;It'll get really verbose doing 100 rounds; imagine doing &lt;code&gt;A @ (A @ (A @ (A @ .....)))&lt;/code&gt;. Instead of doing that, a recursive function may be better. At the same time, with many rounds of network propagation, we will end up with really large numbers. Therefore, it will be helpful to also normalize the result of matrix multiplication.&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;In&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;normalize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
   &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;     &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;In&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;recursive_propagate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;M&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
   &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;     &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
   &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;         &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;recursive_propagate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;normalize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;M&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
   &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;     &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
   &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;         &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;normalize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;M&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
   &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;On a previous blog post, I've alluded to the fact that recursive functions are pretty darn useful!&lt;/p&gt;
&lt;h2 id=&quot;result&quot;&gt;Result&lt;/h2&gt;&lt;p&gt;So, let's take a look at the result of a few rounds of propagation.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;../../../../../blog/2017/10/22/network-propagation/figure_1_sm.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;The first thing that I'd like to highlight is that the information propagation process converges on a final result fairly quickly. The second thing I'd like to highlight is that the purple nodes (there are two lines!), which weren't highlighted at the start, ended up having the highest propagation scores, followed by the red node.&lt;/p&gt;
&lt;p&gt;For those who are familiar with Markov chains, I believe network propagation has strong parallels with that.&lt;/p&gt;
&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;&lt;p&gt;Network propagation is conceptually simple, and easy to implement in Python.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2017/10/11/pypy-impressive/">
    <title type="text">PyPy: Impressive!</title>
    <id>urn:uuid:d33ee57d-b161-39f0-9b6d-9821ba7c6a11</id>
    <updated>2017-10-11T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2017/10/11/pypy-impressive/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;A few years on after trying out PyPy for the first time and wrestling with it, I still find it to be pretty awesome.&lt;/p&gt;
&lt;p&gt;Now that PyPy officially supports &lt;code&gt;numpy&lt;/code&gt;, I'm going to profile a few simple statistical simulation tasks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Computing the mean of a number of random number draws.&lt;/li&gt;
&lt;li&gt;Simulating many coin flips&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I'll profile each of the tasks four ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pure Python implementation running from the CPython and PyPy interpreters&lt;/li&gt;
&lt;li&gt;&lt;code&gt;numpy&lt;/code&gt; implementation running from the CPython and PyPy interpreters.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So, how do PyPy and CPython fare? Let's show the results up front first.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;../../../../../blog/2017/10/11/pypy-impressive/profile.png&quot;&gt;&lt;img src=&quot;../../../../../blog/2017/10/11/pypy-impressive/profile-sm.png&quot; alt=&quot;Profiling results.&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Click on the image to view a higher resolution chart. The raw recorded measurements can be found &lt;a href=&quot;https://docs.google.com/spreadsheets/d/1QB1hF7Z8SGYjvll8sYCjVYEYAgzL4pjqGt1dbO6B2Co/edit?usp=sharing&quot;&gt;on Google Sheets&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Here's a description of what's happening:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(top-left): PyPy is approx. 10X faster than CPython at computing the mean of 10 million random numbers.&lt;/li&gt;
&lt;li&gt;(top-right): When both are running &lt;code&gt;numpy&lt;/code&gt;, the speed is identical.&lt;/li&gt;
&lt;li&gt;(bottom-left): When simulating coin flips, PyPy with a custom &lt;code&gt;binomial()&lt;/code&gt; function is about 3X faster than CPython.&lt;/li&gt;
&lt;li&gt;(bottom-right): When using &lt;code&gt;numpy&lt;/code&gt; instead, there is a bottleneck, and PyPy fails badly compared to CPython.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It's pretty clear that when PyPy is dealing with &quot;pure&quot; data (i.e. not having to pass data between Python and C), PyPy runs very, very fast, and, at least in the scenarios tested here, it performs faster than the CPython interpreter. This is consistent with my previous observations, and probably explains why PyPy is very good for code that is very repetitive; the JIT tracer really speeds things up.&lt;/p&gt;
&lt;p&gt;That last plot (bottom-right) is a big curiosity. Using the code below, I measured the random number generation is actually just as fast as it should be using CPython, but that PyPy failed badly when I was passing in a &lt;code&gt;numpy&lt;/code&gt; array to the &lt;code&gt;Counter()&lt;/code&gt; object (from the standard library). I'm not sure what is happening behind-the-scenes, but I have reached out to the PyPy developers to ask what's going on, and will update this post at a later date.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;UPDATE:&lt;/strong&gt; I heard back from the PyPy devs &lt;a href=&quot;https://bitbucket.org/pypy/pypy/issues/2680/slow-speed-going-from-numpy-data-structure&quot;&gt;on BitBucket&lt;/a&gt;, and this is indeed explainable by data transfer between the C-to-PyPy interface. It's probably parallel to the latency that arises from transferring data between the CPU and GPU, or between compute nodes.&lt;/p&gt;
&lt;p&gt;So, what does this mean? It means that for pure Python code, PyPy can be a very powerful way to accelerate your code. One example I can imagine is agent-based simulations using Python objects. Another example that comes to mind is running a web server that only ever deals with strings, floats and JSONs (in contrast to matrix-heavy scientific computing).&lt;/p&gt;
&lt;p&gt;Now, for those who are curious, here's the source code for the &lt;strong&gt;pure Python implementation of the mean of random numbers&lt;/strong&gt;.&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Mean of 10 million random number draws.&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;time&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;random&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1E7&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;rnds&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;rnds&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rnds&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;{} seconds&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And here's the source code for the &lt;strong&gt;&lt;code&gt;numpy&lt;/code&gt; implementation of the mean of random numbers&lt;/strong&gt;.&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;time&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1E7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;{} seconds&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Next, here's the source code for &lt;strong&gt;coin flips in pure Python&lt;/strong&gt;:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Simulate 10 million biased coin flips with p = 0.3&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;random&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;time&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;collections&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Counter&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;bernoulli&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;rnd&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rnd&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;


&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bernoulli&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1E7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Counter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;{} seconds&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And finally, source code for &lt;strong&gt;coin flips using &lt;code&gt;numpy&lt;/code&gt;&lt;/strong&gt;:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy.random&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;binomial&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;time&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;collections&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Counter&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;coinflips&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;binomial&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1E7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Time for numpy coinflips: {} seconds&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Counter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;coinflips&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;{} seconds&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2017/10/10/recursive-programming-and-dags/">
    <title type="text">Recursive Programming and DAGs</title>
    <id>urn:uuid:5b939f02-8808-3e54-938f-21843ac5e60b</id>
    <updated>2017-10-10T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2017/10/10/recursive-programming-and-dags/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;Over the past few days, I've found myself using recursive programming to implement a &quot;model specification&quot; system with inheritance for deep learning. The goal here is to enable reproducible computational experiments for particular deep learning hyperparameter sets. Reproducibility is something I learned from the Software/Data Carpentry initiative, thus I wanted to ensure that my own work was reproducible, even if it's not (because of corporate reasons) open-able, because it's the right thing to do.&lt;/p&gt;
&lt;p&gt;So, how do these &quot;model spec&quot; files work? I call them &quot;experiment profiles&quot;, and they specify a bunch of things: &lt;strong&gt;model architecture&lt;/strong&gt;, &lt;strong&gt;training parameters&lt;/strong&gt;, and &lt;strong&gt;data tasks&lt;/strong&gt;. These experiment profiles are stored in YAML files on disk. A profile essentially looks like the following (dummy examples provided, naturally):&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Name: default.yaml&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;parent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;null&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;data_tasks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;tasks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p p-Indicator&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;task1&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;task2&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;task3&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;model_architecture&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;hidden_layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p p-Indicator&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;hidden_dropouts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p p-Indicator&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;training_parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;sgd&amp;quot;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;optimizer_options&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;nt&quot;&gt;n_epochs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;20&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In this YAML file, the key-value pairs essentially match the API of the tooling I've built on top of Keras' API to make myself more productive. (From the example, it should be clear that we're dealing with only feed-forward neural networks and nothing else more complicated.) The key here (pun unintended) is that I have a &lt;code&gt;parent&lt;/code&gt; key-value pair that specifies another experiment profile that I can inherit from.&lt;/p&gt;
&lt;p&gt;Let's call the above example &lt;code&gt;default.yaml&lt;/code&gt;. Let's say I want to run another computational experiment that uses the &lt;code&gt;adam&lt;/code&gt; optimizer instead of plain vanilla &lt;code&gt;sgd&lt;/code&gt;. Instead of re-specifying the entire YAML file, by implementing an inheritance scheme, I can re-specify only the optimizer and optimizer_options.&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Name: adam.yaml&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;parent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;default.yaml&amp;quot;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;training_parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;adam&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Finally, let's say I find out that 20 epochs (inherited from &lt;code&gt;default.yaml&lt;/code&gt;) is too much for Adam - after all, Adam is one of the most efficient gradient descent algorithms out there - and I want to change it to 3 epochs instead. I can do the following:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Name: adam-3.yaml&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;parent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;adam.yaml&amp;quot;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;training_parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;optimizer_options&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;nt&quot;&gt;n_epochs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;3&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Okay, so specifying YAML files with inheritance is all good, but how do I ensure that I get the entire parameter set out correctly, without writing verbose code? This is where the power of recursive programming comes in. Using recursion, I can solve this problem with &lt;strong&gt;a single function that calls itself on one condition, and returns a result on another condition&lt;/strong&gt;. That's a recursive function in its essence.&lt;/p&gt;
&lt;p&gt;The core of this problem is traversing the inheritance path, from &lt;code&gt;adam-3.yaml&lt;/code&gt; to &lt;code&gt;adam.yaml&lt;/code&gt; to &lt;code&gt;default.yaml&lt;/code&gt;. Once I have the inheritance path specified, loading the YAML files as a dictionary becomes the easy part.&lt;/p&gt;
&lt;p&gt;How would this look like in code? Let's take a look at an implementation.&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;yaml&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;inheritance_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yaml_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    :param str yaml_file: The path to the yaml file of interest.&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    :param list path: A list specifying the existing inheritance path. First&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        entry is the file of interest, and parents are recursively appended to&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        the end of the list.&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yaml_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;r+&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yaml&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;parent&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;parent&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inheritance_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;parent&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The most important part of the function is in the &lt;code&gt;if&lt;/code&gt;/&lt;code&gt;else&lt;/code&gt; block. If I have reached the &quot;root&quot; of the inheritance path, (that is, I have hit &lt;code&gt;default.yaml&lt;/code&gt; which has no parent), then I return the &lt;code&gt;path&lt;/code&gt; traversed. Otherwise, I return into the &lt;code&gt;inheritance_path&lt;/code&gt; function call again, but with an updated &lt;code&gt;path&lt;/code&gt; list, and a different &lt;code&gt;yaml_file&lt;/code&gt; to read. It's a bit like doing a &lt;code&gt;while&lt;/code&gt; loop, but in my opinion, a bit more elegant aesthetically.&lt;/p&gt;
&lt;p&gt;Once I've gotten the path list, I can finally load the parameters using a single function that calls on &lt;code&gt;inheritance_path&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;load_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yaml_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inheritance_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yaml_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yaml_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_tasks&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
             &lt;span class=&quot;n&quot;&gt;model_architecture&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
             &lt;span class=&quot;n&quot;&gt;training_parameters&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fn&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[::&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# go in reverse!&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;r+&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yaml&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;items&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;update&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This is the equivalent of traversing a Directed Acyclic Graph (DAG), or in some special cases, a tree data structure, but in a way where we don't have to know the entire tree structure ahead of time. The goal is to reach the root from any node:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;root
    |- A
        |- B
        |- C
            |- D
            |- E
    |- F
        |- G
        |- H
        |- I
            |- J
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Also, because we only have one pointer in each YAML file to its parent, we have effectively created a &quot;Linked List&quot; that we can use to trace a path back to the &quot;root&quot; node, along the way collecting the information that we need together. By using this method of traversal, we only need to know the neighbors, and at some point (however long it takes), we will reach the root.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;D -&amp;gt; C -&amp;gt; A -&amp;gt; root
E -&amp;gt; C -&amp;gt; A -&amp;gt; root
J -&amp;gt; I -&amp;gt; F -&amp;gt; root
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you were wondering why linked lists, trees and other data structures might be useful as a data scientist, I hope this illustrates on productive example!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2017/10/10/pydata-nyc-2017/">
    <title type="text">PyData NYC 2017</title>
    <id>urn:uuid:e720c68f-51c8-3391-9f3c-97ce2ba8267c</id>
    <updated>2017-10-10T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2017/10/10/pydata-nyc-2017/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;I'm seriously looking forward to PyData NYC this year -- there's a great lineup of talks that I'm particularly looking forward to hearing! The theme for my set of must-see talks this year is &quot;Bayesian machine learning&quot; - there's much for me to learn!&lt;/p&gt;
&lt;p&gt;The first is by my fellow Boston Bayesian &lt;strong&gt;&lt;a href=&quot;https://colindcarroll.com/&quot;&gt;Colin Caroll&lt;/a&gt;&lt;/strong&gt; with his talk titled &lt;a href=&quot;https://pydata.org/nyc2017/schedule/presentation/12/&quot;&gt;Two views on regression with PyMC3 and scikit-learn&lt;/a&gt;. Colin is a mathematician at heart, even though he does software engineering for living now, and I can't wait to hear about regularization strategies!&lt;/p&gt;
&lt;p&gt;The second is by &lt;strong&gt;&lt;a href=&quot;https://pydata.org/nyc2017/speaker/profile/6/&quot;&gt;Nicole Carlson&lt;/a&gt;&lt;/strong&gt;, with her talk titled &lt;a href=&quot;https://pydata.org/nyc2017/schedule/presentation/24/&quot;&gt;Turning PyMC3 into scikit-learn&lt;/a&gt;. Nicole's talk is of interest to me because I've implemented models in PyMC3 before, and now would like to know how to make them reusable!&lt;/p&gt;
&lt;p&gt;The third talk is by &lt;strong&gt;&lt;a href=&quot;https://pydata.org/nyc2017/speaker/profile/118/&quot;&gt;Chaya Stern&lt;/a&gt;&lt;/strong&gt;, with her talk titled &lt;a href=&quot;https://pydata.org/nyc2017/schedule/presentation/53/&quot;&gt;Bayesian inference in computational chemistry&lt;/a&gt;. Super relevant to my work at Novartis!&lt;/p&gt;
&lt;p&gt;The fourth is by my fellow Boston Pythonista &lt;strong&gt;&lt;a href=&quot;https://pydata.org/nyc2017/speaker/profile/34/&quot;&gt;Joe Jevnik&lt;/a&gt;&lt;/strong&gt;, who will be speaking on the first day about his journey into deep learning on some really cool time-series data. He works at Quantopian, BUT the spoiler here is that his talk is NOT about financial data! (I've heard his talk outline already.)&lt;/p&gt;
&lt;p&gt;The fifth is a tutorial by &lt;strong&gt;&lt;a href=&quot;https://pydata.org/nyc2017/speaker/profile/29/&quot;&gt;Jacob Schrieber&lt;/a&gt;&lt;/strong&gt;, with his talk titled &lt;a href=&quot;https://pydata.org/nyc2017/schedule/presentation/30/&quot;&gt;pomegranate: fast and flexible probabilistic modeling in python&lt;/a&gt;. &lt;code&gt;pomegranate&lt;/code&gt;'s API models after the &lt;code&gt;scikit-learn&lt;/code&gt;'s API; with the API being the user-facing interface, and &lt;code&gt;scikit-learn&lt;/code&gt; being the &lt;em&gt;de facto&lt;/em&gt; go-to library for machine learning, I'd be interested to see how much more &lt;code&gt;pomegranate&lt;/code&gt; adds to the ecosystem, particularly w.r.t. Bayesian models.&lt;/p&gt;
&lt;p&gt;There are a swathe of other good talks that I'm expecting to be able to catch online later on. &lt;strong&gt;&lt;a href=&quot;https://matthewrocklin.com/&quot;&gt;Matt Rocklin&lt;/a&gt;&lt;/strong&gt;, who is the lead developer of Dask, has done a ton of work on speeding Python up through parallelism. His talk will be on &lt;a href=&quot;https://pydata.org/nyc2017/schedule/presentation/22/&quot;&gt;the use of Cython &amp;amp; Dask to speed up GeoPandas&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Also, &lt;strong&gt;&lt;a href=&quot;https://pydata.org/nyc2017/speaker/profile/80/&quot;&gt;Thomas Caswell&lt;/a&gt;&lt;/strong&gt;, one of the &lt;a href=&quot;http://matplotlib.org/&quot;&gt;&lt;code&gt;matplotlib&lt;/code&gt;&lt;/a&gt; lead devs who helped guide my first foray into open source contributions, is giving a tutorial on &lt;a href=&quot;https://pydata.org/nyc2017/schedule/presentation/3/&quot;&gt;developing interactive figures in matplotlib&lt;/a&gt;. Highly recommended if you're into the visualization world!&lt;/p&gt;
&lt;p&gt;Finally, the always-interesting, always entertaining &lt;strong&gt;&lt;a href=&quot;https://pydata.org/nyc2017/schedule/presentation/25/&quot;&gt;en zyme&lt;/a&gt;&lt;/strong&gt; will be speaking on an &lt;a href=&quot;https://pydata.org/nyc2017/schedule/presentation/25/&quot;&gt;interesting topic&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Looking forward to being at the conference, and meeting old and new friends there!&lt;/p&gt;
</content>
  </entry>
</feed>
