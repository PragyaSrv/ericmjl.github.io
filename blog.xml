<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title type="text">Eric Ma's Blog</title>
  <id>urn:uuid:8e5496e4-8606-3632-a35c-1d9694b4313d</id>
  <updated>2020-06-29T00:00:00Z</updated>
  <link href="http://www.ericmjl.com/blog/" />
  <link href="http://www.ericmjl.com/blog.xml" rel="self" />
  <author>
    <name></name>
  </author>
  <generator uri="https://github.com/ajdavis/lektor-atom" version="0.3">Lektor Atom Plugin</generator>
  <entry xml:base="http://www.ericmjl.com/blog/2020/6/29/how-i-feel-about-hey/">
    <title type="text">How I feel about Hey</title>
    <id>urn:uuid:94dde895-bd9d-322a-b3bb-caed5a5ef73f</id>
    <updated>2020-06-29T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2020/6/29/how-i-feel-about-hey/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;&lt;a href=&quot;../../../../../blog/2020/6/29/how-i-feel-about-hey/hey.com&quot;&gt;Hey&lt;/a&gt; is a new email service by Basecamp. If you’re a techie type, you’ve probably heard of it via the Twitter world.&lt;/p&gt;
&lt;p&gt;I saw the demo by their CEO Jason Fried, and I was sold enough to write an email to their invite request address (&lt;a href=&quot;mailto:iwant@hey.com&quot;&gt;iwant@hey.com&lt;/a&gt;).&lt;/p&gt;
&lt;h2 id=&quot;my-likes&quot;&gt;My likes&lt;/h2&gt;&lt;p&gt;Now that I’ve been trying it out for about a week, I wanted to share about my favourite features about Hey email that makes me ready to pay up for it.&lt;/p&gt;
&lt;h3 id=&quot;batched-replies&quot;&gt;Batched replies&lt;/h3&gt;&lt;p&gt;Hey gives us an interface that lets us set aside emails into a bucket that one can “Reply Later”. With any email open, we can mark it as an email to “Reply Later”, and assign it to the “Reply Later” stack.&lt;/p&gt;
&lt;p&gt;At a later time in the day, we can go into that stack, and hit the “Focus &amp;amp; Reply” button, which gives us another interface to go down the stack and respond to emails one by one.&lt;/p&gt;
&lt;p&gt;This makes so much more sense, because:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;It’s easier to give each email the attention that it needs,&lt;/li&gt;
&lt;li&gt;Without having to pass through the “Inbox” and being distracted by something else we “feel” we need to respond to (but don’t actually have to).&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&quot;the-screener&quot;&gt;The Screener&lt;/h3&gt;&lt;p&gt;The Screener is pretty rad. I can quickly screen emails into the appropriate buckets: The Imbox, The Feed, and The Papertrail. This helps reduce the amount of clutter that hits my eyes.&lt;/p&gt;
&lt;p&gt;The screening procedure is quite deterministic. There’s no fancy machine learning that goes on that attempts to stochastically move emails unexpectedly. Emails from important people, or important newsletters, get sent to the Imbox. (I get to decide what is important!) Other emails are sent out of sight, out of mind, until I feel ready to look at them as a collection.&lt;/p&gt;
&lt;h3 id=&quot;functional-piles&quot;&gt;Functional piles&lt;/h3&gt;&lt;p&gt;“Functional piles” is, by far, the biggest thing I’ve come to like about Hey. I’ve alluded to this above, but I thought I’d detail this a bit more.&lt;/p&gt;
&lt;p&gt;Hey gives us a functionally broad definition of “piles” of email:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The Screening&lt;/li&gt;
&lt;li&gt;The Imbox&lt;/li&gt;
&lt;li&gt;The Papertrail&lt;/li&gt;
&lt;li&gt;The Feed&lt;/li&gt;
&lt;li&gt;The Set Aside Pile&lt;/li&gt;
&lt;li&gt;The Reply Later Pile&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What’s really cool about this functional definition is that:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The definition is “functional”, in that it determines &lt;em&gt;what I do&lt;/em&gt; with the email in a sane workflow.&lt;/li&gt;
&lt;li&gt;The definition is broad enough that I don’t have to think &lt;em&gt;too&lt;/em&gt; hard about which pile it goes into.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For example, “The Screening” pile is for all of the first-time emails in my Inbox, and all I have to decide is whether I might want to see that email later or not.&lt;/p&gt;
&lt;p&gt;As another example, the “Reply Later” pile lets me batch together emails that I might want to focus and reply to at one shot.&lt;/p&gt;
&lt;p&gt;The workflow feels &lt;em&gt;very&lt;/em&gt; functional, and the user interaction design is definitely geared towards making sane the way we handle email.&lt;/p&gt;
&lt;p&gt;By contrast, in Gmail, I was paying for SaneBox to help me automatically triage my email for me, but even then, I couldn’t leisurely scroll through them because it was a separate service with a separate interface that made it difficult to work with.&lt;/p&gt;
&lt;p&gt;Also, Gmail never gave us the ability to screen out emails, neither did it give us the ability to set aside special piles to “reply to” later.&lt;/p&gt;
&lt;p&gt;I know I could have used Gmail tags to handle this, but dragging and dropping an email to a generic tag qualitatively &lt;em&gt;feels&lt;/em&gt; different compared to hitting a button to send it to a pile that is still visible but not in my face.&lt;/p&gt;
&lt;p&gt;Finally, whenever an email is read, it gets moved from the top of the Imbox into the “read” pile. This is another one of those subtle designs that makes usage such a joy. There’s a feed of email below that I’ve seen once, but only the &lt;em&gt;truly&lt;/em&gt; unread ones are still above. Having ingrained email habits for over 20 years, this takes a bit of getting used to, but once one is used to Hey’s design, one realizes how helpful it is for triaging and batching (i.e. “pretty good” email workflow).&lt;/p&gt;
&lt;h3 id=&quot;forwarding-in&quot;&gt;Forwarding in&lt;/h3&gt;&lt;p&gt;Any email service can do this, but yes, email forwarding is the way that we get emails ported over from our old addresses. Forwarding is the most straightforward (ahem!) way to get started with the service without needing to resort to emailing a ton of people.&lt;/p&gt;
&lt;p&gt;I still keep my old Gmail account around because I use it for the calendar scheduling story, and for Google Drive. It’s hard to part with an address I’ve had for a long time, but I’ve done migration off digital services before, and I know it just takes a bit of patience to finish.&lt;/p&gt;
&lt;h2 id=&quot;concluding-thoughts&quot;&gt;Concluding thoughts&lt;/h2&gt;&lt;p&gt;Perhaps my situation has become typical of the kind of user that Hey is trying to help: lots of emails, a small fraction being important, the others I might need to know.&lt;/p&gt;
&lt;p&gt;Yet again, good design is shown to matter - Hey makes “great workflow” front-and-center, whereas most email services don’t. This seemingly little workflow detail has a disproportionate impact on the quality of the service. Good design matters, and I’m happy to pay for it!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2020/6/28/statistical-tests-are-just-fancy-model-comparisons/">
    <title type="text">Statistical tests are just canned model comparisons</title>
    <id>urn:uuid:4b1701e8-f200-3f5a-9e38-a8fd0624eb16</id>
    <updated>2020-06-28T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2020/6/28/statistical-tests-are-just-fancy-model-comparisons/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;I reached another statistics epiphany today.&lt;/p&gt;
&lt;p&gt;Statistical tests? They are nothing more than a comparison between statistical models
that has been made canned.&lt;/p&gt;
&lt;p&gt;What do I mean?&lt;/p&gt;
&lt;p&gt;Let's say we have data for Korean men from both the North and the South.
We could write a generative model for men's height that looks something like the following:&lt;/p&gt;
&lt;p&gt;$$h^n \sim N(\mu^{n}, \sigma^{n})$$&lt;/p&gt;
&lt;p&gt;$$h^s \sim N(\mu^{s}, \sigma^{s})$$&lt;/p&gt;
&lt;p&gt;(The superscripts don't refer to powers, but &quot;north&quot; and &quot;south&quot;; I was just having difficulty getting Markdown + LaTeX subscripts parsed correctly.)&lt;/p&gt;
&lt;p&gt;In effect, we have one data generating model per country.&lt;/p&gt;
&lt;p&gt;Think back to how you might analyze Korean male height data using canned statistical procedures.
You might choose the t-test.
In performing the t-test, we make some assumptions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The two populations have equal variance, and&lt;/li&gt;
&lt;li&gt;We are only interested in comparing the means.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Yes, there are variations on the t-test, such as t-test with unequal variance and the likes,
but I wonder how many of us would &lt;em&gt;really&lt;/em&gt; think deep about the assumptions underlying the procedure
when faced with data and a &quot;canned protocol&quot;?
Instead, we'd simply ask, &quot;is there a difference in the means?&quot;
And then calculate some convoluted p-value.&lt;/p&gt;
&lt;p&gt;How could we go beyond canned procedures and tell if the two populations have a difference?
Well, we could compare their means, but we could also add another comparison of the variance too.
All of this comes very naturally from thinking about the data generating story.
Let's see this in action.&lt;/p&gt;
&lt;p&gt;In performing Bayesian inference, we would obtain posterior distributions
for both the $\mu$ and $\sigma$ parameters of both models.
We could then compare the $\mu$ and the $\sigma$ parameters,
and they would both be informative.&lt;/p&gt;
&lt;p&gt;For example, if $\mu$s turned out to be different and their posteriors do not overlap,
we would claim that the two populations differ in their average height,
and explain it as being due to the nutritional difference arising from economic mismanagement.&lt;/p&gt;
&lt;p&gt;On the other hand, if the $\sigma$ turned out to be different and their posterios didn't overlap,
we would claim that the two populations differ in their &lt;em&gt;variation in height&lt;/em&gt;.
How would we explain this?
For example, if the distribution of North Korean heights was tighter
giving rise to a smaller $\sigma$ value,
we might explain this as being due to rationing and genetic homogeneity.&lt;/p&gt;
&lt;p&gt;In both comparisons, by thinking more precisely about the data generating process,
we had access to a broader set of hypotheses.
If we had defaulted to the t-test as a &quot;canned statistical procedure&quot;,
and not thought carefully about setting up a data generating model,
we would be missing out on a rich world of inquiry into our data.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2020/6/15/whats-the-most-optimal-way-to-learn-bayesian-statistics/">
    <title type="text">What's the most optimal way to learn Bayesian statistics?</title>
    <id>urn:uuid:5e897219-ccca-31a1-b090-eb8af84b6ebc</id>
    <updated>2020-06-15T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2020/6/15/whats-the-most-optimal-way-to-learn-bayesian-statistics/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;I've been reflecting on the way I learned statistics, and I think I learned it in a flawed fashion.&lt;/p&gt;
&lt;p&gt;Traditionally, statistics is taught in the format of performing hypothesis tests to infer whether there's a difference between groups, or to learn the parameters of some curve.&lt;/p&gt;
&lt;p&gt;Learning statistics in this direction leads to &lt;strong&gt;a ton&lt;/strong&gt; of confusion, because we're taught &lt;em&gt;the shortcut to the answer&lt;/em&gt;, rather than the first-principles way of thinking about a problem. We end up with the &quot;standard t-test&quot; and multiple confusing names for regression modelling, masquerading as canned procedures that can be used on any problem. (OK, that's a bit of a stretch, but please do tell me you were &lt;em&gt;at least tempted to use the t-test in a situation where you just had to crank out an analysis&lt;/em&gt;...)&lt;/p&gt;
&lt;p&gt;After seeing the following tweet from Michael Betancourt...&lt;/p&gt;
&lt;p&gt;&lt;blockquote class=&quot;twitter-tweet&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;Unpopular opinion: regression models are _much_ harder to use well than generative models of an actual data generating process and consequently should not be the first and only modeling techniques that many people are taught.&lt;/p&gt;&amp;mdash; \mathfrak{Michael &amp;quot;El Muy Muy&amp;quot; Betancourt} (@betanalpha) &lt;a href=&quot;https://twitter.com/betanalpha/status/1272358065447329792?ref_src=twsrc%5Etfw&quot;&gt;June 15, 2020&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;...I realized that the only reason why &lt;a href=&quot;https://ericmjl.github.io/essays-on-data-science/machine-learning/markov-models/&quot;&gt;Markov Models and their variants&lt;/a&gt; clicked for me was thinking through the data generating process. The only reason why hierarchical models clicked for me was stepping through the data generating process on a real problem and linking them to statistical parameters. Without thinking through the data generating process, none of those models made any sense.&lt;/p&gt;
&lt;p&gt;In some sense, thinking through the data generating process is &lt;em&gt;an extremely natural thing to do&lt;/em&gt;. It's like telling a story about how our data came into being, and we know that telling stories is &lt;em&gt;exactly&lt;/em&gt; what humans are great at. Storytelling helps us reason about the world. There should be no reason why we don't use statistical storytelling to reason about our problems.&lt;/p&gt;
&lt;p&gt;Worrying &lt;em&gt;first&lt;/em&gt; about the data generating process and then about the inferential procedure makes statistical inference less of a black box and more of a natural conclusion of statistical storytelling. We become less concerned with whether something is &quot;significant&quot;, and instead more concerned with whether we &quot;got the model right&quot;.&lt;/p&gt;
&lt;p&gt;To put this into concrete action, I've been working on an alternative introduction to probabilistic programming and Bayesian inference that is lighter on math than most introductions, involves a lot of verbal storytelling, and goes heavier than most introductions in its use of programming. Here we practice the skill of hypothesizing a data generating story and translating that into the language of probability distributions, which can then be translated into SciPy stats Python code. Stay tuned!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2020/6/14/pab-to-phd/">
    <title type="text">P(A,B) to P(H,D)??</title>
    <id>urn:uuid:c4d3ae7b-9991-34db-bcfe-e14d22031f4f</id>
    <updated>2020-06-14T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2020/6/14/pab-to-phd/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;Bayes' rule looks like this:&lt;/p&gt;
&lt;p&gt;$$P(A|B) = 
\frac{P(B|A)P(A)}
{P(B)}
$$&lt;/p&gt;
&lt;p&gt;It is a natural result that comes straight from the rules of probability,
being that the joint distribution of two random variables
can be written in two equivalent ways:&lt;/p&gt;
&lt;p&gt;$$P(A, B) = P(A|B)P(B) = P(B|A)P(A)$$&lt;/p&gt;
&lt;p&gt;Now, I have encountered in many books write,
regarding the application of Bayes' rule to statistical modelling,
something along the lines of the following:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Now, there is an alternative &lt;em&gt;interpretation&lt;/em&gt; of Bayes' rule,
one that replaces the symbol &quot;A&quot; with &quot;Hypothesis&quot;,
and &quot;B&quot; with the &quot;Data&quot;, such that we get:&lt;/p&gt;
&lt;p&gt;$$P(H|D) = \frac{P(D|H)P(H)}{P(D)}$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;At first glance, nothing seems wrong about this statement,
but I did remember having a lingering nagging feeling
that there was a logical jump unexplained here.&lt;/p&gt;
&lt;p&gt;More specifically, that logical jump yielded the following question: &lt;em&gt;Why are we allowed to take this interpretation?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;It took asking the question to a mathematician friend, &lt;a href=&quot;https://colindcarroll.com/&quot;&gt;Colin Carroll&lt;/a&gt;,
to finally &quot;grok&quot; the idea.
Let me try to explain.&lt;/p&gt;
&lt;h2 id=&quot;spaces-of-models-and-data&quot;&gt;Spaces of models and data&lt;/h2&gt;&lt;p&gt;We have to think back to the fundamental idea of possible spaces.&lt;/p&gt;
&lt;p&gt;If we set up a Bernoulli probability distribution with parameter $p$,
then the space of possible probability distributions that we could instantiate is infinite!
This result should not surprise you: $p$ can take on any one of an infinite set of values between 0 and 1, each one giving a different instantiated Bernoulli.
As such, a &lt;code&gt;Bernoulli(p)&lt;/code&gt; hypothesis is drawn from a (very large) space of possible &lt;code&gt;Bernoulli(p)&lt;/code&gt;s,
or more abstractly, hypotheses, thereby giving us a $P(H)$.&lt;/p&gt;
&lt;p&gt;Moreover, consider our data.
The Bernoulli data that came to us, which for example might be &lt;code&gt;0, 1, 1, 1, 0&lt;/code&gt;,
were drawn from a near-infinite space of possible configurations of data.
First off, there's no reason why we always have to have three 1s and two 0s in five draws;
it could have been five 1s or five 0s.
Secondly, the order of data (though it doesn't really matter in this case)
for three 1s and two 0s might well have been different.
As such, we have the $P(D)$ interpretation.&lt;/p&gt;
&lt;p&gt;As a modelling decision, we &lt;em&gt;choose&lt;/em&gt; to say
that our data and model are jointly distributed,
thus we have the &lt;em&gt;joint distribution&lt;/em&gt;
between model and data, $P(H, D)$.&lt;/p&gt;
&lt;p&gt;And once we have the joint distribution between model and data,
we can begin to reason about Bayes' rule on it.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2020/6/9/with-what-symbols-do-you-write-thermodynamic-equations/">
    <title type="text">With what symbols do you write thermodynamic equations?</title>
    <id>urn:uuid:786ccbe2-be36-3bc3-8770-77adf43eb92e</id>
    <updated>2020-06-09T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2020/6/9/with-what-symbols-do-you-write-thermodynamic-equations/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;&lt;em&gt;A rant &lt;a href=&quot;https://twitter.com/vboykis/status/1270401190124310529&quot;&gt;egged on&lt;/a&gt; by &lt;a href=&quot;https://twitter.com/vboykis&quot;&gt;Vicki Boykis&lt;/a&gt; after reading her &lt;a href=&quot;http://veekaybee.github.io/2020/06/09/ml-in-prod/&quot;&gt;excellent essay on model deployment&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;/beginrant&lt;/p&gt;
&lt;p&gt;When I was in my first year of undergrad, I was in a first-year program in which the sciences were taught as an integrated unit. (The program, &lt;a href=&quot;https://scienceone.ubc.ca/&quot;&gt;Science One&lt;/a&gt;, was quite influential in my learning journey in learning across disciplinary silos.) There, the lecturers would coordinate their use of terminology and symbols in equations. The reason? They recognized that having defined, precise, and consistent terminology reduced learner confusion, increased retention, and aided in us linking key &lt;em&gt;concepts&lt;/em&gt;. In other words, because they knew that symbolism and choice of words mattered, they were able to more easily teach the things that &lt;em&gt;mattered more&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;One example of this was in thermodynamics. Heat, energy, and entropy have quite specific definitions, and in certain pedagogical traditions, they have the same equation forms but use different symbols. When we learned thermodynamics, we avoided all of that confusion by simply adopting the physicist's set of symbols. Nobody argued about it, because though the form changed, the &quot;spirit&quot; of the matter was preserved.&lt;/p&gt;
&lt;p&gt;Well, fast forward 14 years later, it appears the machine learning world has yet to learn the same lessons taught in Science One. No big deal, after all, Science One only educated 80 students at a time. But I think it's worth  writing about, because we might end up with a new generation of data scientists who can't tell the difference between inference and inference.&lt;/p&gt;
&lt;p&gt;Wait, what?! &quot;You're willing to die on the &lt;em&gt;inference&lt;/em&gt; hill?!&quot;&lt;/p&gt;
&lt;p&gt;In some ways, not. You might want to keep doing what you do, but hear me out. &lt;em&gt;I got reasonz, yo.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The following items are key components to a statistical modelling/machine learning workflow: (a) input data, (b) model/structure, (c) parameters to learn, and (d) output data. One can frame statistical learning methods in terms of a &quot;direction&quot; w.r.t. the output data (d). &quot;Simulation&quot;/&quot;prediction&quot; refer to the forward direction of going from (a-c) to (d), while &quot;inference&quot; or &quot;learning&quot; refer to the backward direction of going from (a, c, d) to (b). We use distinct terms because they are distinct and, in fact, basically opposite subclasses of the larger thing of calculating 1 thing from the other 3.&lt;/p&gt;
&lt;p&gt;&quot;Inference&quot;, then, has a specific definition in the quantitative sciences, rooted in its use in statistics. It refers to &quot;inferring&quot; the parameters of a model, given data. It is distinct from simple &quot;guessing&quot;, in that there are criteria (maximum likelihood, for example) for evaluating how good individual inferred values are.&lt;/p&gt;
&lt;p&gt;The term &quot;inference&quot;, however, has been  co-opted incorrectly by deep learning practitioners to refer to the task of &lt;em&gt;generating&lt;/em&gt; or &lt;em&gt;predicting&lt;/em&gt; what an output should be, given known input data and model structure + parameters. We are starting to hear the term  mis-used all the time: &quot;At inference time...&quot;, &quot;When performing inference...&quot;. How this came about, I do not know, but I have a hypothesis that the first few people who  co-opted the term were genuinely trying to make &lt;em&gt;some&lt;/em&gt; form of linguistic connection between the statistics and machine learning worlds. I shall not try to, ahem, &lt;em&gt;infer their intents from observed behaviour&lt;/em&gt; here though. :)&lt;/p&gt;
&lt;p&gt;But imagine the poor statistician trying to decipher what the machine learner is trying to say when they read the term, &quot;at inference time...&quot; It's like the chemist staring at the physicist's heat equation and being ever so subtly thrown off by symbol mismatch! Likewise, the statistician stands to be tripped up by the &lt;em&gt;completely opposite&lt;/em&gt; use of &quot;inference&quot; when reading a machine learning paper.&lt;/p&gt;
&lt;p&gt;So what's the big deal here? Languages evolve, don't they?&lt;/p&gt;
&lt;p&gt;Yes! But surely that's not the only criteria for letting words slip? If words and their precise meanings do not matter, we might as well &lt;code&gt;#abolish #communication&lt;/code&gt;! (If you disagree with me, go ahead and decipher what I &lt;em&gt;really&lt;/em&gt; mean by that, since my words didn't matter and I can evolve the language anyways...)&lt;/p&gt;
&lt;p&gt;Statistics and machine learning are extremely interrelated fields. Machine learning research built upon statistical foundations and has given statistics practitioners new model classes to work with and tooling to perform inference on models more easily (I'm thinking about things like automatic differentiation!). Both fields stand to learn a ton from one another, but also stand to lose a lot without precise and consistent vocabulary. If you accept the premise (and, if I may dare claim, &lt;em&gt;very common mental model&lt;/em&gt;) of directionality w.r.t. the output data,  then using the same term to refer to opposite-direction procedures is &lt;em&gt;bewilderingly&lt;/em&gt; confusing! Especially for learners of both domains.&lt;/p&gt;
&lt;p&gt;What's the solution going forth?&lt;/p&gt;
&lt;p&gt;The unfortunate reality is that the  co-opting of the term has already happened. The optimistic reality is that &lt;a href=&quot;https://twitter.com/PhDemetri/status/1270396350195212289&quot;&gt;there are individuals who are merely &quot;accepting of the fact but don't necessarily like/agree with it&quot;&lt;/a&gt;. Much like I push back on the use of &quot;aye eye&quot; at work, I think we can push our colleagues to a better standard of communication: lucid, consistent, precise, and concise.&lt;/p&gt;
&lt;p&gt;If you &lt;em&gt;still&lt;/em&gt; want to use the term inference for &quot;forward prediction/simulation&quot;, then prefix or suffix it &lt;a href=&quot;https://twitter.com/ryanstravis/status/1270400888885202948&quot;&gt;with some word.&lt;/a&gt; Call it &quot;output inference&quot; if you'd like. But make it distinct from &quot;parameter inference&quot;. (I just set an example, so there.)&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://media1.tenor.com/images/4b7d6e1687f09ba7fafd2d13b71321da/tenor.gif?itemid=15798788&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;If you agree, please share the message around.&lt;/p&gt;
&lt;p&gt;/endrant&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2020/6/2/data-science-design-manual/">
    <title type="text">Data Science Design Manual</title>
    <id>urn:uuid:7ac247ad-ec53-34d2-9c63-eccc216acdff</id>
    <updated>2020-06-02T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2020/6/2/data-science-design-manual/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;During this extraordinary COVID-19 time,
Springer did an extraordinary thing that I never expected:
They released a whole bucketload of books for free online!
One of them caught my eye: &lt;a href=&quot;https://link.springer.com/book/10.1007%2F978-3-319-55444-0&quot;&gt;&quot;The Data Science Design Manual&quot;&lt;/a&gt;.
Having browsed through the book PDF, I'm impressed by its coverage of the foundational topics
that I think &lt;em&gt;every&lt;/em&gt; data scientist should be equipped with:
statistical inference, data wrangling, linear algebra, and machine learning.
The author, Steven Skiena, also covers more in there.&lt;/p&gt;
&lt;p&gt;Go &lt;a href=&quot;https://link.springer.com/book/10.1007%2F978-3-319-55444-0&quot;&gt;grab the PDF&lt;/a&gt; while it's still free!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2020/5/31/development-containers/">
    <title type="text">VSCode Development Containers</title>
    <id>urn:uuid:be56b343-64d1-36de-8c13-9dca4204a97e</id>
    <updated>2020-05-31T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2020/5/31/development-containers/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;It's been about four days since I first discovered the idea of &quot;development containers&quot;, and I'm absolutely hooked.&lt;/p&gt;
&lt;p&gt;We know that &quot;containers&quot;, such as Docker or Singularity containers, give us operating system-level &quot;virtualization&quot;. Or in other words, it gives us completely isolated &lt;em&gt;runtime&lt;/em&gt; environments.&lt;/p&gt;
&lt;p&gt;Development containers take this idea one step further, and provide us with isolated &lt;em&gt;development&lt;/em&gt; environments. This actually can take down the time needed for a newcomer to the project to get up and running with software &lt;em&gt;development&lt;/em&gt;. They don't have to configure anything. In VSCode, simply &quot;open a repository in a remote container&quot; (a command palette command), and ensure that they have Docker running locally. Boom! Start developing.&lt;/p&gt;
&lt;p&gt;I added dev containers to &lt;code&gt;pyjanitor&lt;/code&gt;, my personal website, my Essays collection, and also started using Dockerhub to &lt;em&gt;prebuild&lt;/em&gt; the containers so that a newcomer &lt;em&gt;just has to pull the container and go!&lt;/em&gt; This stands in contrast to building the container locally, which is a power drain (if on battery) and can take time.&lt;/p&gt;
&lt;p&gt;I plan to do more of this at work, so I can onboard interns and new colleagues onto a project much faster. I also got running &lt;em&gt;remote&lt;/em&gt; development containers, i.e. containers that are running on my slightly more powerful remote GPU tower, but the VSCode client is running locally on my puny little 12&quot; MacBook. The future is looking bright! :)&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2020/5/26/easy-matplotlib-animations/">
    <title type="text">Easy `matplotlib` animations</title>
    <id>urn:uuid:7f5683b1-7f71-3837-bd3f-94861b0d3677</id>
    <updated>2020-05-26T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2020/5/26/easy-matplotlib-animations/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;Recently, [&lt;code&gt;celluloid&lt;/code&gt;][celluloid]caught my eye: it's a package that lets you create &lt;code&gt;matplotlib&lt;/code&gt; animations easily!&lt;/p&gt;
&lt;p&gt;If you need a dead-simple example to convince you to check it out, here's one lifted straight from the repository:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;celluloid&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Camera&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;camera&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Camera&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;camera&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;snap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;animation&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;camera&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;animate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;But seriously though, if you use the workhorse Python drawing package &lt;code&gt;matplotlib&lt;/code&gt; for anything,
this package can be considered to be one of those &quot;great tricks to have&quot; in your bag!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2020/5/17/what-are-lambda-expressions-in-python/">
    <title type="text">What are lambda expressions in Python?</title>
    <id>urn:uuid:facf9642-99b7-3f91-ab29-959466828397</id>
    <updated>2020-05-17T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2020/5/17/what-are-lambda-expressions-in-python/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;Inspired by a conversation I had with a colleague who is learning Python, I wanted to write down an explainer of what &quot;lambda expressions&quot; are in Python.&lt;/p&gt;
&lt;p&gt;You might have seen lambda expressions in someone else's Python code, which looks like such:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This, actually, is the equivalent of writing a function that we might name &lt;code&gt;is_even&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;is_even&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Here's an explainer of the &lt;em&gt;anatomy&lt;/em&gt; of the lambda function.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;lambda&lt;/code&gt; tells Python that we're constructing a function.&lt;/li&gt;
&lt;li&gt;The signature of the function, meaning, the arguments the function takes in, is given by everything between &lt;code&gt;lambda&lt;/code&gt; and &lt;code&gt;:&lt;/code&gt;. In our example, the signature of the function is &lt;code&gt;x&lt;/code&gt;, meaning the function only takes in a single argument, &lt;code&gt;x&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;The stuff the function returns is everything after the &lt;code&gt;:&lt;/code&gt;. In our case, it's the boolean result of &lt;code&gt;x % 2 == 0&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So a lambda function &lt;em&gt;basically&lt;/em&gt; equivalent to a Python function. The key difference here is that it is considered &quot;anonymous&quot;, in that we have not given it an explicit name.&lt;/p&gt;
&lt;p&gt;Let me explain. When we use the following pattern:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;func_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arg1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arg2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;something&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;the function has a name, given by &lt;code&gt;func_name&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;However, when we do a lambda function:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arg1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arg2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;something&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;this function doesn't have a name. Hence, the term &quot;anonymous&quot;.&lt;/p&gt;
&lt;p&gt;But what's the use of a lambda function if all it does is nothing more than be &quot;anonymous&quot;? Well, one place I have used lambda functions is when I determine that a function that I want to implement is a simple one-liner that can get slotted in anywhere. For example, in &lt;code&gt;pyjanitor&lt;/code&gt;, when transforming a column to see whether it's even:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform_column&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;my_column&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;is_even&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;That would be less verbose than:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;is_even&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;


&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform_column&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;my_column&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;is_even&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;is_even&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;RealPython has a great article which also details &lt;a href=&quot;https://realpython.com/python-lambda/#appropriate-uses-of-lambda-expressions&quot;&gt;the appropriate uses of lambda expressions&lt;/a&gt;; definitely check it out!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2020/4/21/use-pyprojroot-and-pythons-pathlib-to-manage-your-data-paths/">
    <title type="text">Use pyprojroot and Python’s pathlib to manage your data paths!</title>
    <id>urn:uuid:4cfd25f8-3da4-3218-8cad-2b4edbdceb65</id>
    <updated>2020-04-21T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2020/4/21/use-pyprojroot-and-pythons-pathlib-to-manage-your-data-paths/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;If you adopt a proper organizational structure for your data projects,
then each project gets its own directory (i.e. a clean and isolated &quot;workspace&quot;)
and its own isolated analysis environment (e.g. a &lt;code&gt;conda&lt;/code&gt; environment).&lt;/p&gt;
&lt;p&gt;In that workspace, your directory structure might look like this:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;project/
- data/
- notebooks/
- src/
- setup.py
- README
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;As such, your notebook are all going to be in a different directory from your data.
This is one way that keeps the mind sane:
you might have subdirectories in the &lt;code&gt;notebooks/&lt;/code&gt; directory
that you use to organize the notebooks further,
yet you have multiple notebooks that use the same file,
leading to brittle path linking.
After all in one notebook, you might do:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pd&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;data.csv&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;But in another notebook that lives in a different directory,
to link to the dataset, you might have to do:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pd&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;../other_dir/data.csv&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The potential for confusion is just immense here.&lt;/p&gt;
&lt;p&gt;A better way is to provide one authoritative path to a particular dataset that you can use.
For example:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pd&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;../data/data.csv&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;But even that is a bit tricky: if you move the notebook for whatever good reason,
the path to the data might break.
It’s still brittle.
We need a better way to resolve paths.&lt;/p&gt;
&lt;p&gt;Enter &lt;code&gt;pyprojroot&lt;/code&gt;.
Written by my fellow PyData conference doppleganger Daniel Chen,
it provides a &lt;code&gt;here&lt;/code&gt; function that will resolve to your project root directory (hence the package name).
The original was written in R (&lt;code&gt;rprojroot&lt;/code&gt;),
and it’s a wonderful tool for data scientists.
Let’s see it in action:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pd&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pyprojroot&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;here&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;here&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;data/data.csv&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And &lt;em&gt;voila&lt;/em&gt;!
No fragile relative paths,
and no perpetually long chains of &lt;code&gt;../../..&lt;/code&gt;!
Just nice and clean resolution to your project root.&lt;/p&gt;
&lt;p&gt;How does it work?
What &lt;code&gt;pyprojroot&lt;/code&gt; does underneath the hood is
recursively climb the file tree until it finds
one of a set of pre-specified files
that are commonly found in a project’s root directory.
For example, &lt;code&gt;.git&lt;/code&gt; is a common one.
For Python packages, &lt;code&gt;setup.py&lt;/code&gt; is another.&lt;/p&gt;
&lt;p&gt;If your project doesn’t &quot;fit&quot; any of the conventions assumed,
or if you have a fancier structure,
you can always add a &lt;code&gt;.here()&lt;/code&gt; to your project root,
and configure the &lt;code&gt;project_files&lt;/code&gt; keyword argument
so that &lt;code&gt;here&lt;/code&gt; only looks for that one authoritative file:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pd&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pyprojroot&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;here&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;root&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;here&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;project_files&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;.here&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;root&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;data/data.csv&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And what &lt;em&gt;exactly&lt;/em&gt; is the &lt;code&gt;here&lt;/code&gt; function returning?
Well, it’s returning a &lt;code&gt;pathlib.Path&lt;/code&gt; object,
which has some seriously clever patching
to allow it to work with the &lt;code&gt;/&lt;/code&gt; operator
to represent paths in native Python code!&lt;/p&gt;
&lt;p&gt;Now, let us all toast to cleaner path resolution in our data projects!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2020/4/12/introducing-a-new-essay-on-markov-models/">
    <title type="text">Introducing a new essay on Markov models</title>
    <id>urn:uuid:3dc37d24-a8bf-3848-8eee-bc25e8c4e3cc</id>
    <updated>2020-04-12T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2020/4/12/introducing-a-new-essay-on-markov-models/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;Two weeks ago on a Monday afternoon, I hacked on a work project with a colleague Zachary Barry (who was in the same program as I was in grad school). In that project, we wanted to build an autoregressive hidden Markov model that could model motion. (I cannot go further than that, given that this touches front-line workplace work.) This turned out to be both&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;non-trivial, because of the high degree of problem-specificity involved, and&lt;/li&gt;
&lt;li&gt;difficult to grok, because of the lack of explanatory papers and teaching material out there that was oriented towards programming-types.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Not all was lost, though, as the last time I touched Markov models was in 2014, when I first encountered it in a computational biology class at MIT. Leveraging that, my familiarity with PyMC3, and Zach's prior background, we hacked together a prototype that he could take forth (or that we would at least schedule another hack session for).&lt;/p&gt;
&lt;p&gt;That second difficulty, though, stuck with me. Since I couldn't find a satisfactory explainer on Markov models for programmers, I decided I'd write one.&lt;/p&gt;
&lt;p&gt;The result is the &lt;a href=&quot;https://ericmjl.github.io/essays-on-data-science/machine-learning/markov-models/&quot;&gt;latest essay&lt;/a&gt; on Markov models, which I have just posted to my &lt;a href=&quot;https://ericmjl.github.io/essays-on-data-science/&quot;&gt;Essays collection&lt;/a&gt;. This is a culmination of two days of continuous writing plus two weeks of review by gracious colleagues and collaborators who have generously given their time to help me improve it.&lt;/p&gt;
&lt;p&gt;In this essay, we cover what Markov models are, interleaving prose, equations, code, and figures to help communicate to programmer-types what exactly Markov models are, particularly the mathematics behind them but presented with more code than equations.&lt;/p&gt;
&lt;p&gt;It's also a preview of the kind of writing I'd like to focus on going forth. I'm launching a monthly newsletter curating programmer-friendly data science tools, tips and techniques at &lt;a href=&quot;https://tinyletter.com/ericmjl&quot;&gt;Tinyletter&lt;/a&gt;. And if you'd like to support &lt;strong&gt;more programmer-oriented data science learning material&lt;/strong&gt;, please consider pledging me a cup of coffee on &lt;a href=&quot;https://www.patreon.com/ericmjl&quot;&gt;Patreon&lt;/a&gt;!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2020/3/25/resources-for-learning-python-during-covid-19/">
    <title type="text">Resources for learning Python during COVID-19</title>
    <id>urn:uuid:e523a41c-c989-339b-9a41-a7d36218c837</id>
    <updated>2020-03-25T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2020/3/25/resources-for-learning-python-during-covid-19/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;With COVID-19 on hand, you might have some time to go deeper into Python programming.&lt;/p&gt;
&lt;p&gt;I'd like to recommend some resources for you, in case you wish to brush up, learn, or go deeper.&lt;/p&gt;
&lt;h2 id=&quot;[datacamp]-https://www.datacamp.com/&quot;&gt;&lt;a href=&quot;https://www.datacamp.com/&quot;&gt;DataCamp&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;If you're a complete beginner, I recommend going with DataCamp.
I say this with a full up-front disclosure that I'm a DataCamp instructor,
and if you make it to the Network Analysis courses, then I benefit from you.
(If you don't, then I don't.)&lt;/p&gt;
&lt;p&gt;The reasons why I recommend DataCamp is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;They have a hosted compute environment that you use,
thus freeing you from the potentially tricky task of navigating
how best to install Python on your local system,
and letting you focus on learning Python.&lt;/li&gt;
&lt;li&gt;The exercises are designed in such a way to be bite-sized,
so you can pace yourself through the curriculum.
A little practice every day goes a very long way to picking it up.
This I know because I also had to design the network analysis curricula the same way.&lt;/li&gt;
&lt;li&gt;There are clearly delineated specializations for both R and Python programmers,
and psychologically, it can be very rewarding to navigate a pre-defined path for a learner, thus easier to keep up the learning.
(This is also a very smart business strategy that ed-tech firms use.)&lt;/li&gt;
&lt;li&gt;Beyond just the beginner-oriented courses, they go all the way up to the core PyData stack through to deep learning. &lt;/li&gt;
&lt;li&gt;Finally, there are example projects that they recommend to you when you've passed the pre-requisite courses, thus giving you more practice.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The reason why you wouldn't want to go with DataCamp is because you have to pay for it,
and maybe because you don't trust the words of someone who has a conflict of interest.
(I don't blame you, I'd take the same position.)&lt;/p&gt;
&lt;p&gt;You might need to discuss with your management if your team is going to sponsor your learning.&lt;/p&gt;
&lt;h2 id=&quot;[coursera]-https://www.coursera.org/&quot;&gt;&lt;a href=&quot;https://www.coursera.org/&quot;&gt;Coursera&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Coursera has a wide range of offerings for learning Python,
and they are completely free with certification
if your organization sponsors it (my current employer Novartis includes this as a perk).
(Otherwise, it's just free - also hard to beat.)
Just on the basis of price I would not hesitate to recommend it.&lt;/p&gt;
&lt;p&gt;That said, there's a lot of courses to choose from.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The well-reviewed ones, that also come as a series, generally are from the University of Michigan.&lt;/li&gt;
&lt;li&gt;JHU also provides some neat ones, such as &quot;Python for Genomic Data Science&quot;.&lt;/li&gt;
&lt;li&gt;As a prior (i.e. having not seen the course content), I would avoid the ones by IBM and other big firms.
Prior experience has told me they're likely not hands-on (though I might be wrong).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;One of the bigger issues here is that you might need to learn how to set up your own Python programming environment.
For this, consult your friendly neighborhood &lt;a href=&quot;https://www.reddit.com/r/ProgrammerHumor/comments/7pxp2b/parsertongue/&quot;&gt;parsertongue&lt;/a&gt; speaker
(i.e. a colleague who knows Python) to help you.&lt;/p&gt;
&lt;h2 id=&quot;[think-python]-https://greenteapress.com/wp/think-python-2e/&quot;&gt;&lt;a href=&quot;https://greenteapress.com/wp/think-python-2e/&quot;&gt;Think Python&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Think Python is an online book by Allen Downey,
a professor of computer science and more at the Olin College of Engineering in Needham, MA.
Allen is also a fellow Python community educator,
and has generously let me test-drive my deep learning tutorials at his classes.&lt;/p&gt;
&lt;p&gt;In this book, Allen leverages the friendliness of the Python programming language to teach you basic computing concepts.&lt;/p&gt;
&lt;p&gt;Allen is incredibly generous, and has made the book freely available online, though you can buy it via Amazon or O'Reilly Media.&lt;/p&gt;
&lt;p&gt;He's got other titles for those who already know Python:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://greenteapress.com/thinkstats/&quot;&gt;Think Stats&lt;/a&gt;: Statistics taught using computation.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://greenteapress.com/wp/think-bayes/&quot;&gt;Think Bayes&lt;/a&gt;: &lt;em&gt;Bayesian&lt;/em&gt; statistics (YAY!) taught using computation.
(I used this book to get brushed up on Bayesian inference when in grad school.)&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://greenteapress.com/complexity/&quot;&gt;Think Complexity&lt;/a&gt;: A computation-oriented introduction to data science.
This book actually kickstarted half of my doctoral thesis.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;[automate-the-boring-stuff-with-python]-https://automatetheboringstuff.com/&quot;&gt;&lt;a href=&quot;https://automatetheboringstuff.com/&quot;&gt;Automate the Boring Stuff with Python&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;This is a book by a Python friend of mine, Al Swiegart, whom I met at the annual PyCon USA.&lt;/p&gt;
&lt;p&gt;In this book, Al teaches you how to use Python code
to automate all the boring, repetitive stuff that you encounter in your day-to-day work on a computer.
It's this kind of project, which directly impacts your day-to-day,
that can keep your motivation levels high while learning.&lt;/p&gt;
&lt;p&gt;Al is also incredibly generous, and has made the book freely available online,
and sometimes gives away the physical book for free.
But as it's one of his income sources, I'd encourage you to buy the book
(just as I did, even though I don't really need to read it anymore). &lt;/p&gt;
&lt;h2 id=&quot;[e2eml-by-brandon-rohrer]-https://end-to-end-machine-learning.teachable.com/courses&quot;&gt;&lt;a href=&quot;https://end-to-end-machine-learning.teachable.com/courses&quot;&gt;e2eML by Brandon Rohrer&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;This is an online course created by Brandon Rohrer,
who is a data scientist at iRobot (the maker of those fancy robot vacuums!).&lt;/p&gt;
&lt;p&gt;Brandon is pretty active on social media, and is a fellow education enthusiast.
(I sometimes wish my current role could include formal classroom teaching as part of my professional goals.)
With this online course, he brings you through one opinionated path to getting good with machine learning and data science.
As a pre-requisite, you should know how to set up your own Python programming environment,
as there's no hosted computing environment for you.&lt;/p&gt;
&lt;h2 id=&quot;your-own-project&quot;&gt;Your Own Project&lt;/h2&gt;&lt;p&gt;If you're proficient with some basic Python, and in particular, have learned how to use &lt;code&gt;pandas&lt;/code&gt;,
then you might want to take the time to re-do an analysis that you did once, except now done in &lt;code&gt;pandas&lt;/code&gt; and Python.&lt;/p&gt;
&lt;p&gt;Doing so will get you lots of practice in what &lt;em&gt;actual&lt;/em&gt; day-to-day data science programming looks like, where you:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;encounter new error messages you've never seen before,&lt;/li&gt;
&lt;li&gt;have to learn how to ask the &quot;right&quot; questions to debug them,&lt;/li&gt;
&lt;li&gt;might end up pinging a friend/colleague to help you,&lt;/li&gt;
&lt;li&gt;end up knowing deeply how to solve that problem, because &lt;strong&gt;you&lt;/strong&gt; encountered it for yourself.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If possible, write a blog post as well, to document your learning journey, especially explaining how you solved the problem.
I have found this to be an incredibly effective way of making that thing I just learned stick in memory.&lt;/p&gt;
&lt;h2 id=&quot;scipy/pycon-conferences&quot;&gt;SciPy/PyCon conferences&lt;/h2&gt;&lt;p&gt;The SciPy and PyCon conferences are two annual Python conferences that I have attended since grad school,
and they have a wealth of resources available for learners.&lt;/p&gt;
&lt;p&gt;There are YouTube playlists available for each of them
(&lt;a href=&quot;https://www.youtube.com/user/EnthoughtMedia/playlists?view=50&amp;amp;sort=dd&amp;amp;shelf_id=1&quot;&gt;SciPy&lt;/a&gt;
and &lt;a href=&quot;https://www.youtube.com/channel/UCxs2IIVXaEHHA4BtTiWZ2mQ&quot;&gt;PyCon&lt;/a&gt;).
PyCon has a new YouTube channel each year, while SciPy uses Enthought Media's own channel.&lt;/p&gt;
&lt;p&gt;If you dig deep enough, my tutorials are available online as well, freely available for anybody to watch.
(I link them from my &lt;a href=&quot;https://ericmjl.github.io/teaching/&quot;&gt;personal website&lt;/a&gt; if you want a shortcut there...
alrighty, enough of my shameless self-promotion.)&lt;/p&gt;
&lt;p&gt;Some videos that have been helpful in my own learning journey:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://youtu.be/fMycLa1bsno&quot;&gt;Computational Statistics 1 by Allen Downey&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://youtu.be/heFaYLKVZY4&quot;&gt;Computational Statistics 2 by Chris Fonnesbeck&lt;/a&gt;
(Chris Fonnesbeck is an ex-Vanderbilt biostatistics professor who quit and joined the Yankees.
He is also the creator and BDFL of PyMC, for which I help out with development.)&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://youtu.be/mbfsog3e5DA&quot;&gt;Parallelizing Scientific Python with Dask by Jim Crist &amp;amp; co.&lt;/a&gt;
(Dask is a highly productive tool for interactive parallel data science!)&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://youtu.be/oGzU688xCUs&quot;&gt;Pandas for Data Analysis by Daniel Chen&lt;/a&gt;.
(Dan Chen and I are often mistaken for each other. He also has a good book for &lt;code&gt;pandas&lt;/code&gt;.)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The YouTube videos are quite good for those who have some basic knowhow on managing their own Python environments.
There are some beginner-friendly ones that show you how to get started with Python too.
In particular, &lt;a href=&quot;https://www.youtube.com/watch?v=pB3BFP001co&amp;amp;list=PLytH1Th3ORyNIBPyOQg20CsVhVIJh1u2a&quot;&gt;this playlist&lt;/a&gt;
should cover everything for you.&lt;/p&gt;
&lt;h2 id=&quot;pandas-resources&quot;&gt;&lt;code&gt;pandas&lt;/code&gt; Resources&lt;/h2&gt;&lt;p&gt;If you're already proficient with Python, then learning &lt;code&gt;pandas&lt;/code&gt; can only help you.
&lt;code&gt;pandas&lt;/code&gt; is the idiomatic package for working with data tables in Python.
Knowing how to use it can help you be productive 
when working with tables that come from collaborators,
or data from databases.&lt;/p&gt;
&lt;p&gt;Here's some resources for picking up &lt;code&gt;pandas&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://tomaugspurger.github.io/modern-1-intro.html&quot;&gt;Modern Pandas&lt;/a&gt; by &lt;a href=&quot;https://tomaugspurger.github.io/&quot;&gt;Tom Augspurger&lt;/a&gt;. Tom is also someone I have met on the conference circuit.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.amazon.com/Pandas-Cookbook-Scientific-Computing-Visualization-ebook/dp/B06W2LXLQK&quot;&gt;Pandas Cookbook&lt;/a&gt; by &lt;a href=&quot;https://www.linkedin.com/in/tedpetrou/&quot;&gt;Ted Petrou&lt;/a&gt;, a Python trainer and consultant.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.amazon.com/Pandas-Everyone-Analysis-Addison-Wesley-Analytics-ebook/dp/B0789WKTKJ&quot;&gt;Pandas for Everyone&lt;/a&gt; by &lt;a href=&quot;https://chendaniely.github.io/&quot;&gt;Daniel Chen&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;other-notes&quot;&gt;Other notes&lt;/h2&gt;&lt;h3 id=&quot;setting-up-python-locally&quot;&gt;Setting up Python locally&lt;/h3&gt;&lt;p&gt;Thus far, I've alluded to &quot;setting up your own Python environment&quot; many times.
To demystify what I mean by that, it really boils down to installing
the &lt;a href=&quot;https://www.anaconda.com/distribution/&quot;&gt;Anaconda distribution&lt;/a&gt; of Python more than anything else.
(&lt;strong&gt;Don't download the Python 2.7 version, it's outdated!&lt;/strong&gt;)
The Anaconda Python distribution, distributed by the distribution namesake Anaconda,
solved a lot of Python packaging problems that weren't actively being solved in the early 2010s,
and &quot;robustified&quot; the distribution of Python packages.
I myself was once skeptical about using it,
until I screwed up my own system Python installation and broke iPhoto.
That's when I finally bit the bullet and installed it - and never looked back since.&lt;/p&gt;
&lt;h3 id=&quot;jupyter&quot;&gt;Jupyter&lt;/h3&gt;&lt;p&gt;You might encounter the name &quot;Jupyter&quot;, and think that &quot;Jupyter&quot; provides packages.
This is incorrect - Jupyter is the name of an ecosystem of tools that data scientists use,
and it covers&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a computation notebook (&quot;Jupyter Notebook&quot;) where you can weave code, prose, and figures together,&lt;/li&gt;
&lt;li&gt;connectors to computation engines (&quot;Kernels&quot;), such as a Python or R or Scala language kernel,
that in turn houses the packages you install, and&lt;/li&gt;
&lt;li&gt;an integrated development environment (&quot;Jupyter Lab&quot;), which provides you the interface for coding in the browser.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;IPython is the precursor monolith project to Jupyter, and it was primarily focused on the computation engine and notebook.&lt;/p&gt;
&lt;p&gt;Hope that disambiguates the terms for you.&lt;/p&gt;
&lt;h3 id=&quot;the-most-important-trick-for-learning...&quot;&gt;The Most Important Trick For Learning...&lt;/h3&gt;&lt;p&gt;...is nothing more than getting practice every single day.&lt;/p&gt;
&lt;p&gt;Even if it's only for a single DataCamp exercise, getting that practice in &lt;em&gt;daily&lt;/em&gt; is important for mastery.
Otherwise, your time spent learning now will simply go to waste,
filed away in the &quot;I guess I learned it&quot; cabinet never to be retrieved and tested again.
If you want the knowledge to stick, you need to increase the odds that you'll get practice every day.&lt;/p&gt;
&lt;p&gt;If you have a project you need to solve, you increase the odds that you'll get practice every day.&lt;/p&gt;
&lt;p&gt;If you have sunken costs (time or money) into a course, you increase the odds that you'll practice every day.&lt;/p&gt;
&lt;p&gt;If you have a community of learners to learn with, you increase the odds that you'll practice every day.&lt;/p&gt;
&lt;p&gt;If you have a resource person with whom you click that you can ask questions of, you increase the odds that you'll practice every day.&lt;/p&gt;
&lt;h3 id=&quot;other-resources&quot;&gt;Other resources&lt;/h3&gt;&lt;p&gt;I clearly have a biased view of the world; if you have other resources for learning Python, don't hesitate to DM me or share them with your friends!&lt;/p&gt;
&lt;h2 id=&quot;finally...&quot;&gt;Finally...&lt;/h2&gt;&lt;p&gt;Stay safe, y'all. Stay indoors, stay away from other people, and keep washing your hands!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2020/3/15/what-can-data-scientists-do-during-covid-19/">
    <title type="text">What can data scientists do during COVID-19?</title>
    <id>urn:uuid:c84a85ee-6051-3c04-9820-9a8fba1727f9</id>
    <updated>2020-03-15T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2020/3/15/what-can-data-scientists-do-during-covid-19/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;A friend in Austria pinged me with the following question:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;I was wondering if you have any ideas of how I (and other link-minded) people could help with combating this threat. Do you know how a Data Scientist can contribute to our global efforts?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;My response is below.
The tl;dr version is:
&lt;em&gt;Your number-crunching and communication skills,
paired with independent re-analyses of publications,
can help you with local mitigation efforts.&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;The usual measures are your best bet:
stay at home,
interact with people virtually if you need to do meetings,
and minimize number of trips to the grocery store.&lt;/p&gt;
&lt;p&gt;That said, I'm sure you pinged me for a different answer :)&lt;/p&gt;
&lt;p&gt;Here's what I can think of.
The overarching theme is to use your skills as a data scientist,
particularly number-crunching and communication skills,
to help re-emphasize and re-communicate the good practices
that the epidemiology experts have been saying all along,
the only difference being we're local to our communities,
and can advocate among our own community.&lt;/p&gt;
&lt;p&gt;Firstly, independently replicating others' analyses is a good place to start.
This doesn't necessarily mean ML models - it can mean an ODE model for case counts,
or &lt;a href=&quot;https://github.com/twiecki/covid19/blob/master/covid19_growth_bayes.ipynb&quot;&gt;Bayesian models for estimation of growth rate&lt;/a&gt;, etc.
Doing this will take some deep diving into the epidemiology literature,
as epidemiologists have given much more thought to how to properly count cases and the likes than we have.&lt;/p&gt;
&lt;p&gt;Once you've done the first,
extend those models to show how certain interventions that have been strongly recommended -
such as the social distancing one I am emphasizing as well -
can impact the spread of the virus.
I trust the recommendation from epi experts to practice social distancing
because I was once adjacent to the epi world (with my flu research),
but I'm not sure others share the same level of trust.
The lack of trust is probably partially due to the distance most of us have
from a professional epidemiologist.
It helps for one to hear it from someone one knows personally,
especially someone one can trust
who also has the necessary number-crunching and communication skills.&lt;/p&gt;
&lt;p&gt;Thirdly, produce communications of some kind (posters, blog posts, infographics)
that help you communicate to others in your local community.
This, I think, is something we can already do without doing any independent re-analyses,
but the message among your local community might be more effective
if you have already done your homework by doing independent analysis replication.
(There's just something about being prepared to answer questions that helps
when preparing communication pieces.)&lt;/p&gt;
&lt;p&gt;Some caveats when doing this.&lt;/p&gt;
&lt;p&gt;Firstly, if you find something in your analyses that disagrees with the experts,
be ready to hold your own view with a grain of salt.
There may be a key assumption you missed.
Dig into the literature,
ping your local epidemiology experts for help,
and discuss with other data-oriented people.
A good prior is,
&quot;I'm wrong, they're probably right, I might need to re-look what I've done.&quot;&lt;/p&gt;
&lt;p&gt;Secondly, making dashboards/notebooks and publishing them is fun,
but in these times, I would call them cheap fun if it isn't paired with local action.
(To know why I call it cheap fun,
see &lt;a href=&quot;https://medium.com/nightingale/ten-considerations-before-you-create-another-chart-about-covid-19-27d3bd691be8&quot;&gt;this article on #vizresponsibly&lt;/a&gt;.
There's harder but more satisfying work to be done actually helping to influence local action
than publishing something to the web and tweeting about it...
unless doing that tweeting thing actually helps with influencing local action.
Use good judgment here, and don't go for cheap fame.
I trust you won't.&lt;/p&gt;
&lt;p&gt;From my time next to the epi world,
the best thing you can do is, of course, encouraging local action:
social distancing,
leaving masks for medical personnel (so don't go raiding shelves),
being prepared with 2 weeks of food supplies,
and don't go to the hospital unless you have medical symptoms (like fevers),
so you don't contribute to overwhelming the medical system.&lt;/p&gt;
&lt;p&gt;And while in isolation,
don't forget to take care of your mental health.
Nerds like myself can deal with the isolation,
but others might need some social interaction.
Audio/video call someone if you need to.
One of the nice things of the internet age
is that it's easier to stay connected than ever before.&lt;/p&gt;
&lt;p&gt;Finally, stay kind to the local grocery staff.
They're probably under a lot of mental and physical stress
with people flowing through their workplace.
Don't sneeze in their vicinity!
They're keeping the shelves stocked for the good of everybody.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2020/2/13/one-weird-trick-to-speed-up-your-tensorflow-model-100x/">
    <title type="text">One Weird Trick to Speed Up Your TensorFlow Model 100X...</title>
    <id>urn:uuid:4f298f94-5a92-3d64-80a9-72c43ed5d026</id>
    <updated>2020-02-13T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2020/2/13/one-weird-trick-to-speed-up-your-tensorflow-model-100x/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;You’ve got a TensorFlow 1.13 model on hand,
written by an academic group,
and you know it’s going to be useful,
because you’ve already done some preliminary experiments with it.
The model takes in protein sequences,
and returns vectors, one per protein sequence.
The vectors turn out to be excellent descriptors
for downstream supervised learning,
to predict protein activity from sequence data.&lt;/p&gt;
&lt;p&gt;The only problem?
It’s slow.
&lt;em&gt;Excruciatingly slow&lt;/em&gt;.
Not just on CPU.
It’s slow on your local GPU as well.
You can’t run this locally without waiting for hours
to process the thousands of protein sequences that you have on hand.&lt;/p&gt;
&lt;h2 id=&quot;slow?-what-now?&quot;&gt;Slow? What now?&lt;/h2&gt;&lt;p&gt;What do we do, then?
Install a newer GPU with faster processors and more RAM?
&quot;Lift and shift&quot; onto cloud GPUs?&lt;/p&gt;
&lt;p&gt;I’d argue that all of those are evil choices,
because premature optimization is the root of all evil (Donald Knuth).
Moving the model onto a GPU
without knowing the root cause of the model’s slowness
could be construed as a form of premature optimization,
because we are prematurely moving to more powerful hardware
without first finding out whether it can be improved on existing ones.
Moving the model onto a &lt;em&gt;cloud&lt;/em&gt; GPU,
renting another person’s computer
while not maximizing pushing the model to its extremes locally,
is probably even worse,
because we end up with more variables to debug.
By the aforementioned premises and Knuth’s logical conjugation,
those options are, therefore, evil choices.&lt;/p&gt;
&lt;p&gt;Rather provocatively,
I’d like to suggest that one right option
when faced with a very slow deep learning model
is to make it fast on a &lt;em&gt;single CPU&lt;/em&gt;.
If we make the model fast locally on a single CPU,
then we might have a good chance
of dissecting the source of the model’s slowness,
and thus make it really fast on different hardware.&lt;/p&gt;
&lt;p&gt;This is exactly what we did with the UniRep model.
Our protein engineering intern in Basel, Arkadij,
had already done the groundwork testing the model
on our high performance computing cluster.
It was slow in our hands,
and so we decided it was time to really dig into the model.&lt;/p&gt;
&lt;h2 id=&quot;reimplementation&quot;&gt;Reimplementation&lt;/h2&gt;&lt;p&gt;Because the model is an RNN model that processes sequential data,
we need to know how the model processes one window of sequence first.
In the deep learning literature, this is known as an &quot;RNN cell&quot;.
We isolated that part of the RNN,
and carefully translated each line of code into NumPy.
Because it processes one &quot;window&quot; or &lt;strong&gt;&quot;step&quot;&lt;/strong&gt; at a time,
we call it &lt;code&gt;mlstm1900_step&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Because the RNN cell is &quot;scanned&quot; over a single sequence,
we then defined the function that processes a single sequence.
To do this, we took advantage of the &lt;code&gt;jax.lax&lt;/code&gt; submodule’s &lt;code&gt;scan&lt;/code&gt; function,
which literally &quot;scans&quot; the RNN cell over the entire sequence
the way TensorFlow’s RNN cell would be scanned.
&lt;del&gt;This basically is a vectorized for-loop.&lt;/del&gt;
Thanks to Stephan Hoyer &lt;a href=&quot;https://twitter.com/shoyer/status/1228416820119400448&quot;&gt;pointing out my conceptual error here&lt;/a&gt;;
&lt;code&gt;lax.scan&lt;/code&gt; is a compiled for-loop with known number of iterations.
Together, that defined the function call that processes a single sequence.
This, incidentally, gives us the &quot;batch-&quot; or &quot;sample-wise&quot; RNN function,
&lt;code&gt;mlstm1900_batch&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Finally, we need a function that processes multiple sequences
that are the same length.
Here, we need the function to be vectorized,
so that we do not incur a Python loop penalty
for processing sequences one at a time.
This forms the &lt;code&gt;mlstm1900&lt;/code&gt; layer,
which, semantically speaking, takes in a batch of sequences
and returns a batch of outputs.&lt;/p&gt;
&lt;h2 id=&quot;testing&quot;&gt;Testing&lt;/h2&gt;&lt;p&gt;So, does the reimplementation work in the same way as the original?
Absolutely. We passed an &lt;em&gt;attrappe&lt;/em&gt; (Google it, it’s German) sequence through the model,
and checked that the reps were identical to the original - which they were.
But don’t take my word for it,
check out the &lt;a href=&quot;https://elarkk.github.io/jax-unirep/&quot;&gt;draft preprint&lt;/a&gt; we wrote to document this!
Meanwhile, in case you don't want to click over and read our hard work,
here's the figure linked from the source.
(It's only off by a bit because we added a delta
to make the two representations visible.)&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://elarkk.github.io/jax-unirep/figures/rep_trace_lf.png&quot; alt=&quot;Congruence between the reps.&quot;&gt;&lt;/p&gt;
&lt;p&gt;Besides that,
we also added tests and elaborate docstrings
for the functions in there,
so anybody reading the model source code
can know what the &lt;em&gt;semantic meaning&lt;/em&gt; of each of the tensors’ axes are.
Having wrestled through undocumented academic code,
knowing what input tensor dimensions were supposed to be
were one of the hardest things to grok
from looking at a relatively poorly undocumented model.&lt;/p&gt;
&lt;p&gt;By carefully reimplementing the deep learning model in pure JAX/NumPy,
we were able to achieve approximately 100X speedup
over the original implementation on a single CPU.
Given JAX’s automatic compilation to GPU and TPU,
the speed improvements we could obtain might be even better,
though we have yet to try it out.
Again, don't take my word at it -
check out the &lt;a href=&quot;https://elarkk.github.io/jax-unirep/&quot;&gt;draft preprint&lt;/a&gt;
if you're curious!
The exact figure, though is below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://elarkk.github.io/jax-unirep/figures/speed_comparison.png#center&quot; alt=&quot;Speed comparison between the original and `jax`-based reimplementation.&quot;&gt;&lt;/p&gt;
&lt;p&gt;I suspect that by reimplementing the model in NumPy,
we basically eliminated the TensorFlow graph compilation overhead,
which was the original source of slowness.
By using some of JAX’s tricks, including &lt;code&gt;lax.scan&lt;/code&gt; and &lt;code&gt;jax.vmap&lt;/code&gt;,
we could take advantage of vectorized looping
that the JAX developers built into the package,
thus eliminating Python looping overhead.&lt;/p&gt;
&lt;h2 id=&quot;distribution&quot;&gt;Distribution&lt;/h2&gt;&lt;p&gt;So, now, how do we get the model distributed to end-users?
This is where thinking about the API matters.
I have this long-held opinion that data scientists
need some basic proficiency in software skills,
as we invariably end up doing tool-building whether we want to or not.
Part of software skills involves knowing
how to package the model for use in a variety of settings,
and thinking about how the end-user would use it.
Then, only, have we made a &lt;strong&gt;data product&lt;/strong&gt; that is of use to others.&lt;/p&gt;
&lt;p&gt;Two settings we thought of are the Python user setting, and a web user setting.&lt;/p&gt;
&lt;h3 id=&quot;python-user-setting&quot;&gt;Python-user setting&lt;/h3&gt;&lt;p&gt;Serving Python users means building &lt;strong&gt;&lt;code&gt;pip&lt;/code&gt;-installable packages&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;In the Python world,
it’s idiomatic to consider a Python package
as the unit of software that is shipped to others.
Some packages are large (think NumPy),
others are diverse (think SciPy),
some are both (e.g. &lt;code&gt;scikit-learn&lt;/code&gt;),
and yet others do one and only one thing well (think &lt;code&gt;humanize&lt;/code&gt;).
UniRep probably falls within to the last category.&lt;/p&gt;
&lt;p&gt;To fit this idiom,
we decided to package up the model as a Python-style package
that is pip-installable into the environment of an end-user.
While we develop the package,
it is &lt;code&gt;pip&lt;/code&gt;-installable from our GitHub repository
(where we also give full credit and reference to the original authors for the original).
We might release it to PyPI, with licenses and the likes as well.&lt;/p&gt;
&lt;p&gt;Moreover, we needed a &lt;strong&gt;nice API&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;With UniRep, there’s basically two things one would want to do:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Compute &quot;reps&quot; for a sequence conditioned on the pre-trained weights, and&lt;/li&gt;
&lt;li&gt;Evolutionarily-tune (&quot;evo-tune&quot;) the model weights to compute more locally-relevant reps.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As such, we designed a &lt;code&gt;get_reps(seqs)&lt;/code&gt; API
that would correctly process single sequences,
multiple sequences of same lengths,
and multiple sequences of varying lengths,
while not needing the user to think about that.&lt;/p&gt;
&lt;p&gt;For evolutionary tuning,
we desired that the API worked regardless of whether
the user tuned on sequences of equal or varying lengths.
In other words,
we wanted to wrap the complexity of handling sequences of multiple lengths
away from the end-user.
This is something we are working on right now,
but at least the API sketch as it stands now
gives the end-user a single &lt;code&gt;evotune(sequences)&lt;/code&gt; function to work with.
Designing it this way allows us to work backwards from the outside-in,
starting with the desired API and flexibly designing the inner parts.
A friend of mine, Ex-Googler Jesse Johnson at Cellarity,
&lt;a href=&quot;https://medium.com/@jejo.math/writing-software-from-the-outside-in-e5359f60fa30&quot;&gt;wrote an excellent blog post on this&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&quot;web-api&quot;&gt;Web API&lt;/h3&gt;&lt;p&gt;With a &lt;code&gt;pip&lt;/code&gt;-installable package on-hand,
we could then depend on it to build other single-use tools.
For example, we built a web API for the model by using &lt;a href=&quot;https://fastapi.tiangolo.com/&quot;&gt;FastAPI&lt;/a&gt;,
and designed it to return the calculated representations for a single sequence
based on the pre-trained weights.
Because evo-tuning is computationally expensive,
we don’t plan to offer that on the web API.
The web API is also packaged up as a &lt;code&gt;pip&lt;/code&gt;-installable package,
and can be installed inside a Docker container,
thus forming the basis of a deep learning-powered micro-service
that pre-calculates &quot;standard&quot;, non-tuned representations
of every new protein we might encounter.&lt;/p&gt;
&lt;h2 id=&quot;lessons-learned&quot;&gt;Lessons Learned&lt;/h2&gt;&lt;h3 id=&quot;on-the-fragility-of-deep-learning-models&quot;&gt;On the fragility of deep learning models&lt;/h3&gt;&lt;p&gt;Our modern way of communicating the architecture of deep learning models
is still a work-in-progress, with overloaded terms and ambiguity present.
As such, there were minor details that we reimplemented incorrectly
that incidentally also had a major impact on the correctness of the reimplementation.
For example, the &lt;code&gt;sigmoid()&lt;/code&gt; function in TensorFlow
was different from the other implementations that we had seen in multiple codebases
(we detail this in our preprint),
yet both are colloquially described as &quot;sigmoids&quot;.&lt;/p&gt;
&lt;p&gt;We also know from adversarial examples
that deep learning models are notoriously fragile
against inputs that have noise added to them.
This has implications in the design of the API to the model.
If we design the user-facing API to accept tensors rather than strings,
then we leave the burden of correctly translating string inputs into tensors
(with semantically correct dimensions) on the end-user.
If the end-user processed the strings in a way that the model is not robust against,
it will handle the processed tensors in a way that may be incorrect for their purposes;
yet, there is no robust way to write a unit test against this.
The key problem here is that the semantic meaning of the tensors
cannot be easily and automatically tested.&lt;/p&gt;
&lt;p&gt;As such, in order for the user-facing API of the model to be &quot;friendly&quot; (so to speak),
it must accept data in a format that the intended end-user will be handling.
This is nothing new in the world of software development, to be user-centric.
At the same time, because of its small size and the way we structure our codebase,
all of the internals are easily accessible,
and how they fit together is also not difficult to understand.&lt;/p&gt;
&lt;h3 id=&quot;why-tools-and-not-full-apps?&quot;&gt;Why tools, and not full apps?&lt;/h3&gt;&lt;p&gt;The environment that we're in is a research environment.
That, by definition, means we're always doing non-standard things.
Moreover, a model is usually but one small component
in a large interconnected web of tools
that come together to make a useful, consumable app.
Rather than try to go end-to-end,
it makes more sense to produce modular parts that fit well with one another.
The mantra I like to keep in mind here is:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Modularity and composability give us the flexibility for creativity.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;And if creativity is not the heart of research, what then is?&lt;/p&gt;
&lt;h3 id=&quot;if-you-want-to-accelerate-a-deep-learning-model...&quot;&gt;If you want to accelerate a deep learning model...&lt;/h3&gt;&lt;p&gt;…&lt;em&gt;how about making sure it first runs well on a single CPU?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;But beyond that,
also consider that whole frameworks
and their associated overheads might not necessarily be what you need.
In our case, we didn't need the entire TF framework to get the model reimplemented.
We only really needed XLA (pre-compiled in &lt;code&gt;jaxlib&lt;/code&gt; for our systems)
and the NumPy API to get it right.
Also consider whether a framework is getting in the way of your modelling work,
or if it's helping.
(If it's the latter, keep using what you're using!)&lt;/p&gt;
&lt;h3 id=&quot;encouraging-a-co-creation-mindset&quot;&gt;Encouraging a co-creation mindset&lt;/h3&gt;&lt;p&gt;Co-creation is &lt;em&gt;the&lt;/em&gt; ethos of the open source world,
where unrelated people come together and build software
that is useful to others beyond themselves.
I've experienced it many times on the &lt;code&gt;pyjanitor&lt;/code&gt; and &lt;code&gt;nxviz&lt;/code&gt; projects.
Technically, nobody has to ask for permission to fork a project,
make changes, and even submit a pull request back.
Of course, we do so anyways,
just for the polity of being courteous to the original creator -
it's always good to let someone know something's coming their way.&lt;/p&gt;
&lt;p&gt;Pair-coding, as it turns out,
happens to be a wonderful way to encourage co-creation.
Much of the reimplementation work
was done pair coding with my co-author, Arkadij Kummer,
who lives and works in our Basel site.
Yes, we're separated by six hours of time,
but that doesn't stop us
from using all of the good internet-enabled tools available to us
to collaborate together.
We used MS Teams at work for video chatting,
and sometimes just did VSCode with VSLiveShare (with audio)
to develop code together.
All work was version-controlled and shared via Git,
which means we could work asynchronously when needed as well.
The time spent pair coding is time where knowledge is shared,
trust is built,
and mentoring relationships are forged.
In an upcoming blog post,
I will write about pair coding as a practice in data science,
particularly for levelling-up junior members of the data science team
and sharing knowledge between peers.&lt;/p&gt;
&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;&lt;p&gt;I hope you enjoyed reading about our journey reimplementing UniRep in JAX.
Keep in mind that JAX isn't necessarily always going to speed up your TF model 100X;
I'm clearly being facetious in writing it that way.
&lt;del&gt;JAX and TF share XLA underneath the hood,&lt;/del&gt;
Matt Johnson pointed out that JAX uses XLA by default,
while TF does not,
yet XLA is really the secret sauce that makes the models execute fast.
&lt;del&gt;The only difference is TF1.x needed this compilation time,
while JAX does it just-in-time (when ordered to do so).&lt;/del&gt;
Because JAX gets us autodiff on the NumPy API,
it's a super productive research and teaching tool
with a familiar API to many Pythonistas.
My hope is this post encourages you to try it out; happy experimenting!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2020/1/18/create-your-own-auto-publishing-slides-with-reveal-md-and-travis-ci/">
    <title type="text">Create your own auto-publishing slides with reveal-md and Travis CI</title>
    <id>urn:uuid:5530170e-4cb1-38c4-a996-92fafb9c7775</id>
    <updated>2020-01-18T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2020/1/18/create-your-own-auto-publishing-slides-with-reveal-md-and-travis-ci/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;For my PyData Ann Arbor Meetup talk, I decided to use &lt;a href=&quot;https://github.com/webpro/reveal-md&quot;&gt;&lt;code&gt;reveal-md&lt;/code&gt;&lt;/a&gt; and a Markdown file to generate my slides. Here, I'd like to write about how I used &lt;code&gt;reveal-md&lt;/code&gt; and Travis CI to continually publish my slides as I updated them, thus making them accessible to everybody on the web.&lt;/p&gt;
&lt;h2 id=&quot;what-is-reveal-md-?&quot;&gt;What is &lt;code&gt;reveal-md&lt;/code&gt;?&lt;/h2&gt;&lt;p&gt;&lt;code&gt;reveal-md&lt;/code&gt; is nothing more than a live web server for that converts Markdown files into &lt;code&gt;reveal.js&lt;/code&gt; slides that can be hosted via a web server, static site, or PDF.&lt;/p&gt;
&lt;h2 id=&quot;why-reveal-md-?&quot;&gt;Why &lt;code&gt;reveal-md&lt;/code&gt;?&lt;/h2&gt;&lt;p&gt;I much prefer to use Markdown to write my slides, as doing so comes with one big benefit: I am focused on the content that I want to deliver, and not the less important details that are easy to screw up (animations, positioning, etc.). Constraining what's accessible to me forces me to be extremely clear and succinct on what I'm trying to communicate. And if I really desired anything fancier, I could weave in some HTML with no issue.&lt;/p&gt;
&lt;h2 id=&quot;create-your-own-auto-publishing-reveal-md-slides&quot;&gt;Create your own auto-publishing &lt;code&gt;reveal-md&lt;/code&gt; slides!&lt;/h2&gt;&lt;p&gt;Let’s walk through the steps needed to make this a reality for you!&lt;/p&gt;
&lt;h3 id=&quot;create-a-new-repository&quot;&gt;Create a new repository&lt;/h3&gt;&lt;p&gt;On GitHub, create a new repository that has a nice and informative name. (For now, we’ll just refer to that repository as &lt;code&gt;my-talk&lt;/code&gt; for convenience.)&lt;/p&gt;
&lt;h3 id=&quot;get-setup-locally&quot;&gt;Get setup locally&lt;/h3&gt;&lt;p&gt;To get setup, you need to make sure that &lt;code&gt;reveal-md&lt;/code&gt; is on your &lt;code&gt;PATH&lt;/code&gt;. I choose to use &lt;code&gt;conda&lt;/code&gt; environments to manage my packages, so I have a slightly convoluted way of doing this, by using &lt;code&gt;conda&lt;/code&gt; to install &lt;code&gt;nodejs&lt;/code&gt; (which installs the &lt;code&gt;npm&lt;/code&gt; node package manager), followed by using the node package manager to install &lt;code&gt;reveal-md&lt;/code&gt;. We first start by preparing an &lt;code&gt;environment.yml&lt;/code&gt; specification file that &lt;code&gt;conda&lt;/code&gt; can use to build your environment:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;my_talk&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;conda-forge&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;dependencies&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;python=3.8&lt;/span&gt;
&lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;nodejs&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, we can execute the installation commands.&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Installation commands&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Create environment&lt;/span&gt;
conda env create -f environment.yml

&lt;span class=&quot;c1&quot;&gt;# Activate environment&lt;/span&gt;
conda activate my_talk

&lt;span class=&quot;c1&quot;&gt;# Install reveal-md&lt;/span&gt;
npm install -g reveal-md
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To learn more about &lt;code&gt;conda&lt;/code&gt; hacks that can improve your efficiency, I &lt;a href=&quot;https://ericmjl.github.io/blog/2018/12/25/conda-hacks-for-data-science-efficiency/&quot;&gt;have a blog post&lt;/a&gt; that you can reference.&lt;/p&gt;
&lt;h3 id=&quot;write-your-slides&quot;&gt;Write your slides&lt;/h3&gt;&lt;p&gt;Before we go onto the automation, it’s important that you get a feel for the workflow so you know what’s being automated.&lt;/p&gt;
&lt;p&gt;Let’s write a simple Markdown file that has two slides:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;---
title: My Fancy Talk!
---

&lt;span class=&quot;gh&quot;&gt;#&lt;/span&gt; My Fancy Talk

Speaker Name

Date

---

&lt;span class=&quot;gu&quot;&gt;##&lt;/span&gt; Hello!
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Save it as &lt;code&gt;slides.md&lt;/code&gt;. The filename isn’t special, it’s just convenient to remember.&lt;/p&gt;
&lt;p&gt;To see more of what &lt;code&gt;reveal.js&lt;/code&gt; can do, check out the &lt;a href=&quot;https://github.com/hakimel/reveal.js&quot;&gt;RevealJS GitHub repository&lt;/a&gt;!&lt;/p&gt;
&lt;h3 id=&quot;preview-your-slides&quot;&gt;Preview your slides&lt;/h3&gt;&lt;p&gt;To serve it up, run the following command at your terminal:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;reveal-md slides.md  &lt;span class=&quot;c1&quot;&gt;# replace with the filename of your slides&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Your browser should now pop open, and your slides will be there! &lt;code&gt;reveal.js&lt;/code&gt; made simple, thanks to &lt;code&gt;reveal-md&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&quot;continue-editing-your-slides&quot;&gt;Continue editing your slides&lt;/h3&gt;&lt;p&gt;Now, you can continue editing your slides, keeping in mind the following pointers.&lt;/p&gt;
&lt;p&gt;Firstly, &lt;code&gt;---&lt;/code&gt; (three of them) denotes horizontal slide transition, while &lt;code&gt;----&lt;/code&gt; (four of them) denotes vertical slide transitions. Use this to organize your content.&lt;/p&gt;
&lt;p&gt;Secondly, to progressively reveal pointers on a slide, you need to add the following HTML comment right after the element. For example, to show bullet points progressively:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;-&lt;/span&gt; Bullet Point 1
&amp;lt;!-- .element class=&amp;quot;fragment&amp;quot; --&amp;gt;
&lt;span class=&quot;k&quot;&gt;-&lt;/span&gt; Bullet Point 2
&amp;lt;!-- .element class=&amp;quot;fragment&amp;quot; --&amp;gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id=&quot;aside:-fancier-items&quot;&gt;Aside: fancier items&lt;/h3&gt;&lt;p&gt;If you need fancier things, you can weave in HTML at your own convenience. For example, I embedded an HTML table to organize &lt;a href=&quot;https://github.com/ColCarroll/imcmc&quot;&gt;&lt;code&gt;imcmc&lt;/code&gt;&lt;/a&gt; logos that I had previously compiled.&lt;/p&gt;
&lt;h3 id=&quot;get-travis-ci-setup&quot;&gt;Get Travis CI setup&lt;/h3&gt;&lt;p&gt;You’ll now want to create a &lt;code&gt;.travis.yml&lt;/code&gt;, which commands Travis to do things. It’s generally nothing more than a collection of bash commands that are executed in order. An example Travis configuration file from my &lt;a href=&quot;https://ericmjl.github.io/testing-for-data-scientists&quot;&gt;data science testing talk&lt;/a&gt; looks like this:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;language&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;python&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;python&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# We don&amp;#39;t actually use the Travis Python, but this keeps it organized.&lt;/span&gt;
    &lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;3.5&amp;quot;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;install&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# We do this conditionally because it saves us some downloading if the&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# version is the same.&lt;/span&gt;
    &lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh;&lt;/span&gt;
    &lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;bash miniconda.sh -b -p $HOME/miniconda&lt;/span&gt;
    &lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;export PATH=&amp;quot;$HOME/miniconda/bin:$PATH&amp;quot;&lt;/span&gt;
    &lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;hash -r&lt;/span&gt;
    &lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;conda config --set always_yes yes --set changeps1 no&lt;/span&gt;
    &lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;conda update -q conda&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Useful for debugging any issues with conda&lt;/span&gt;
    &lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;conda info -a&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Install Python and required packages.&lt;/span&gt;
    &lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;conda env create -f environment.yml&lt;/span&gt;
    &lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;source activate testing-for-data-scientists&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Install reveal-md&lt;/span&gt;
    &lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;npm install -g reveal-md&lt;/span&gt;


&lt;span class=&quot;nt&quot;&gt;script&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Create the docs directory. This is where we will be publishing from&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# (see the &amp;quot;deploy&amp;quot; section below).&lt;/span&gt;
    &lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;mkdir -p docs/&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Use reveal-md to generate static docs.&lt;/span&gt;
    &lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;reveal-md slides.md --static docs --disable-auto-open --theme white&lt;/span&gt;
    &lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;cp -r assets docs/.&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Use reveal-md to generate PDF.&lt;/span&gt;
    &lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;reveal-md slides.md --disable-auto-open --theme white --print slides.pdf&lt;/span&gt;
    &lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;cp slides.pdf docs/.&lt;/span&gt;


&lt;span class=&quot;c1&quot;&gt;# This is an example to deploy to a branch through Travis.&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;deploy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;provider&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;pages&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;skip-cleanup&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;true&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;github-token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;$GITHUB_TOKEN&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# Set in the settings page of your repository, as a secure variable&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;keep-history&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;true&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# We read the master branch&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;on&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;nt&quot;&gt;branch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;master&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Take the docs/ directory&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;local-dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;docs&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Publish to the gh-pages branch&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;target-branch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;gh-pages&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;true&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You’ll notice that the commands we want Travis to execute are basically the same as those we executed manually. The only difference now is that we command &lt;code&gt;reveal-md&lt;/code&gt; to build a static site under the directory &lt;code&gt;site/&lt;/code&gt;, which we then command Travis to push to GitHub pages.&lt;/p&gt;
&lt;p&gt;In the &lt;code&gt;deploy&lt;/code&gt; section, we are specifying to Travis that we want all of the content under the directory &lt;code&gt;site/&lt;/code&gt; to be pushed to the &lt;code&gt;gh-pages&lt;/code&gt; branch of our repository. (We have not yet connected Travis to our repo; that will happen next, so sit tight!)&lt;/p&gt;
&lt;p&gt;Notice also the &lt;code&gt;$GITHUB_TOKEN&lt;/code&gt; environment variable: we need to declare that as well. The &lt;code&gt;GITHUB_TOKEN&lt;/code&gt; is an authentication token that GitHub will recognize when Travis CI pushes the &lt;code&gt;site/&lt;/code&gt; directory to &lt;code&gt;gh-pages&lt;/code&gt;. Because underneath the hood we are using &lt;code&gt;bash&lt;/code&gt; syntax in the YAML file, when we declare the &lt;code&gt;GITHUB_TOKEN&lt;/code&gt;, we do it without the &lt;code&gt;$&lt;/code&gt; symbol, but when we need to grab it from the environment, we include the &lt;code&gt;$&lt;/code&gt; symbol, just as in regular plain old &lt;code&gt;bash&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&quot;get-a-github-deploy-token&quot;&gt;Get a GitHub deploy token&lt;/h3&gt;&lt;p&gt;Under your repository settings, generate a deploy/&quot;personal access&quot; token. Exact docs are &lt;a href=&quot;https://help.github.com/en/github/authenticating-to-github/creating-a-personal-access-token-for-the-command-line&quot;&gt;here&lt;/a&gt;, so in the spirit of &quot;don’t repeat yourself&quot; and &quot;learn to read the docs&quot;, I will encourage you to read them.&lt;/p&gt;
&lt;p&gt;Once you have generated a deploy key, copy them somewhere - you will need it later. (Tip: also make sure you don’t accidentally save it to disk!)&lt;/p&gt;
&lt;h3 id=&quot;connect-travis-ci-to-your-repository&quot;&gt;Connect Travis CI to your repository&lt;/h3&gt;&lt;p&gt;On Travis CI, connect your Travis CI account to GitHub, and then enable Travis to look for changes on the &lt;code&gt;my-talk&lt;/code&gt; repository. Generally, this is done by going to your user settings, and searching for &quot;Legacy Services Integration&quot;, then toggling the checkbox to enable it on your &lt;code&gt;my-talk&lt;/code&gt; repository.&lt;/p&gt;
&lt;p&gt;Once that is done, go into the Travis CI settings for the repository. Navigate to the &quot;Environment Variables&quot; section, and declare the &lt;code&gt;GITHUB_TOKEN&lt;/code&gt; there. Be sure to keep it hidden from the output!&lt;/p&gt;
&lt;h3 id=&quot;turn-on-github-pages&quot;&gt;Turn on GitHub pages&lt;/h3&gt;&lt;p&gt;To turn on GitHub pages, we are going to stick to a pretty sane and widely-used set of practices when interacting with GitHub repositories and static sites.&lt;/p&gt;
&lt;p&gt;Firstly, on the GitHub repository for &lt;code&gt;my-talk&lt;/code&gt;, create a new branch (using the web interface or through the CLI) called &lt;code&gt;gh-pages&lt;/code&gt;. This time round, the name is definitely special, as GitHub recognizes this branch as a legitimate GitHub pages branch to serve content from.&lt;/p&gt;
&lt;p&gt;Secondly, go to the &lt;strong&gt;repository&lt;/strong&gt; settings (not your user settings), and ensure that GitHub pages is enabled for the repository. Usually, adding the &lt;code&gt;gh-pages&lt;/code&gt; branch will result in this option being  automagically turned on.&lt;/p&gt;
&lt;p&gt;Now, run:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;git add slides.md environment.yml .travis.yml&lt;/span&gt;
&lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;git commit -m &amp;quot;first commit&amp;quot;&lt;/span&gt;
&lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;git push&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Your slides, environment config, and Travis CI config files are now pushed to GitHub.&lt;/p&gt;
&lt;h3 id=&quot;check-on-travis&quot;&gt;Check on Travis&lt;/h3&gt;&lt;p&gt;Travis is now going to be building your slides and pushing them to the &lt;code&gt;gh-pages&lt;/code&gt; branch. If all goes well, you will see the slides show up at the URL: &lt;code&gt;https://your_username.github.io/my-talk&lt;/code&gt;. (Naturally, replace &lt;code&gt;your_username&lt;/code&gt; with your GitHub username, and &lt;code&gt;my-talk&lt;/code&gt; with your repository name.&lt;/p&gt;
&lt;h2 id=&quot;debugging-builds&quot;&gt;Debugging builds&lt;/h2&gt;&lt;p&gt;In case the build fails, you can inspect the output. Any errors are basically your standard bash &lt;code&gt;stderr&lt;/code&gt;, so if you know how to debug error messages, you should be able to debug issues with the build.&lt;/p&gt;
&lt;h2 id=&quot;beyond-this&quot;&gt;Beyond this&lt;/h2&gt;&lt;p&gt;Going beyond serving up &lt;code&gt;reveal.js&lt;/code&gt; slides, Travis hooked up to GitHub pages can help you build static sites very easily.&lt;/p&gt;
&lt;p&gt;If you use a static site generator (such as &lt;a href=&quot;https://www.getlektor.com/&quot;&gt;Lektor&lt;/a&gt;, &lt;a href=&quot;https://gohugo.io/&quot;&gt;Hugo&lt;/a&gt;, &lt;a href=&quot;https://blog.getpelican.com/&quot;&gt;Pelican&lt;/a&gt;, &lt;a href=&quot;https://www.gatsbyjs.org/&quot;&gt;Gatsby&lt;/a&gt; or &lt;a href=&quot;https://getnikola.com/&quot;&gt;Nikola&lt;/a&gt;), then you can create websites whose sources are fully under your control, 100% customizable, and fast to load. I do not blog on Medium because I desire full control over the display of my blog content, and I want to be able to take it anywhere I desire, without relying on a platform that might lock my content in. My personal website uses Lektor + Travis to push to GitHub pages; please feel free to look at &lt;a href=&quot;https://github.com/ericmjl/website&quot;&gt;the source&lt;/a&gt; and raid the repo for anything you’d like!&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Addendum: I learned today from a fellow friend Nathan Matias that Netflix’s blog posts on Medium have been paywalled. Another reason for us to take back hosting of slides and blog content into our own hands!&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;blockquote class=&quot;twitter-tweet&quot; data-lang=&quot;en&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;Today I learned the &lt;a href=&quot;https://twitter.com/netflix?ref_src=twsrc%5Etfw&quot;&gt;@netflix&lt;/a&gt; tech blog posts are paywalled by Medium and I may have to take their post off my syllabus.&lt;br&gt;&lt;br&gt;Now I wonder if they paywalled posts which I intend to be freely available. Have any of you found that Medium was unexpectedly charging people for your work?&lt;/p&gt;&amp;mdash; J. Nathan Matias (@natematias) &lt;a href=&quot;https://twitter.com/natematias/status/1218244174299963392?ref_src=twsrc%5Etfw&quot;&gt;January 17, 2020&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2020/1/16/pydata-ann-arbor-meetup-testing-for-data-science/">
    <title type="text">PyData Ann Arbor Meetup: Testing for Data Science</title>
    <id>urn:uuid:8adc6ded-60f5-3b49-b48d-fbb6a86ce30b</id>
    <updated>2020-01-16T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2020/1/16/pydata-ann-arbor-meetup-testing-for-data-science/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;I had the privilege of being invited to deliver a talk at the PyData Ann Arbor meetup this January, held at TD Ameritrade. My hosts, &lt;a href=&quot;https://seanlaw.github.io/&quot;&gt;Sean Law&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/roseputler/&quot;&gt;Rose Putler&lt;/a&gt; and &lt;a href=&quot;https://twitter.com/quasiben&quot;&gt;Ben Zaitlen&lt;/a&gt; were very welcoming and inviting, and I enjoyed my time in Ann Arbor (or A2, as the locals seem to call it).&lt;/p&gt;
&lt;h2 id=&quot;the-talk&quot;&gt;The Talk&lt;/h2&gt;&lt;p&gt;The talk I delivered was on testing for data scientists. (&lt;a href=&quot;https://ericmjl.github.io/testing-for-data-scientists/&quot;&gt;Slides are available here&lt;/a&gt;), and the &lt;a href=&quot;https://www.youtube.com/watch?time_continue=632&amp;amp;v=5RKuHvZERLY&amp;amp;feature=emb_logo&quot;&gt;YouTube video is up too&lt;/a&gt;. The topic stemmed from a long-standing problem that I had seen: untested code that I depended on slowing later analyses down because I did not have the confidence that it would behave correctly outside of the original situations I used it in.&lt;/p&gt;
&lt;p&gt;To communicate this point, I used two examples from work I had done before: one being the use of an automagic testing system, Hypothesis, to ferret out bugs in my code for me, and the other being the use of tests on our data schema to make the creation and caching of views robust and dependable.&lt;/p&gt;
&lt;h2 id=&quot;the-community&quot;&gt;The Community&lt;/h2&gt;&lt;p&gt;The PyData Ann Arbor community has some very dedicated members. There was someone who drove a whole 1.5 hours across the US-Canada border from Windsor, ON to listen in on the talk; another came by from a town 53 minutes away. Sean plays the role that &lt;a href=&quot;https://nedbatchelder.com/&quot;&gt;Ned Batchelder&lt;/a&gt; has done for the Boston Python User Group, and has really fostered a wonderful community here. I was really honored by the dedication that they possessed.&lt;/p&gt;
&lt;p&gt;There were also a bunch of people I had never met in person,  with whom I had interacted with online, with whom I finally got a chance to interact with in-person. It was great to meet some of them, including &lt;a href=&quot;https://bradleydice.com/&quot;&gt;Bradley Dice&lt;/a&gt; (U of M PhD student) and &lt;a href=&quot;https://www.linkedin.com/in/kyleweaton/&quot;&gt;Kyle Eaton&lt;/a&gt; (a UX engineer at Superconductive Health who is closely affiliated with the Great Expectations project).&lt;/p&gt;
&lt;h2 id=&quot;the-food&quot;&gt;The Food&lt;/h2&gt;&lt;p&gt;Ann Arbor, according to Sean, has quite the foodie scene. It is quite true, even as a land-locked midwestern university town.&lt;/p&gt;
&lt;p&gt;On the first afternoon here, I went to this restaurant called &lt;a href=&quot;https://holaseoul.com/&quot;&gt;Hola Seoul&lt;/a&gt;, which served Korean-Mexican fusion food. On my second day here, I had Panera for breakfast (not knowing anything better), skipped lunch, but went out with the PyData organizers to &lt;a href=&quot;http://slurpingturtle.com/annarbor/&quot;&gt;Slurping Turtle&lt;/a&gt; for some  very delicious sushi, fried chicken, and spicy miso ramen. And on my third day here, I decided to enjoy: smoked salmon and avocado omelette at &lt;a href=&quot;http://www.cafezola.com/content/&quot;&gt;Cafe Zola&lt;/a&gt; (it was expensive… but &lt;em&gt;worth  it&lt;/em&gt;), and &lt;a href=&quot;https://savasannarbor.com/&quot;&gt;Sava’s&lt;/a&gt; for a late lunch with Ben (NVIDIA) and Logan (TD Ameritrade).&lt;/p&gt;
&lt;h2 id=&quot;career-chat-with-sean&quot;&gt;Career Chat with Sean&lt;/h2&gt;&lt;p&gt;A few hours before the talk, I had a free-ranging chat with Sean about the problems we tackle in our respective roles. Here’s a smattering of thinking and talking points from our conversation.&lt;/p&gt;
&lt;p&gt;We share an &quot;internal consulting&quot; role at work, where both our teams’ missions are to spearhead new initiatives. His team is explicitly tasked with feeling out what’s upcoming in the next 5 years, POC-ing it, and seeing how it best fits in TD’s systems. It’s a very similar type of role that I’m in.&lt;/p&gt;
&lt;p&gt;I shared with him the frustrations I had battling what I saw was  unnecessary vendor tech. We both seemed to agree that front-liners and decision-makers not operating in the same circles was  an important reason for this phenomena.&lt;/p&gt;
&lt;p&gt;I also learned a few lessons from Sean that I think I need to emphasize more at work:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A relentless focus on generating wins for other teams.&lt;/li&gt;
&lt;li&gt;Strategically cycling between quick wins for credibility, and parlaying that into longer-term (but riskier) wins for the organization.&lt;/li&gt;
&lt;li&gt;Coffee talk tours throughout the company to spread good ideas.&lt;/li&gt;
&lt;li&gt;Minimizing surprises on colleagues (especially nasty ones; pleasant surprises are ok though).&lt;/li&gt;
&lt;li&gt;Strategically holding back on &quot;just doing everything&quot;, and instead letting colleagues co-create, to generate a sense of ownership over the final product.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;the-miscellaneous&quot;&gt;The Miscellaneous&lt;/h2&gt;&lt;p&gt;I was picked up by a limo to and from the hotel. While extremely comfortable, I was definitely pleasantly surprised, to the point of being a little bit not used to it.&lt;/p&gt;
&lt;p&gt;Delta Airlines was half-empty on the way over, but really full on the way back. Either way, my impressions are that without the WiFi access for free, they can’t beat JetBlue for me. That said, PyData Ann Arbor paid for the flight and lodging, so I won’t complain, it was still a comfortable flight nonetheless.&lt;/p&gt;
&lt;p&gt;This was my first time ever being in Michigan!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2020/1/12/honk-on-you-fools-insecure-in-your-miniscule-private-parts/">
    <title type="text">&quot;Honk on, you fools insecure in your miniscule private parts.&quot;</title>
    <id>urn:uuid:6d56bddc-44f4-30f7-a5e8-b167806684db</id>
    <updated>2020-01-12T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2020/1/12/honk-on-you-fools-insecure-in-your-miniscule-private-parts/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;A pattern I have noticed: the drivers who honk the most at cyclists are the ones who have the largest vehicles. Based on my anecdotal counting (highly inaccurate, probably exaggerated, but captures the effect), 1 in 15 pickup truck drivers will honk at me for cycling on the road in Quincy, and will shout out their window that I should ride on the sidewalk, followed by a gas pedal run right after. Supremely insecure ignoramus is the name I would give them.&lt;/p&gt;
&lt;p&gt;Usually, out of pure frustration, I just give them the metaphorical finger back by ignoring them. Based on my memory from my driving exam, I vaguely remembered a few pointers about the rules of the road pertaining to cyclist-motorist interactions. I decided to check them out.&lt;/p&gt;
&lt;p&gt;From the Massachusetts government website, &lt;a href=&quot;https://www.mass.gov/doc/laws-for-bicyclists-and-motorists-in-the-presence-of-bicyclists/download&quot;&gt;the rules of the road document Chapter 4&lt;/a&gt; (page 108), a selection of rules pertaining to the cyclist-driver interactions and cyclist road usage.&lt;/p&gt;
&lt;p&gt;As a cyclist:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You can use the &lt;strong&gt;full lane&lt;/strong&gt; (emphasis in original) anywhere, anytime, and on any street (except limited access or express state highways where signs specifically prohibiting bicycles have been posted), &lt;em&gt;even if there is a bike lane&lt;/em&gt; (emphasis mine).&lt;/li&gt;
&lt;li&gt;You can keep to the right when passing a motor vehicle moving in the travel lane and you can move to the front of an intersection at stop lights.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As a motorist:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Do Not Squeeze Bicycles in a Narrow Lane:&lt;/strong&gt; If a lane is too narrow to pass a bicycle at a safe distance, be &lt;strong&gt;PATIENT&lt;/strong&gt; (emphasis original) until you can safely use an adjacent lane or &lt;strong&gt;WAIT&lt;/strong&gt; (emphasis original) until it is safe to pass in the lane you share. (Chap. 89, Sec. 2) &lt;em&gt;You should stay at least three feet away when passing.&lt;/em&gt; (emphasis mine)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Be aware that bicyclists Do Not Always Have to Signal Turns!&lt;/strong&gt; Bicyclists must signal their intent by either hand to stop or turn. However, the signal does not have to be continuous or be made at all if both hands are needed for the bicycle’s safe operation. (Chap. 85, Sec. 11B).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Turns out, I was not wrong about riding on the road!&lt;/p&gt;
&lt;p&gt;So there, according to Massachusetts’ government-endorsed rules of the road, as a cyclist, I am entitled to use the entire lane whenever I see fit, &lt;em&gt;even in the presence of a bike lane&lt;/em&gt;, and am under zero obligation to use the sidewalk simply for a motorist’s convenience. I choose to ride on the right side just to make things easier for drivers (and as an occasional driver myself, I much appreciate it when cyclists do so; the Golden Rule is pretty good here). But where the rules are quite clear, I definitely do not appreciate being honked and shouted at for riding on a road that I am rightfully allowed to use.&lt;/p&gt;
&lt;h2 id=&quot;addendum&quot;&gt;Addendum&lt;/h2&gt;&lt;p&gt;To be clear, I’m already thankful it’s only about 1 in 15 pickup-truck drivers and about 1 in 50 regular drivers who honk at me.&lt;/p&gt;
&lt;p&gt;At the same time, I’m looking for ideas to make the ignoramuses aware of the rules of the road. Would be happy to chat on &lt;a href=&quot;https://octodon.social/web/accounts/7711&quot;&gt;Mastodon&lt;/a&gt; or &lt;a href=&quot;../../../../../blog/2020/1/12/honk-on-you-fools-insecure-in-your-miniscule-private-parts/twitter.com/ericmjl&quot;&gt;Twitter&lt;/a&gt; if you have ideas.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2020/1/7/fastapi-flask-like-generator-of-web-apis/">
    <title type="text">FastAPI: Flask-like generator of web APIs</title>
    <id>urn:uuid:b3367081-dc60-3b82-8a63-eb54cfe84e6d</id>
    <updated>2020-01-07T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2020/1/7/fastapi-flask-like-generator-of-web-apis/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;I test-drove &lt;a href=&quot;https://fastapi.tiangolo.com/&quot;&gt;FastAPI&lt;/a&gt; yesterday, and true to it's name, it's fast! And when the developers say fast, they mean it in at least two ways:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;It's fast to execute.&lt;/li&gt;
&lt;li&gt;It's fast to develop an API.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I'd like to write about the second point.&lt;/p&gt;
&lt;p&gt;Prior to FastAPI, my development framework of choice to build a web API would have been Flask (though I will admit, I have only developed web apps, not web APIs before). My familiarity with Flask turned out to be extremely helpful for learning FastAPI: its own API tracks very closely with Flask! Many of the same idioms, including instantiating a &lt;code&gt;FastAPI&lt;/code&gt; object and using &lt;code&gt;@app.route&lt;/code&gt;  decorators with string interpolation in the URIs, carry over. This is the first way that FastAPI makes API development fast: &lt;strong&gt;by using existing idioms&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Prior to FastAPI, I did have reservations about developing a web API using Flask because I wasn't sure how to provide sufficient documentation to API users/testers on how to use it. (Have you seen Swagger APIs &lt;em&gt;without&lt;/em&gt; example usage documentation? Frustrating!) Turns out, FastAPI &lt;strong&gt;parses your docstrings to provide API docs&lt;/strong&gt;. This means you can provide an example usage inside the routing function, and on the &lt;code&gt;/docs&lt;/code&gt; endpoint, all they will render on the API docs!&lt;/p&gt;
&lt;p&gt;So in summary, FastAPI is a very productive tool to use for developing APIs. I highly recommend you check it out once you hit the appropriate opportunity!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2020/1/4/build-your-digital-profile-as-a-data-scientist/">
    <title type="text">Build your digital profile as a data scientist</title>
    <id>urn:uuid:f294b030-879e-3e9c-a070-eeaa5a3ec421</id>
    <updated>2020-01-04T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2020/1/4/build-your-digital-profile-as-a-data-scientist/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;I have received questions from others on how to build a digital profile for career development. Everybody’s going to have a unique path, and what I think I can offer are observations on what I think have been helpful for myself and others who have enjoyed similar successes in getting started. My hope is that these pointers, which come from self-reflection over the past two years of work, help you.&lt;/p&gt;
&lt;h2 id=&quot;the-maybe-timeless-and-cross-industry-principles&quot;&gt;The maybe-timeless and cross-industry principles&lt;/h2&gt;&lt;p&gt;First off, here are some things that I think are timeless principles in the job search, and are probably true for a large swathe of professional roles.&lt;/p&gt;
&lt;p&gt;Firstly, nobody owes me their time or a job; &lt;strong&gt;I have a lot of leeway to make it as easy as possible for others to say, &quot;Yes, I would like to have you on board&quot;&lt;/strong&gt;. Naturally, this statement ignores structural privileges and inequalities that exist in society, so I would be realistic in what I can accomplish. That said, it's usually a good prior to assume that everybody is busy, and that you need to properly advertise what you can offer.&lt;/p&gt;
&lt;p&gt;In line with the same idea, I start with the presumption that nobody has the spare time to think deeply about a candidate when there are many other things in one's mind. The corollary is that &lt;strong&gt;it is on me to frame how I want others to view me&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&quot;data-science-specific-things&quot;&gt;Data science-specific things&lt;/h2&gt;&lt;p&gt;Now, for a data science role, being able to demonstrate the following can only help, not hurt, your application and candidacy:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Ability to use your skills to solve real problems of value.&lt;/li&gt;
&lt;li&gt;Good coding practices.&lt;/li&gt;
&lt;li&gt;Good storytelling and communication ability.&lt;/li&gt;
&lt;li&gt;Proficiency with building &quot;products&quot; that aid in decision-making.&lt;/li&gt;
&lt;li&gt;Ability to work collaboratively with others.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In data science, &quot;value&quot; usually means saving money. Saving time = saving money, in case that was not clear. Hence, automation &lt;em&gt;is&lt;/em&gt; valuable.&lt;/p&gt;
&lt;p&gt;Good coding practices are important: I have an &lt;a href=&quot;https://ericmjl.github.io/essays-on-data-science/software-skills/&quot;&gt;essay collection&lt;/a&gt; on them, if you need a resource to get started.&lt;/p&gt;
&lt;p&gt;Communication skills are universally important in professional roles, and data science is no different.&lt;/p&gt;
&lt;p&gt;What does enhance communication is the ability to build interactive tools that guide a busy decision-maker towards ethical and profitable choices (hopefully in that order). Good value judgment is needed here!&lt;/p&gt;
&lt;h2 id=&quot;where-are-prospective-hiring-teams-going-to-look?&quot;&gt;Where are prospective hiring teams going to look?&lt;/h2&gt;&lt;p&gt;This comes from &lt;code&gt;n=7&lt;/code&gt; candidates for (I think) 3 candidate searches at work that I was involved in. The order in which I was looking was:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Resume&lt;/li&gt;
&lt;li&gt;LinkedIn&lt;/li&gt;
&lt;li&gt;GitHub&lt;/li&gt;
&lt;li&gt;Personal website&lt;/li&gt;
&lt;li&gt;Google Scholar&lt;/li&gt;
&lt;li&gt;Old/current research group website&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The resume is what you turn in. Make sure it's clean, readable, and that it concisely captures exactly what you're looking to communicate to the hiring team: how you're going to be a good fit for the role.&lt;/p&gt;
&lt;p&gt;When I looked at LinkedIn, I was, quite interestingly, looking at their social network. Who might they plausibly know? At least for the team I'm on, I know credentials and certifications matter less than evidence of projects done, which brings me to the next place...&lt;/p&gt;
&lt;p&gt;GitHub. I was looking for evidence of candidates' ability to code. A well fleshed-out GitHub profile with publicly browsable repositories and a contribution record that is mostly your own makes it so much easier to see your coding style. I also looked for evidence of familiarity with packages, continuous integration tooling, good version control, and collaborations with other package developers.&lt;/p&gt;
&lt;p&gt;Your projects that demonstrate the data science skills above should be prominently featured on your profile page. Project types that, in contemporary times, communicate these skills well include:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Data products that you've built&lt;/li&gt;
&lt;li&gt;Teaching material that you've made&lt;/li&gt;
&lt;li&gt;Contributions you've made to other repositories, in particular pull requests and issues politely raised.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I looked at Google Scholar as well to get a flavour for a candidate's prior research work. It's an indication of one's domain expertise, and &lt;em&gt;possibly&lt;/em&gt; an indicator of the kinds of problems one will gravitate towards. (This last point has been at least true for myself; however, for one jumping from, say, biological data science to flight data science, this will be much less relevant.)&lt;/p&gt;
&lt;p&gt;The diversity of one's collaborators also helps paint a picture: did you specialize in work with one other person all of your academic career, or did you work in large teams, or did you work mostly solo? (Don't put a value judgment so quickly: each has their own strengths.)&lt;/p&gt;
&lt;p&gt;A candidate's old research group is something I would check only out of curiosity, just to know more.&lt;/p&gt;
&lt;h2 id=&quot;the-kitchen-sink-of-tips&quot;&gt;The kitchen sink of tips&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Tip #1:&lt;/strong&gt; if you put your source code on GitHub, always include in the README why the repository exists, and a guide to how to use the repository. It is a marker of &quot;sociable working style&quot;: in other words, you're able to think of how others are going to interact with things that you've created. (Using others' tools happens all the time at work!)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tip #2:&lt;/strong&gt; If you put a notebook up in your repository, be sure to make the repo &lt;a href=&quot;https://mybinder.org/&quot;&gt;Binder-friendly&lt;/a&gt;. It doesn't take much: environment spec file is all thats needed!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2020/1/2/on-automating-principled-statistical-analyses/">
    <title type="text">On automating principled statistical analyses</title>
    <id>urn:uuid:ac86f8f4-829c-3409-8608-a393d4c21d76</id>
    <updated>2020-01-02T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2020/1/2/on-automating-principled-statistical-analyses/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;I’ve been known to rant against the t-test, because I see it as a canned statistical test that most scientists &quot;just&quot; reach for. From a statistical viewpoint, reaching for the t-test by default is unprincipled because our data may not necessarily fulfill the Gaussian-distributed assumptions of the t-test.&lt;/p&gt;
&lt;p&gt;That isn’t to say, though, that I’m against automated statistical analyses.&lt;/p&gt;
&lt;p&gt;If there’s a data generating process that will need continual analysis, and we are aware that these processes can be broadly standardized enough that we can use a single statistical model across multiple groups and/or samples, then we might be able to automate the analysis method used.&lt;/p&gt;
&lt;p&gt;An example from my line of work is standardized high throughput (and/or large-scale) measurements with the same randomized experimental structure. If the high throughput measurement assay stays the same from project to project, and is a standardized assay measurement, then we should be able to use a single statistical model across all samples in the assay.&lt;/p&gt;
&lt;p&gt;I have done this with large-scale electrophysiology measurements, where we quantified electrophysiological curve decay constants as a function of molecule concentrations, and wrote a custom hierarchical Bayesian model for the data. In another project, my colleagues and I built a hierarchical Bayesian model for enzyme catalysis efficiency. In both cases, because we had confidence that the data generating process was constant over time, we could write a program through which we fed in standardized data and from which we obtained robust, regularized estimates of our quantities of interest.&lt;/p&gt;
&lt;p&gt;Counterfactually, if we had &lt;em&gt;just&lt;/em&gt; picked some quantity and gone with the t-test (or worse, used t-test assumptions with multiple hypothesis correction), we would have likely made a number of errors in our automated analyses that would compound in our later decision-making steps. More pedestrian would have been the fact that I would not have been able to properly defend what we were doing in front of a properly-trained statistician who knows how to use likelihoods in appropriate situations. (Our data didn’t necessarily have t-distributed likelihoods!)&lt;/p&gt;
&lt;p&gt;There’s always this &quot;smell test&quot; that we can do. &lt;strong&gt;The &quot;likelihood smell test&quot; is a good one.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In conclusion, automating a principled statistical analysis is fine, as long as the data generating process is more or less constant. Reaching for a canned test by default is not.&lt;/p&gt;
&lt;p&gt;And friends, if you write an automated pipeline, don’t forget to &lt;a href=&quot;https://ericmjl.github.io/essays-on-data-science/software-skills/testing/&quot;&gt;write tests&lt;/a&gt;!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/12/26/serving-multiple-panel-apps-together/">
    <title type="text">Serving multiple Panel apps together</title>
    <id>urn:uuid:a4a07dc4-3a93-330c-8cb1-79e1cbec98d2</id>
    <updated>2019-12-26T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/12/26/serving-multiple-panel-apps-together/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;I learned a new thing today! If I have a bunch of small dashboard-like utilities, &lt;code&gt;panel&lt;/code&gt;, which uses the &lt;code&gt;bokeh&lt;/code&gt; server behind the scenes, can serve up multiple files together from the same server.&lt;/p&gt;
&lt;p&gt;Here's an example. Assume I have the following directory structure:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;|- /
  |- app1.py
  |- app2.py
  |- app3.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If I start a Panel server here using:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;panel serve *.py
&lt;span class=&quot;c1&quot;&gt;# or, to be selective&lt;/span&gt;
panel serve src1.py src2.py src3.py
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If you have a bunch of Jupyter notebooks, the analogous command is:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;panel serve *.ipynb
&lt;span class=&quot;c1&quot;&gt;# or, to be selective:&lt;/span&gt;
panel serve nb1.ipynb nb2.ipynb nb3.ipynb
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then all of the apps will be served up, with a default Bokeh landing page provided to link to each of the apps.&lt;/p&gt;
&lt;p&gt;Doing so lets us build multiple little utilities that can help ourselves and our colleagues be more productive!&lt;/p&gt;
&lt;p&gt;For an example of this, check out the &lt;a href=&quot;https://minimal-panel-app.herokuapp.com/home&quot;&gt;minimal panel app&lt;/a&gt; I built to record these ideas. (Source code is available &lt;a href=&quot;https://github.com/ericmjl/minimal-panel-app&quot;&gt;here&lt;/a&gt;.)&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/12/19/simplifying-uncertainty-responsibly/">
    <title type="text">Simplifying Uncertainty Responsibly</title>
    <id>urn:uuid:e86b3753-7b6d-3738-8eda-d66aeea78b40</id>
    <updated>2019-12-19T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/12/19/simplifying-uncertainty-responsibly/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;I read an article from Brandon Rohrer titled &quot;&lt;a href=&quot;https://brohrer.github.io/oversimplify.html&quot;&gt;Oversimplify&lt;/a&gt;&quot;. The article provoked some thoughts.&lt;/p&gt;
&lt;p&gt;As someone who strives to model uncertainty in a day-to-day setting, it’s easy to misread Brandon’s article as saying, &quot;discard your modelling of uncertainty&quot;. It took me a few reads and a bit of thinking to realize that my misunderstanding would have been wrong. The gist of Brandon’s article is to communicate the fact that &lt;em&gt;most people don’t like uncertainty, so simplify &lt;strong&gt;the communication&lt;/strong&gt; of uncertainty&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;In a Bayesian setting, I think this corresponds mostly to &quot;minimize regret&quot; when making decisions.&lt;/p&gt;
&lt;p&gt;Here’s a classic example: weather forecast says &quot;30% probability of precipitation&quot; (assume this is rain). How does this fit into Brandon’s conception of simplifying the communication of uncertainty?&lt;/p&gt;
&lt;p&gt;Simply telling someone the probability of precipitation doesn’t help. It’s like stating an unhelpful fact - unhelpful if the audience doesn’t have a thought framework for acting on that uncertainty.&lt;/p&gt;
&lt;p&gt;Any good card-carrying Bayesian who also knows how to minimize regret in-line with pretty universal human values would say, &quot;there’s a 30% probability of precipitation. Since getting wet is more miserable than carrying an umbrella on a cloudy day, take that umbrella, and throw in your waterproof jacket and boots while you’re at it.&quot;&lt;/p&gt;
&lt;p&gt;By contrast, someone other consultant might take the same probabilities, and instead recommend, &quot;Oh, there’s basically greater odds of not raining than raining, so wear your cotton jacket since it’s cold, but don’t bother about an umbrella.&quot; This consultant, we would say, is basically deaf to human psychology and ethics. (I will not go deeper on the question of &quot;what makes ethics&quot; - too much for this blog post.)&lt;/p&gt;
&lt;p&gt;The key question most people are seeking an answer to is not &quot;what do the data say&quot;, but rather &quot;how &lt;em&gt;ought&lt;/em&gt; I act?&quot; Though the latter question is normative, it &lt;em&gt;can&lt;/em&gt; be informed by quantitative reasoning; we just have to make sure it is not &lt;em&gt;only&lt;/em&gt; informed by quantitative reasoning.&lt;/p&gt;
&lt;p&gt;So to summarize: &lt;em&gt;most people don’t like uncertainty, so simplify &lt;strong&gt;the communication&lt;/strong&gt; of uncertainty by providing an actionable recommendation in-line with universal of values (as much as possible) and culturally-specific behaviours of your audience&lt;/em&gt;. In that way, our handling of uncertainty can be responsible and ethical.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/12/15/a-review-of-the-python-data-science-dashboarding-landscape-in-2019/">
    <title type="text">A Review of the Python Data Science Dashboarding Landscape in 2019</title>
    <id>urn:uuid:de09337d-5023-3bca-960c-70e369a20733</id>
    <updated>2019-12-15T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/12/15/a-review-of-the-python-data-science-dashboarding-landscape-in-2019/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;&lt;em&gt;This blog post is also available on &lt;a href=&quot;https://ericmjl.github.io/essays-on-data-science/&quot;&gt;my collection of essays&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;&lt;p&gt;As Pythonista data scientists,
we are spoiled for choice when it comes to developing front-ends
for our data apps.
We used to have to fiddle with HTML in Flask (or Plotly's Dash),
but now, there are tools in which
&quot;someone wrote the HTML/JS so I didn't have to&quot;.&lt;/p&gt;
&lt;p&gt;Let me give a quick tour of the landscape of tools
as I've experienced it in 2019.&lt;/p&gt;
&lt;h3 id=&quot;beginnings:-voila&quot;&gt;Beginnings: Voila&lt;/h3&gt;&lt;p&gt;Previously, I had test-driven
&lt;a href=&quot;https://voila.readthedocs.io/en/latest/&quot;&gt;Voila&lt;/a&gt;.
The key advantage I saw back then was that in my workflow,
once I had the makings of a UI present in the Jupyter notebook,
and just needed a way to serve it up
independent of having my end-users run a Jupyter server,
then Voila helped solve that use case.
By taking advantage of existing the &lt;code&gt;ipywidgets&lt;/code&gt; ecosystem
and adding on a way to run and serve the HTML output of a notebook,
Voila solved that part of the dashboarding story quite nicely.
In many respects,
I regard Voila as the first proper dashboarding tool for Pythonistas.&lt;/p&gt;
&lt;p&gt;That said, development in a Jupyter notebook
didn't necessarily foster best practices
(such as refactoring and testing code).
When my first project at work ended,
and I didn't have a need for further dashboarding,
I didn't touch Voila for a long time.&lt;/p&gt;
&lt;h3 id=&quot;another-player:-panel&quot;&gt;Another player: Panel&lt;/h3&gt;&lt;p&gt;Later, &lt;a href=&quot;http://panel.pyviz.org/&quot;&gt;Panel&lt;/a&gt; showed up.
Panel's development model allowed a more modular app setup,
including importing of plotting functions defined inside &lt;code&gt;.py&lt;/code&gt; files
that returned individual plots.
Panel also allowed me to prototype in a notebook and see the output live
before moving the dashboard code into a source &lt;code&gt;.py&lt;/code&gt; file.&lt;/p&gt;
&lt;p&gt;At work, we based a one-stop shop dashboard for a project on Panel,
and in my personal life,
I also built a
&lt;a href=&quot;https://github.com/ericmjl/minimal-panel-app&quot;&gt;minimal panel app&lt;/a&gt;
that I also
&lt;a href=&quot;https://minimal-panel-app.herokuapp.com/&quot;&gt;deployed to Heroku&lt;/a&gt;.
Panel was definitely developed
targeting notebook and source file use cases in mind,
and this shows through in its source development model.&lt;/p&gt;
&lt;p&gt;That said, panel apps could be slow to load,
and without having a &quot;spinner&quot; solution in place
(i.e. something to show the user
that the app is &quot;doing something&quot; in the background),
it sometimes made apps &lt;em&gt;feel&lt;/em&gt; slow
even though the slowness was not Panel's fault really.
(My colleagues and I pulled out all the tricks in our bag to speed things up.)&lt;/p&gt;
&lt;p&gt;Additionally, any errors that show up don't get surfaced to the app's UI,
where developer eyeballs are on -
instead, they get buried in the browser's JavaScript console
or in the Python terminal where the app is being served.
When deployed, this makes it difficult to see where errors show up
and debug errors.&lt;/p&gt;
&lt;h3 id=&quot;enter-streamlit&quot;&gt;Enter Streamlit&lt;/h3&gt;&lt;p&gt;Now, Streamlit comes along, and some of its initial demos are pretty rad.
In order to test-drive it,
I put together this &lt;a href=&quot;https://minimal-streamlit.herokuapp.com/&quot;&gt;little tutorial&lt;/a&gt;
on the Beta probability distribution for my colleagues.&lt;/p&gt;
&lt;p&gt;Streamlit definitely solves some of the pain points
that I've observed with Panel and Voila.&lt;/p&gt;
&lt;p&gt;The most important one that I see is that errors are captured by Streamlit
and bubbled up to the UI,
where our eyeballs are going to be when developing the app.
For me, this is a very sensible decision to make, for two reasons:&lt;/p&gt;
&lt;p&gt;Firstly, it makes debugging interactions that much easier.
Instead of needing to have two interfaces open,
the error message shows up right where the interaction fails,
in the same browser window as the UI elements.&lt;/p&gt;
&lt;p&gt;Secondly, it makes it possible for us
to use the error messages as a UI &quot;hack&quot; to inform users
where their inputs (e.g. free text) might be invalid,
thereby giving them &lt;em&gt;informative error messages&lt;/em&gt;.
(Try it out in the Beta distribution app:
it'll give you an error message right below
if you try to type something that cant be converted into a float!)&lt;/p&gt;
&lt;p&gt;The other key thing that Streamlit provides as a UI nice-ity
is the ability to signal to end-users that a computation is happening.
Streamlit does this in three ways, two of which always come for free.
&lt;strong&gt;Firstly&lt;/strong&gt;, if something is &quot;running&quot;,
then in the top-right hand corner of the page,
the &quot;Running&quot; spinner will animate.
&lt;strong&gt;Secondly&lt;/strong&gt;, anything that is re-rendering will automatically be greyed out.
&lt;strong&gt;Finally&lt;/strong&gt;, we can use a special context manager
to provide a custom message on the front-end:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;streamlit&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;st&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;st&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;spinner&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Message goes here...&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# stuff happens&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;So all-in-all, Streamlit seems to have a solution of some kind
for the friction points that I have observed with Panel and Voila.&lt;/p&gt;
&lt;p&gt;Besides that, Streamlit, I think, uses a procedural paradigm,
rather than a callback paradigm, for app construction.
We just have to think of the app as a linear sequence of actions
that happen from top to bottom.
State is never really an issue, because every code change
and interaction re-runs the source file from top to bottom, from scratch.
When building quick apps,
this paradigm really simplifies things compared to a callback-based paradigm.&lt;/p&gt;
&lt;p&gt;Finally, Streamlit also provides a convenient way to add text to the UI
by automatically parsing as Markdown any raw strings unassigned to a variable
in a &lt;code&gt;.py&lt;/code&gt; file and rendering them as HTML.
This opens the door to treating a &lt;code&gt;.py&lt;/code&gt; file as a
&lt;a href=&quot;https://en.wikipedia.org/wiki/Literate_programming&quot;&gt;literate programming document&lt;/a&gt;,
hosted by a Python-based server in the backend.
It'd be useful especially in teaching scenarios.
(With &lt;code&gt;pyiodide&lt;/code&gt; bringing the PyData stack to the browser,
I can't wait to see standalone &lt;code&gt;.py&lt;/code&gt; files rendered to the DOM!)&lt;/p&gt;
&lt;p&gt;Now, this isn't to say that Streamlit is problem-free.
There are still rough edges,
the most glaring (as of today) in the current release
is the inability to upload a file and operate on it.
This has been fixed in &lt;a href=&quot;https://github.com/streamlit/streamlit/pull/488&quot;&gt;a recent pull request&lt;/a&gt;,
so I'm expecting this should show up in a new release any time soon.&lt;/p&gt;
&lt;p&gt;The other not-so-big-problem that I see with Streamlit at the moment
is the procedural paradigm -
by always re-running code from top-to-bottom afresh on every single change,
apps that rely on long compute may need a bit more thought to construct,
including the use of Streamlit's caching mechanism.
Being procedural does make things easier for development though,
and on balance, I would not discount Streamlit's simplicity here.&lt;/p&gt;
&lt;h2 id=&quot;where-does-streamlit-fit?&quot;&gt;Where does Streamlit fit?&lt;/h2&gt;&lt;p&gt;As I see it, Streamlit's devs are laser-focused on enabling devs
to &lt;em&gt;very quickly&lt;/em&gt; get to a somewhat good-looking app prototype.
In my experience, the development time for the Beta distribution app
took about 3 hours, 2.5 of which were spent on composing prose.
So effectively, I only used half an hour doing code writing,
with a live and auto-reloading preview
greatly simplifying the development process.
(I conservatively estimate that this is about 1.5 times
as fast as I would be using Panel.)&lt;/p&gt;
&lt;p&gt;Given Streamlit, I would use it to develop two classes of apps:
(1) very tightly-focused utility apps that do one lightweight thing well, and
(2) bespoke, single-document literate programming education material.&lt;/p&gt;
&lt;p&gt;I would be quite hesitant to build more complex things;
then again, for me, that statement would be true more generally anyways
with whatever tool.
In any case, I think bringing UNIX-like thinking to the web
is probably a good idea:
we make little utilities/functional tools
that can pipe standard data formats from to another.&lt;/p&gt;
&lt;h2 id=&quot;common-pain-points-across-all-three-dashboarding-tools&quot;&gt;Common pain points across all three dashboarding tools&lt;/h2&gt;&lt;p&gt;A design pattern I have desired is to be able to serve up a fleet of small,
individual utilities served up from the same codebase,
served up by individual server processes,
but all packaged within the same container.
The only way I can think of at the moment
is to build a custom Flask-based gateway
to redirect properly to each utility's process.
That said, I think this is probably out of scope
for the individual dashboarding projects.&lt;/p&gt;
&lt;h2 id=&quot;how-do-we-go-forward?&quot;&gt;How do we go forward?&lt;/h2&gt;&lt;p&gt;The ecosystem is ever-evolving, and,
rather than being left confused by the multitude of options available to us,
I find myself actually being very encouraged
at the development that has been happening.
There's competing ideas with friendly competition between the developers,
but they are also simultaneously listening to each other and their users
and converging on similar things in the end.&lt;/p&gt;
&lt;p&gt;That said, I think it would be premature to go &quot;all-in&quot; on a single solution
at this moment.
For the individual data scientist,
I would advise to be able to build something
using each of the dashboarding frameworks.
My personal recommendations are to know how to use:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Voila + &lt;code&gt;ipywidgets&lt;/code&gt; in a Jupyter notebook&lt;/li&gt;
&lt;li&gt;Panel in Jupyter notebooks and standalone &lt;code&gt;.py&lt;/code&gt; files&lt;/li&gt;
&lt;li&gt;Streamlit in &lt;code&gt;.py&lt;/code&gt; files.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These recommendations stem mainly from
the ability to style and layout content without needing much knowledge of HTML.
In terms of roughly when to use what,
my prior experience has been that
Voila and Streamlit are pretty good for quicker prototypes,
while Panel has been good for more complex ones,
though in all cases, we have to worry about speed impacting user experience.&lt;/p&gt;
&lt;p&gt;From my experience at work,
being able to quickly hash out key visual elements in a front-end prototype
gives us the ability to better communicate with UI/UX designers and developers
on what we're trying to accomplish.
Knowing how to build front-ends ourselves
lowers the communication and engineering barrier
when taking a project to production.
It's a worthwhile skill to have;
be sure to have it in your toolbox!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/11/9/principled-git-based-workflow-in-collaborative-data-science-projects/">
    <title type="text">Principled Git-based Workflow in Collaborative Data Science Projects</title>
    <id>urn:uuid:54f99df0-bb6a-32f8-ab64-13d6a77605ee</id>
    <updated>2019-11-09T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/11/9/principled-git-based-workflow-in-collaborative-data-science-projects/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;Having worked with GitFlow on a data science project and coming to a few epiphanies with it, I decided to share some of my thoughts &lt;a href=&quot;https://ericmjl.github.io/essays-on-data-science/workflow/gitflow/&quot;&gt;in an essay&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;One of my thoughts here is that most data scientists aren't resistant to using GitFlow (and more generally, just being more intentional about what gets worked on) because it's a bad idea, but because there's a lack of incentives to do so. In there, I try to address this concern.&lt;/p&gt;
&lt;p&gt;And because GitFlow does require knowledge of Git, it can trigger an, &quot;Oh no, one more thing to learn!&quot; response. These things do take time to learn, yes, but I see it also as an investment of time with a future payoff.&lt;/p&gt;
&lt;p&gt;Apart from that, I hope you enjoy the essay; writing it was also a great opportunity for me to pick up more advanced features of &lt;code&gt;pymdownx&lt;/code&gt;, a package that extends Markdown syntax with other really cool features.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/10/31/reimplementing-and-testing-deep-learning-models/">
    <title type="text">Reimplementing and Testing Deep Learning Models</title>
    <id>urn:uuid:eda83e45-1c19-3613-9a34-21831ed14a99</id>
    <updated>2019-10-31T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/10/31/reimplementing-and-testing-deep-learning-models/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;&lt;em&gt;Note: this blog post is cross-posted on my personal &lt;a href=&quot;https://ericmjl.github.io/essays-on-data-science/machine-learning/reimplementing-models/&quot;&gt;essay collection on the practice of data science&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;At work, most deep learners I have encountered
have a tendency to take deep learning models
and treat them as black boxes that we should be able to wrangle.
While I see this as a pragmatic first step
to testing and proving out the value of a newly-developed deep learning model,
I think that stopping there
and not investing the time into understanding the nitty-gritty of the model
leaves us in a poor position
to know that model's
(1) applicability domain (i.e. where the model should be used),
(2) computational and statistical performance limitations, and
(3) possible engineering barriers to getting the model performant
in a &quot;production&quot; setting.&lt;/p&gt;
&lt;p&gt;As such, with deep learning models,
I'm actually a fan of investing the time to re-implement the model
in a tensor framework that we all know and love,
NumPy (and by extension, JAX).&lt;/p&gt;
&lt;h2 id=&quot;benefits-of-re-implementing-deep-learning-models&quot;&gt;Benefits of re-implementing deep learning models&lt;/h2&gt;&lt;p&gt;Doing a model re-implementation from a deep learning framework
into NumPy code actually has some benefits for the time being invested.&lt;/p&gt;
&lt;h3 id=&quot;developing-familiarity-with-deep-learning-frameworks&quot;&gt;Developing familiarity with deep learning frameworks&lt;/h3&gt;&lt;p&gt;Firstly, doing so forces us to know the translation/mapping
from deep learning tensor libraries into NumPy.
One of the issues I have had with deep learning libraries
(PyTorch and Tensorflow being the main culprits here)
is that their API copies something like 90% of NumPy API
without making easily accessible
the design considerations discussed when deciding to deviate.
(By contrast, CuPy has an explicit API policy
that is well-documented and front-and-center on the docs,
while JAX strives to replicate the NumPy API.)&lt;/p&gt;
&lt;p&gt;My gripes with tensor library APIs aside, though,
translating a model by hand from one API to another
forces growth in familiarity with both APIs,
much as translating between two languages
forces growth in familiarity with both languages.&lt;/p&gt;
&lt;h3 id=&quot;developing-a-mechanistic-understanding-of-the-model&quot;&gt;Developing a mechanistic understanding of the model&lt;/h3&gt;&lt;p&gt;It is one thing to describe a deep neural network
as being &quot;like the brain cell connections&quot;.
It is another thing to know that the math operations underneath the hood
are nothing more than dot products (or tensor operations, more generally).
Re-implementing a deep learning model
requires combing over every line of code,
which forces us to identify each math operation used.
No longer can we hide behind an unhelpfully vague abstraction.&lt;/p&gt;
&lt;h3 id=&quot;developing-an-ability-to-test-and-sanity-check-the-model&quot;&gt;Developing an ability to test and sanity-check the model&lt;/h3&gt;&lt;p&gt;If we follow the workflow (that I will describe below)
for reimplementing the model,
(or as the reader should now see, translating the model between APIs)
we will develop confidence in the correctness of the model.
This is because the workflow I am going to propose
involves proper basic software engineering workflow:
writing documentation for the model,
testing it,
and modularizing it into its logical components.
Doing each of these requires a mechanistic understanding
of how the model works,
and hence forms a useful way of building intuition behind the model
as well as correctness of the model.&lt;/p&gt;
&lt;h3 id=&quot;reimplementing-models-is-_not_-a-waste-of-time&quot;&gt;Reimplementing models is &lt;em&gt;not&lt;/em&gt; a waste of time&lt;/h3&gt;&lt;p&gt;By contrast, it is a highly beneficial practice
for gaining a deeper understanding into the inner workings
of a deep neural network.
The only price we pay is in person-hours,
yet under the assumption that the model is of strong commercial interest,
that price can only be considered an investment, and not a waste.&lt;/p&gt;
&lt;h2 id=&quot;a-proposed-workflow-for-reimplementing-deep-learning-models&quot;&gt;A proposed workflow for reimplementing deep learning models&lt;/h2&gt;&lt;p&gt;I will now propose a workflow for re-implementing deep learning models.&lt;/p&gt;
&lt;h3 id=&quot;identify-a-coding-partner&quot;&gt;Identify a coding partner&lt;/h3&gt;&lt;p&gt;Pair programming is a productive way of teaching and learning.
Hence, I would start by identifying a coding partner
who has the requisite skillset and shared incentive
to go deep on the model.&lt;/p&gt;
&lt;p&gt;Doing so helps a few ways.&lt;/p&gt;
&lt;p&gt;Firstly, we have real-time peer review on our code,
making it easier for us to catch mistakes that show up.&lt;/p&gt;
&lt;p&gt;Secondly, working together at the same time means that
both myself and my colleague will learn something about the neural network
that we are re-implementing.&lt;/p&gt;
&lt;h3 id=&quot;pick-out-the-forward-step-of-the-model&quot;&gt;Pick out the &quot;forward&quot; step of the model&lt;/h3&gt;&lt;p&gt;The &quot;forward&quot; pass of the model is where the structure of the model is defined:
basically the mathematical operations
that transform the input data into the output observations.&lt;/p&gt;
&lt;p&gt;A few keywords to look out for
are the &lt;code&gt;forward()&lt;/code&gt; and  &lt;code&gt;__call__()&lt;/code&gt; class methods.&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;MyModel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# ...&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# Implementation of model happens here.&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;something&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;something&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For models that involve an autoencoder,
somewhat more seasoned programmers
might create a class method called &lt;code&gt;encoder()&lt;/code&gt; and &lt;code&gt;decoder()&lt;/code&gt;,
which themselves reference another model
that would have a &lt;code&gt;forward()&lt;/code&gt; or &lt;code&gt;__call__()&lt;/code&gt; defined.&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;AutoEncoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# ...&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;something&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;something&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Re-implementing the &lt;code&gt;forward()&lt;/code&gt; part of the model
is usually a good way of building a map
of the equations that are being used
to transform the input data into the output data.&lt;/p&gt;
&lt;h3 id=&quot;inspect-the-shapes-of-the-weights&quot;&gt;Inspect the shapes of the weights&lt;/h3&gt;&lt;p&gt;While the equations give the model &lt;em&gt;structure&lt;/em&gt;,
the weights and biases, or the &lt;em&gt;parameters&lt;/em&gt;,
are the part of the model that are optimized.
(In Bayesian statistics, we would usually presume a model structure,
i.e. the set of equations used alongside the priors,
and fit the model parameters.)&lt;/p&gt;
&lt;p&gt;Because much of deep learning hinges on linear algebra,
and because most of the transformations that happen
involve transforming the &lt;em&gt;input space&lt;/em&gt; into the &lt;em&gt;output space&lt;/em&gt;,
getting the shapes of the parameters is very important.&lt;/p&gt;
&lt;p&gt;In a re-implementation exercise with my intern,
where we re-implemented
a specially designed recurrent neural network layer in JAX,
we did a manual sanity check through our implementation
to identify what the shapes would need to be
for the inputs and outputs.&lt;/p&gt;
&lt;h3 id=&quot;write-tests-for-the-neural-network-components&quot;&gt;Write tests for the neural network components&lt;/h3&gt;&lt;p&gt;Once we have the neural network model and its components implemented,
writing tests for those components is a wonderful way of making sure
that
(1) the implementation is correct, to the best of our knowledge, and that
(2) we can catch when the implementation might have been broken inadvertently.&lt;/p&gt;
&lt;p&gt;The shape test (as described above) is one way of doing this.&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;test_layer_shapes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;assert&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_dims&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If there are special elementwise transforms performed on the data,
such as a ReLU or exponential transform,
we can test that the numerical properties of the output are correct:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;test_layer_shapes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nonlinearity&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;relu&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;assert&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id=&quot;write-tests-for-the-entire-training-loop&quot;&gt;Write tests for the entire training loop&lt;/h3&gt;&lt;p&gt;Once the model has been re-implemented in its entirety,
prepare a small set of training data,
and pass it through the model,
and attempt to train it for a few epochs.&lt;/p&gt;
&lt;p&gt;If the model, as implemented, is doing what we think it should be,
then after a dozen epochs or so,
the training loss should go down.
We can then test that the training loss at the end
is less than the training loss at the beginning.
If the loss does go down, it's necessary but not sufficient for knowing
that the model is implemented correctly.
However, if the loss &lt;em&gt;does not&lt;/em&gt; go down, then we will definitely know
that a problem exists somewhere in the code, and can begin to debug.&lt;/p&gt;
&lt;p&gt;An example with pseudocode below might look like the following:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dummy_graph_data&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gnn_model&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;make_gnn_params&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;losses&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mse_loss&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;jax&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;jax.experimental.optimizers&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;adam&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;test_gnn_training&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Prepare training data&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dummy_graph_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kwargs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;make_gnn_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kwargs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;dloss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mse_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;update&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;adam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.005&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;start_loss&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mse_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dloss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;update&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;end_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mse_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;assert&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;end_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start_loss&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;A side benefit of this is that
if you commit to only judiciously changing the tests,
you will end up with a stable
and copy/paste-able
training loop that you know you can trust
on new learning tasks,
and hence only need to worry about swapping out the data.&lt;/p&gt;
&lt;h3 id=&quot;build-little-tools-for-yourself-that-automate-repetitive-boring-things&quot;&gt;Build little tools for yourself that automate repetitive (boring) things&lt;/h3&gt;&lt;p&gt;You may notice in the above integration test,
we wrote a lot of other functions
that make testing much easier,
such as dummy data generators,
and parameter initializers.&lt;/p&gt;
&lt;p&gt;These are tools that make composing parts of the entire training process
modular and easy to compose.
I strongly recommend writing these things,
and also backing them with more tests
(since we will end up relying on them anyways).&lt;/p&gt;
&lt;h3 id=&quot;now-run-your-deep-learning-experiments&quot;&gt;Now run your deep learning experiments&lt;/h3&gt;&lt;p&gt;Once we have the model re-implemented and tested,
the groundwork is present for us to conduct extensive experiments
with the confidence that we know
how to catch bugs in the model
in a fairly automated fashion.&lt;/p&gt;
&lt;h2 id=&quot;concluding-words&quot;&gt;Concluding words&lt;/h2&gt;&lt;p&gt;Re-implementing deep learning models can be a very fun and rewarding exercise,
because it serves as an excellent tool
to check our understanding of the models that we work with.&lt;/p&gt;
&lt;p&gt;Without the right safeguards in place, though,
it can also very quickly metamorphose into a nightmare rabbithole of debugging.
Placing basic safeguards in place when re-implementing models
helps us avoid as many of these rabbitholes as possible.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/10/30/code-review-in-data-science/">
    <title type="text">Code review in data science</title>
    <id>urn:uuid:56184e25-58be-3132-aed3-fed449074129</id>
    <updated>2019-10-30T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/10/30/code-review-in-data-science/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;&lt;em&gt;This blog post is cross-posted in my &lt;a href=&quot;https://ericmjl.github.io/essays-on-data-science/workflow/code-review/&quot;&gt;essays collection&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The practice of code review is extremely beneficial to the practice of software engineering.
I believe it has its place in data science as well.&lt;/p&gt;
&lt;h2 id=&quot;what-code-review-is&quot;&gt;What code review is&lt;/h2&gt;&lt;p&gt;Code review is the process by which a contributor's newly committed code
is reviewed by one or more teammate(s).
During the review process, the teammate(s) are tasked with ensuring that they&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;understand the code and are able to follow the logic,&lt;/li&gt;
&lt;li&gt;find potential flaws in the newly contributed code,&lt;/li&gt;
&lt;li&gt;identify poorly documented code and confusing use of variable names,&lt;/li&gt;
&lt;li&gt;raise constructive questions and provide constructive feedback&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;on the codebase.&lt;/p&gt;
&lt;p&gt;If you've done the practice of scientific research before,
it is essentially identical to peer review,
except with code being the thing being reviewed instead.&lt;/p&gt;
&lt;h2 id=&quot;what-code-review-_isn-t_&quot;&gt;What code review &lt;em&gt;isn't&lt;/em&gt;&lt;/h2&gt;&lt;p&gt;Code review is not the time
for a senior person to slam the contributions of a junior person,
nor vice versa.&lt;/p&gt;
&lt;h2 id=&quot;why-data-scientists-should-do-code-review&quot;&gt;Why data scientists should do code review&lt;/h2&gt;&lt;p&gt;The first reason is to ensure that project knowledge
is shared amongst teammates.
By doing this, we ensure that
in case the original code creator needs to be offline for whatever reason,
others on the team cover for that person and pick up the analysis.
When N people review the code, N+1 people know what went on.
(It does not necessarily have to be N == number of people on the team.)&lt;/p&gt;
&lt;p&gt;In the context of notebooks, this is even more important.
An analysis is complex,
and involves multiple modelling decisions and assumptions.
Raising these questions,
and pointing out where those assumptions should be documented
(particularly in the notebook)
is a good way of ensuring
that N+1 people know those implicit assumptions that go into the model.&lt;/p&gt;
&lt;p&gt;The second reason is that
even so-called &quot;senior&quot; data scientists are humans,
and will make mistakes.
With my interns and less-experienced colleagues,
I will invite them to constructively raise queries about my code
where it looks confusing to them.
Sometimes, their lack of experience gives me an opportunity to explain
and share design considerations during the code review process,
but at other times, they are correct, and I have made a mistake in my code
that should be rectified.&lt;/p&gt;
&lt;h2 id=&quot;what-code-review-can-be&quot;&gt;What code review can be&lt;/h2&gt;&lt;p&gt;Code review can become a very productive time of learning for all parties.
What it takes is the willingness to listen to the critique provided,
and the willingness to raise issues on the codebase in a constructive fashion.&lt;/p&gt;
&lt;h2 id=&quot;how-code-review-happens&quot;&gt;How code review happens&lt;/h2&gt;&lt;p&gt;Code review happens usually in the context of a pull request
to merge contributed code into the master branch.
The major version control system hosting platforms (GitHub, BitBucket, GitLab)
all provide an interface to show the &quot;diff&quot;
(i.e. newly contributed or deleted code)
and comment directly on the code, in context.&lt;/p&gt;
&lt;p&gt;As such, code review can happen entirely asynchronously, across time zones,
and without needing much in-person interaction.&lt;/p&gt;
&lt;p&gt;Of course, being able to sync up either via a video call,
or by meeting up in person,
has numerous advantages by allowing non-verbal communication to take place.
This helps with building trust between teammates,
and hence doing even &quot;virtual&quot; in-person reviews
can be a way of being inclusive towards remote colleagues.&lt;/p&gt;
&lt;h2 id=&quot;parting-words&quot;&gt;Parting words&lt;/h2&gt;&lt;p&gt;If your firm is set up to use a version control system,
then you probably have the facilities to do code review available.
I hope this essay encourages you to give it a try.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/10/29/ai-will-not-solve-medicine/">
    <title type="text">&quot;AI will not solve medicine&quot;</title>
    <id>urn:uuid:2f60d040-fd91-3f65-986f-e24b5d432f6d</id>
    <updated>2019-10-29T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/10/29/ai-will-not-solve-medicine/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;Those who think &quot;AI will solve medicine&quot; are delusional.
I say this as a practitioner of machine learning in drug discovery and development.&lt;/p&gt;
&lt;p&gt;First things first, &quot;AI&quot; is an overused term.
We should stop using it, especially in medicinal research.&lt;/p&gt;
&lt;p&gt;Now, my thoughts are more sanguine.
The real value proposition of machine learning models in drug development
is to navigate chemical, sequence, pathway, and knowledge space
faster and smarter than we might otherwise do so
without machine learning methods.
It’s a modeling tool, and nothing more than that.
It’s a tool for helping the human collective make better decisions than without it,
but it’s also a double-edged sword.
We can use the tool and then constrain our thinking because we have that tool,
because we want to continue using that tool.
Or we can use the tool to our advantage
and liberate our mind to think of other things.&lt;/p&gt;
&lt;p&gt;This thought was sparked off by an email that I was on at work.
A molecule was approved for &lt;em&gt;continued investigation&lt;/em&gt; (not even &quot;go for safety trials&quot;!),
and 63 people were on that email.
Imagine the number of people
who are involved in getting a molecule past all research-phase checkpoints
&lt;em&gt;and&lt;/em&gt; all 3 clinical trial checkpoints.
&lt;em&gt;Hint: Many people are involved.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;As I combed through the names on that email,
the number of machine learners was vastly outnumbered by the number of colleagues
who toiled daily at the bench,
wrangling with even more uncertainty than that we have at our computers.
We machine learners work in service of them,
delivering insights and prioritized directions,
just as they toil to generate the data that our data-hungry models need.
It’s a symbiotic relationship.&lt;/p&gt;
&lt;p&gt;What do all of those 63 people work on?&lt;/p&gt;
&lt;p&gt;Some make the molecules.
Others design the assays to test the molecules in.
Yet others design the assays to find the target to then develop the assay for.
It’s many layers of human creativity in the loop.
I can’t automate the entirety of their work with my software tools, but I can augment them.
I mean, yeah, I can find a new potential target,
but ultimately it's a molecular biologist
who develops the assay, especially if that assay has never existed before.&lt;/p&gt;
&lt;p&gt;There are others who professionally manage the progress of the project.
There’s sufficient complexity at the bench
and in the silicon chips
that we can’t each keep track of the big picture.
Someone has to do that, and keep everybody focused.&lt;/p&gt;
&lt;p&gt;And then there’s the handful of us who deal with numbers and mainly just numbers.
Yes, it’s a handful.
I counted them on my fingers.
We do have an outsized impact compared to our numbers,
but that’s because we can get computers to do our repetitive work for us.
At the bench, robots are harder to work with.
Having been at the bench before and failing badly at it,
I can very much empathize with how tedious the work is.
It’s expensive to collect that data,
so the onus is on us computation types to get help navigate &quot;data space&quot; more smartly.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/10/18/caching-long-running-function-results/">
    <title type="text">Caching Long-Running Function Results</title>
    <id>urn:uuid:7c950ab3-a3cb-3eaa-b470-3e746ed8ca30</id>
    <updated>2019-10-18T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/10/18/caching-long-running-function-results/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;I found this nifty tool for caching the results of long-running functions: &lt;a href=&quot;https://pypi.org/project/cachier/&quot;&gt;&lt;code&gt;cachier&lt;/code&gt;&lt;/a&gt;. This is useful when we’re building, say, Python applications for which quick interactions are necessary, or for caching the results of a long database query.&lt;/p&gt;
&lt;p&gt;How do we use it? Basically it’s nothing more than a decorator!&lt;/p&gt;
&lt;p&gt;Let’s imagine I have a long-running function as below.&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;long_running_function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arg1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arg2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# stuff happens&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Turns out, if you have a need to cache the result in a lightweight fashion, you can simply add &lt;code&gt;cachier&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;cachier&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cachier&lt;/span&gt;

&lt;span class=&quot;nd&quot;&gt;@cachier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;long_running_function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arg1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arg2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# stuff happens&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The result is stored in your home directory, so the cache is accessible to you.&lt;/p&gt;
&lt;p&gt;One nice thing &lt;code&gt;cachier&lt;/code&gt; also offers is the ability to set a time duration after which the cache goes stale. This can be useful in situations where you know that you need to refresh the cache, such as a database query that may go stale because of new data added into it. This is done by specifying the &lt;code&gt;stale_after&lt;/code&gt; keyword argument:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;cachier&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cachier&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;datetime&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;timedelta&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Re-cache result after 1 week.&lt;/span&gt;
&lt;span class=&quot;nd&quot;&gt;@cachier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stale_after&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;timedelta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weeks&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;long_running_function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arg1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arg2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# stuff happens&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If you need to reset the cache manually, you can always do:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;long_running_function&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clear_cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;There are other advanced features that &lt;code&gt;cachier&lt;/code&gt; provides, and so I’d encourage you to go and take a look at it!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/10/5/multiple-coin-flips-vs-one-coin-flip-generalized/">
    <title type="text">Multiple Coin-Flips vs. One Coin Flip Generalized?</title>
    <id>urn:uuid:4f62bcf6-339a-3ffa-b585-fc7108ab6371</id>
    <updated>2019-10-05T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/10/5/multiple-coin-flips-vs-one-coin-flip-generalized/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;Do people learn better by:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Generalizing from one example explained well, or by&lt;/li&gt;
&lt;li&gt;Having multiple case studies that highlight the same point?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I think both are needed, but I am also torn sometimes by whether it’s more effective to communicate using the former or the latter.&lt;/p&gt;
&lt;p&gt;Case in point: In teaching Bayesian statistics, the coin flip is a particular case of the Beta-Binomial model. However, the Beta-Binomial model can be taken from its most elementary form (estimation on one group) through to its most sophisticated form (hierarchically estimating &lt;code&gt;p&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;I guess if the goal is to show how broadly applicable a given model class (i.e. the beta-binomial model) is, a teacher would elect to jump between multiple examples that are apparently distinct. However, if the goal is build depth (i.e. going from single group to multiple group estimation), sticking with one example (e.g. of baseball players, classically) would be the better strategy.&lt;/p&gt;
&lt;p&gt;Both are needed, just at different times, I think. Thinking through this example, I think, gives me a first-principles way of deciding which approach to go for.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/10/5/jupyter-server-with-https-on-personal-server/">
    <title type="text">Jupyter Server with HTTPS on Personal Server</title>
    <id>urn:uuid:76cd7411-e7c6-320a-8edd-b0551d6aaac3</id>
    <updated>2019-10-05T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/10/5/jupyter-server-with-https-on-personal-server/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;Recording this for myself, since I did it once and probably don't have the brain bandwidth to remember this through repetition.&lt;/p&gt;
&lt;p&gt;I have known how to run a &quot;public&quot; Jupyter server (password-protected, naturally), but one thing I've struggled with was getting HTTPS working.&lt;/p&gt;
&lt;p&gt;Turns out, the &lt;code&gt;letsencrypt&lt;/code&gt; instructions aren't that bad on Jupyter's docs. I just was ignorant in the past, and didn't know enough about Linux to get this working right.&lt;/p&gt;
&lt;p&gt;The key here is creating a &lt;code&gt;letsencrypt&lt;/code&gt; certificate, and making sure file permissions are set correctly.&lt;/p&gt;
&lt;p&gt;First off, go to the &lt;a href=&quot;https://certbot.eff.org&quot;&gt;Certbot page&lt;/a&gt;. Select the type of website you're running and operating system. For Jupyter, I chose &quot;None of the Above&quot; and &quot;Ubuntu 18.04 LTS (bionic)&quot; (even though I'm technically on Ubuntu 19). (Here's a &lt;a href=&quot;https://certbot.eff.org/lets-encrypt/ubuntubionic-other&quot;&gt;shortcut link&lt;/a&gt; to the instructions if you're in the same situation.)&lt;/p&gt;
&lt;p&gt;On my system (Ubuntu-based), I used the following commands to install &lt;code&gt;certbot&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Add repository&lt;/span&gt;
sudo apt-get update
sudo apt-get install software-properties-common
sudo add-apt-repository universe
sudo add-apt-repository ppa:certbot/certbot
sudo apt-get update

&lt;span class=&quot;c1&quot;&gt;# Install certbot&lt;/span&gt;
sudo apt-get install certbot

&lt;span class=&quot;c1&quot;&gt;# Run certbot&lt;/span&gt;
sudo certbot certonly --standalone
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Follow the instructions. &lt;code&gt;certbot&lt;/code&gt; will install into a protected directory. In my case, it was &lt;code&gt;/etc/letsencrypt/live/&amp;lt;mywebsite&amp;gt;/&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Here, a problem will show up. That directory above is not accessible by a Jupyter server run under a user other than &lt;code&gt;root&lt;/code&gt;. But a desired property of running Jupyter servers is that we don't have to use &lt;code&gt;sudo&lt;/code&gt; to run it. How can we solve this? Basically, by making sure that the certificate is readable by a non-&lt;code&gt;root&lt;/code&gt; user.&lt;/p&gt;
&lt;p&gt;What I did, then, was to copy the files that were created by &lt;code&gt;certbot&lt;/code&gt; into a location under my home directory. For security by obscurity, I'm naturally not revealing its identity. Then, I changed ownership of those files to my username:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;pwd&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# you should be in the directory where the certbot-created files are located&lt;/span&gt;
su -
chown &amp;lt;myusername&amp;gt; *.pem  &lt;span class=&quot;c1&quot;&gt;# changes ownership of those files&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Finally, I went into my Jupyter config (&lt;code&gt;~/.jupyter/jupyter_notebook_config.py&lt;/code&gt;, this is well-known), and edited the two lines that specified the &quot;certfile&quot; and the &quot;keyfile&quot;:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NotebookApp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;certfile&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;sa&quot;&gt;u&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;/absolute/path/to/your/certificate/mycert.pem&amp;#39;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NotebookApp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keyfile&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;sa&quot;&gt;u&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;/absolute/path/to/your/certificate/mykey.key&amp;#39;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If this helps you, leave me a note in the comments below. :)&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/9/7/dokku-building-an-internal-heroku-at-work/">
    <title type="text">Dokku: Building an internal Heroku at work</title>
    <id>urn:uuid:e193b1a1-5835-3857-b7ba-5e27a6f3bdc3</id>
    <updated>2019-09-07T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/9/7/dokku-building-an-internal-heroku-at-work/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;At work, we don’t have a service that has the simplicity of Heroku. Part of it is that we’re still behind what’s available for free in my FOSS life (both commercial and FOSS offerings), and cybersecurity tends to be a gatekeeper against adoption of new things, which is a reality I have to face at work.&lt;/p&gt;
&lt;p&gt;BUT! I am unwilling to simply bow down to this secnario. &quot;There’s got to be a better way.&quot;&lt;/p&gt;
&lt;p&gt;What does that mean? It means if we want a Heroku-like thing internally, we have to hack together workarounds.&lt;/p&gt;
&lt;p&gt;Enter &lt;a href=&quot;http://dokku.viewdocs.io/dokku/&quot;&gt;Dokku&lt;/a&gt;!&lt;/p&gt;
&lt;p&gt;What is it? It’s a FOSS implementation of the functionality that Heroku provides. It’s only slightly more involved than Heroku, and gives us a really nice taste of what’s possible with Heroku.&lt;/p&gt;
&lt;p&gt;Dokku claims to be the &quot;smallest PaaS implementation you’ve ever seen&quot;, and I fully believe it. The maintainers have done a wonderful thing, making the installation process as simple and clean as possible. I’ve successfully installed it on a bare DigitalOcean droplet and on my home Linux tower. I’ve also successfully installed it in EC2 instances at work, albeit needing a few minor modifications to the script they provide.&lt;/p&gt;
&lt;h2 id=&quot;why-would-i-want-to-use-dokku?&quot;&gt;Why would I want to use Dokku?&lt;/h2&gt;&lt;p&gt;Taking Dokku on my DigitalOcean droplet as an example, what it effectively provides is a self-hosted Heroku.&lt;/p&gt;
&lt;p&gt;This means you can get 95% of the convenience that Heroku offers, except done in-house. This can be handy if you’ve got cybersecurity standing in the way of awesome convenience, or if finance isn’t willing to shell out the moolah.&lt;/p&gt;
&lt;h2 id=&quot;what-can-we-do-with-dokku?&quot;&gt;What can we do with Dokku?&lt;/h2&gt;&lt;p&gt;Here’s a few neat things that we can do.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We can provision a database to run on the same compute node as the app, and then link them together. If your compute node is &quot;beefy&quot; enough (RAM/CPU/storage-wise) to handle both the database and the app (and I mean, I’m confident that most disposable apps aren’t going to be at a large scale), then it can be pretty handy, because it means we save on latency.&lt;/li&gt;
&lt;li&gt;We can deploy apps using either Heroku buildpacks (which look for Procfiles) or using Dockerfiles. Docker containers can be easier to maintain if we have a large and/or complex &lt;code&gt;conda&lt;/code&gt; environment, in my opinion, as we can reuse the existing environment spec, but Procfiles are much nicer for smaller projects. This fits with the paradigm of &quot;declaring what you need&quot;, rather than &quot;programming what you need&quot;.&lt;/li&gt;
&lt;li&gt;Because Dokku is managing everything through isolated Docker containers, we can actually enter into a Docker container and muck around to debug, without worrying about breaking the broader system. I realize now how neat it is to have containerization, but without a unified front-end interface to manage the containers, networking interfaces, and environment variables, it’s tough to keep everything straight. Dokku provides that front-end interface.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;what-are-you-deploying-right-now?&quot;&gt;What are you deploying right now?&lt;/h2&gt;&lt;p&gt;On my DigitalOcean box, which I use for personal projects, I have deployed both the &quot;Getting Started&quot; Ruby app that Heroku provides as well as a &lt;a href=&quot;../../../../../blog/2019/9/7/dokku-building-an-internal-heroku-at-work/minimal-panel.ericmjl.com&quot;&gt;minimal app&lt;/a&gt; showcasing a minimal dashboard using &lt;a href=&quot;../../../../../blog/2019/9/7/dokku-building-an-internal-heroku-at-work/panel.pyviz.org&quot;&gt;Panel&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The easy part was getting Dokku up and running. The hard part, though, was getting URLs and DNSs right. It took some debugging to get that work correctly.&lt;/p&gt;
&lt;p&gt;In particular, Dokku uses a concept called virtual hosts (&lt;code&gt;VHOSTS&lt;/code&gt;) to route from the Dokku host to individual containers. For example, to get &lt;code&gt;minimal-panel.ericmjl.com&lt;/code&gt; up and running correctly, I had to ensure that &lt;code&gt;*.ericmjl.com&lt;/code&gt; was routed to my DigitalOcean box.&lt;/p&gt;
&lt;h2 id=&quot;how-have-we-used-this-at-work?&quot;&gt;How have we used this at work?&lt;/h2&gt;&lt;p&gt;At work, I just finished prototyping the use of Dokku on EC2. In particular, I was able to deploy both Dockerfile-based and Procfile-based projects. Once again, getting a domain was the most troublesome part of this project; spinning up an EC2 instance and configuring it became easy using a simple Bash script which we executed on each test spin-up machine.&lt;/p&gt;
&lt;h2 id=&quot;what-changes-between-heroku-and-dokku?&quot;&gt;What changes between Heroku and Dokku?&lt;/h2&gt;&lt;p&gt;The biggest thing I found is that I need to at least have SSH-access to the compute box that is running Dokku. This is because what we would usually configure on Heroku’s web interface (e.g. environment variables), we would instead configure using &lt;code&gt;dokku&lt;/code&gt;’s command-line interface via SSH. Hence, not being afraid of the CLI is important.&lt;/p&gt;
&lt;h2 id=&quot;whats-your-verdict?&quot;&gt;What’s your verdict?&lt;/h2&gt;&lt;p&gt;If you know Heroku, Dokku gets you 95% of the convenience you’re used to, plus quite a bit more flexibility to customize it to your own compute environment.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/7/29/how-to-be-a-great-code-sprinter/">
    <title type="text">How to be a great code sprinter</title>
    <id>urn:uuid:871d5f77-e092-3bbc-a4ae-5584c93b64e2</id>
    <updated>2019-07-29T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/7/29/how-to-be-a-great-code-sprinter/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;This blog post is the second in a series of two on participating in code sprints. The first one is &lt;a href=&quot;https://ericmjl.github.io/blog/2019/7/21/how-to-lead-a-great-code-sprint/&quot;&gt;here&lt;/a&gt;. In this post, I will write about how a sprinter themselves can also help contribute to a positive sprint experience all-ways.&lt;/p&gt;
&lt;h2 id=&quot;read-the-docs-to-understand-the-scope-of-the-project&quot;&gt;Read the docs to understand the scope of the project&lt;/h2&gt;&lt;p&gt;As a sprinter, we may often have preconceived notions about what a project is about. It helps to have an accurate view on what a project is and isn’t about. This is oftentimes best accomplished by reading the documentation of that project, assuming the docs are well-written. Doing so can help you better align what you think should be done on the project with what the package maintainer sees as priorities for the project.&lt;/p&gt;
&lt;h2 id=&quot;be-ready-to-make-documentation-contributions-of-any-scale&quot;&gt;Be ready to make documentation contributions of any scale&lt;/h2&gt;&lt;p&gt;Documentation is oftentimes the hardest thing for a package maintainer to write, because it often entails slowing down to a beginner’s speed (an unnatural speed at this point), while knowing one’s own blind spots on where a beginner would stumble (also challenging to do).&lt;/p&gt;
&lt;p&gt;If you are newcomer sprinter, by focusing on the sections of the docs that pertain to &quot;processes&quot; (e.g. getting development environment setup) and slowly working through them and documenting what’s missing, that can go a long way to helping other newcomers get set up as well. Anything that the maintainer leaves out may need to be made explicitly clear - and you can help make it clear!&lt;/p&gt;
&lt;p&gt;Package maintainers, and prior contributors, are human. That means that there inadvertently may be errors in language that may have been inserted into the package. Any small patch that fixes the docs, including even small typographical errors, can be very helpful to improving documentation quality.&lt;/p&gt;
&lt;h2 id=&quot;dont-be-afraid-to-ask-questions...&quot;&gt;Don’t be afraid to ask questions...&lt;/h2&gt;&lt;p&gt;You will find that asking questions can really accelerate your own progress on the project. This is important for getting unstuck, wherever you might be stuck.&lt;/p&gt;
&lt;h2 id=&quot;...but-also-try-keep-your-questions-to-the-non-obvious-things.&quot;&gt;...but also try keep your questions to the non-obvious things.&lt;/h2&gt;&lt;p&gt;That said, asking the too-simple questions that can be answered by a Google query is likely going to steal time and attention away from other sprinters who might have more substantial questions on hand.&lt;/p&gt;
&lt;p&gt;A pet peeve of mine is asking questions that can be answered in the docs. Asking these questions of the maintainer doesn’t reflect positively on you, the sprinter. Whether or not you intended, what often gets received/communicated to the maintainer is carelessness and a lack of attention to detail, the opposite of both being generally good qualities to possess and project.&lt;/p&gt;
&lt;p&gt;There’s a pretty broad balance point between the two, so don’t feel inhibited by fear of not hitting a precise balance between looking for docs and asking questions.&lt;/p&gt;
&lt;h2 id=&quot;for-any-feature-requests-try-to-be-ready-with-a-proposed-implementation&quot;&gt;For any feature requests, try to be ready with a proposed implementation&lt;/h2&gt;&lt;p&gt;This one I find very important. Having a proposed implementation on hand for a thing that you think should be in the library goes a long way to helping the package maintainer (or other contributors) see what exactly you’re trying to accomplish with that feature. Having a sketch on-hand makes it much easier for the package maintainer to say &quot;yes&quot; to the new feature, and having written the documentation and a proposed suite of tests for that new feature makes it &lt;em&gt;even easier&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;If you aren’t able to propose an implementation, then raising an inquiry rather than a request makes a world of difference in how a package maintainer perceives the communication of the issue at hand.&lt;/p&gt;
&lt;p&gt;As an example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Request&lt;/strong&gt;: &quot;Package X should be able to do Y.&quot;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Inquiry&lt;/strong&gt;: &quot;Is it within scope for package X to be able to do Y?&quot; or &quot;Has this feature Y been considered before?&quot;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The latter are more thoughtful, and communicates much less a sense of entitlement on the part of the sprinter’s request.&lt;/p&gt;
&lt;h2 id=&quot;were-all-building-mental-maps-of-each-others-knowledge&quot;&gt;We’re all building mental maps of each others’ knowledge&lt;/h2&gt;&lt;p&gt;When two colleagues meet for the first time, we have to build a mental model of each others’ strengths. At a sprint, the package maintainer has to multiply this by however many people are sprinting.&lt;/p&gt;
&lt;p&gt;If they are making an effort to map your skills against theirs, they may be very verbose, asking lots of questions to clarify what you do and don’t know. It pays to be patient here.&lt;/p&gt;
&lt;p&gt;If they don’t have the bandwidth to do so (and this is a charitable description for some maintainers), then they may be glossing over detail. Rather than being stuck, it pays to interrupt them gently and clarify. (Taking notes is a very good way of communicating that you’re treating this process seriously too!)&lt;/p&gt;
&lt;h2 id=&quot;give-your-sprint-leader-sufficient-context&quot;&gt;Give your sprint leader sufficient context&lt;/h2&gt;&lt;p&gt;As mentioned above, the sprint leader will oftentimes be context switching from person to person. It’s mentally exhausting, so spoon-feeding a bit more context (such as the thing you’re working on), and condensing your question to the essentials and asking it very precisely can go a long way to helping your sprint leader help you better.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/7/26/pyviz-panel-apps/">
    <title type="text">PyViz Panel Apps</title>
    <id>urn:uuid:45aaf34d-e0d0-3611-99df-f9c8e2650355</id>
    <updated>2019-07-26T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/7/26/pyviz-panel-apps/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;I finally learned how to build and serve apps with Panel!&lt;/p&gt;
&lt;p&gt;Here are the key ideas:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Prototype the app inside a Jupyter notebook. That gives the real-time feedback on whether your apps/widgets are working or not. &lt;/li&gt;
&lt;li&gt;The most important thing is that the final thing you package together is now a &lt;code&gt;.servable()&lt;/code&gt; object.&lt;/li&gt;
&lt;li&gt;Use Panel’s &lt;code&gt;serve&lt;/code&gt; command to test the app locally. It’s actually quite magical - the serve command can actually parse a Jupyter notebook and serve it up on a local web server.&lt;/li&gt;
&lt;li&gt;When you’ve confirmed that everything is working properly locally, Heroku is a great deployment option. Using the default Python buildpack and a &lt;code&gt;requirements.txt&lt;/code&gt; file, one can easily specify the exact Python environment for deployment.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As a pedagogical implementation, I put up a &lt;a href=&quot;https://github.com/ericmjl/minimal-panel-app&quot;&gt;minimal panel app on GitHub&lt;/a&gt;, and also &lt;a href=&quot;https://minimal-panel-app.herokuapp.com&quot;&gt;served it up on Heroku&lt;/a&gt;. Come check it out! I hope it’s useful for you.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/7/23/t-distributed-likelihoods-are-kind-of-neat/">
    <title type="text">T-distributed likelihoods are kind of neat</title>
    <id>urn:uuid:73bf8792-c5e2-3d6b-8dd6-0d3bf2e6aa6d</id>
    <updated>2019-07-23T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/7/23/t-distributed-likelihoods-are-kind-of-neat/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;The Student’s T distribution is the generalization of the Gaussian and Cauchy distributions. How so? Basically by use of its &quot;degrees of freedom&quot; ($df$) parameter.&lt;/p&gt;
&lt;p&gt;If we plot the probability density functions of the T distribution with varying degrees of freedom, and compare them to the Cauchy and Gaussian distributions, we get the following:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;../../../../../blog/2019/7/23/t-distributed-likelihoods-are-kind-of-neat/t-distributions.png&quot;&gt;&lt;img src=&quot;../../../../../blog/2019/7/23/t-distributed-likelihoods-are-kind-of-neat/t-distributions.png&quot; alt=&quot;Student T distributions with varying degrees of freedom.&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Notice that when $df=1$, the T distribution is identical to the Cauchy distribution, and that as $df$ increases, it gradually becomes more and more like the Normal distribution. At $df=30$, we can consider it to be approximately enough Gaussian.&lt;/p&gt;
&lt;p&gt;On its own, this is already quite useful; when placed in the context of a hierarchical Bayesian model, that’s when it gets even more interesting! In a hierarchical Bayesian model, we are using samples to estimate group-level parameters, but constraining group parameters to vary mostly like each other, unless evidence in the data suggests otherwise. If we allow the $df$ parameter to vary, then if some groups look more Cauchy while other groups look more Gaussian, this can be flexibly captured in the model.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/7/21/how-to-lead-a-great-code-sprint/">
    <title type="text">How to lead a great code sprint</title>
    <id>urn:uuid:d31597fa-6b63-37e8-adbf-b3bf799955a4</id>
    <updated>2019-07-21T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/7/21/how-to-lead-a-great-code-sprint/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;This blog post is the first in a series of two blog posts on participating in code sprints, and is the culmination of two other blog posts I’ve written on leading a sprint. In this post, I’ll be writing it from the perspective of what a sprint participant might appreciate from a sprint leader.&lt;/p&gt;
&lt;h2 id=&quot;write-good-docs&quot;&gt;Write good docs&lt;/h2&gt;&lt;p&gt;Documentation scales you, the package maintainer. Good docs let others get going without needing your intervention, while bad docs create more confusion. Write good docs ahead-of-time on:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The purpose and scope of the project.&lt;/li&gt;
&lt;li&gt;How to get setup for contributing&lt;/li&gt;
&lt;li&gt;What contributors should look out for when contributing, including:&lt;ul&gt;
&lt;li&gt;Code style&lt;/li&gt;
&lt;li&gt;Function scope&lt;/li&gt;
&lt;li&gt;Documentation requirements&lt;/li&gt;
&lt;li&gt;Testing requirements&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;How contributors can contribute without necessarily providing code&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;require-documentation-as-a-first-contribution&quot;&gt;Require documentation as a first contribution&lt;/h2&gt;&lt;p&gt;This may not necessarily apply to all projects, but for small-ish enough projects, this might be highly relevant. Requiring documentation contributions as the first contribution has a few nice side effects for newcomer contributors:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;This enforces familiarity with the project before making contributions.&lt;/li&gt;
&lt;li&gt;It’s a very egalitarian way to kick-off the sprints, reducing the probability of sprinter anxiety from falling behind.&lt;/li&gt;
&lt;li&gt;This reduces the burden of new contributions for first-time sprinters: docs do not break code!&lt;/li&gt;
&lt;li&gt;Apart from being non-intimidating, it can sometimes give rise to repetitive tasks that newcomer sprinters with which newcomers can practice git workflow.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;make-clear-who-should-sprint-with-your-project&quot;&gt;Make clear who should sprint with your project&lt;/h2&gt;&lt;p&gt;Mismatched expectations breed frustration; hence, making clear what pre-requisite knowledge participants should have can go a long way to reducing frustrations later on.&lt;/p&gt;
&lt;p&gt;Drawing on my &lt;code&gt;pyjanitor&lt;/code&gt; experience, I would want participants to at the minimum be me of the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;pandas&lt;/code&gt; users who have frustrations with the library, and would like to make contributions, or&lt;/li&gt;
&lt;li&gt;Individuals who wish to make a documentation contribution and don’t mind doing a fine-toothed pass over the docs to figure out what is unclear in the docs.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Defining your so-called &quot;sprint audience&quot; ahead-of-time can go a long way to making the sprint productive and friendly.&lt;/p&gt;
&lt;h2 id=&quot;communicate-priorities-of-the-project&quot;&gt;Communicate priorities of the project&lt;/h2&gt;&lt;p&gt;Sprinters want to know that their contributions are going to be valued. Though it is easy to say, &quot;come talk with me before you embark on a task&quot;, the reality is that you, the sprint lead, are likely going to be extremely overbooked. One way to get around this, I think, is to have a high-level list of priorities for the sprint, which can help sprinters better strategize which tasks to tackle. Communicate this on a whiteboard, large sticky notes, or online Wiki page that you can direct people to.&lt;/p&gt;
&lt;h2 id=&quot;have-a-publicly-viewable-file-lock&quot;&gt;Have a publicly-viewable &quot;file lock&quot;&lt;/h2&gt;&lt;p&gt;Merge conflicts will inadvertently show up if multiple people are contributing to the same file simultaneously. It helps to have a publicly-viewable &quot;file lock&quot; on, say, a whiteboard or large sticky notes, so that we know who is working on what file. This helps prevent you, the sprint lead, from accidentally getting two people to work on the same file, and then having to resolve merge conflicts later.&lt;/p&gt;
&lt;p&gt;In the &lt;code&gt;pyjanitor&lt;/code&gt; sprints, I frequently approved two people working on the same notebook; resolving merge conflicts in the notebook JSON later proved to be a big pain! This lesson was one learned hard.&lt;/p&gt;
&lt;h2 id=&quot;encourage-contribution-of-life-like-examples&quot;&gt;Encourage contribution of life-like examples&lt;/h2&gt;&lt;p&gt;If there are new package users in the crowd who want to get familiar with the package, then encouraging them to contribute life-like examples is a great way to have them make a contribution! This has some nice side effects:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;In creating the example, they may find limitations in the package that could form the substrate of future contributions.&lt;/li&gt;
&lt;li&gt;By using the library in the creation of an example, they become users of the project themselves.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;celebrate-every-contribution&quot;&gt;Celebrate every contribution&lt;/h2&gt;&lt;p&gt;This one is particularly important for first-time contributors. Oftentimes, they have never done standard Gitflow, and that is intimidating enough. So it doesn’t matter if the contribution is nothing more than deleting an unnecessary plural &lt;code&gt;s&lt;/code&gt; or correcting a broken URL. We should celebrate that contribution, because they have now learned how to make a contribution (regardless of type), and can repeat the unfamiliar Gitflow pattern until they have it muscle memorized.&lt;/p&gt;
&lt;p&gt;At the SciPy sprints, for the &lt;code&gt;pyjanitor&lt;/code&gt; project, once a contributor’s PR finished building and passed all checks, I brought my iPad over to their table to let them hit the Big Green Button on GitHub. This is one touch I am quite confident our sprinters loved!&lt;/p&gt;
&lt;h2 id=&quot;recognize-and-assign-non-code-tasks&quot;&gt;Recognize and assign non-code tasks&lt;/h2&gt;&lt;p&gt;While code contributions are useful, I think a great way to encourage them to help out would be to have them help with non-code contributions. A few examples include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Debugging others’ setups&lt;/li&gt;
&lt;li&gt;Triaging/tagging issues on your issue tracker&lt;/li&gt;
&lt;li&gt;Talking with sprinters to help them prioritize&lt;/li&gt;
&lt;li&gt;Social media sprinting for the project&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is because you, the sprinter, might have your hands full helping beginners, and so having as much help as possible is extremely helpful to you. Be sure, of course, the acknowledge them in some public way that expresses your appreciation of their effort, because they might not necessarily get into the commit record (which, by the way, is not the only way to keep track of contributions).&lt;/p&gt;
&lt;h2 id=&quot;stay-humble-and-calm&quot;&gt;Stay humble and calm&lt;/h2&gt;&lt;p&gt;In open source software development, it is hard to find contributors who are willing to sustain an effort, and so any contributions are generally welcome (barring those that are clearly out of scope). Hence, as far as it is humanly possible, I would be inclined to express appreciation for contributors’ contributions.&lt;/p&gt;
&lt;p&gt;One of the PyMC maintainers, &lt;a href=&quot;https://colindcarroll.com&quot;&gt;Colin Carroll&lt;/a&gt;, said something of a contribution that I wanted to make that stuck with me. The gist of it was as follows:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;It’s a contribution from someone who is willing, and I’d take that any day.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So yes, even though we may see our project as providing an opportunity for newcomers to contribute, the fact that they are willing to contribute is even more so an important thing to recognize! Gratitude makes more sense than entitlement here.&lt;/p&gt;
&lt;p&gt;Staying calm is also important. It’s easy to get irritated because of all of the context switching that happens. Leverage the help you can get from your sprint co-leads to help shoulder the load. If you take good care of your mental state, you can help make the sprints fun and productive for others.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/7/15/scipy-2019-post-conference/">
    <title type="text">SciPy 2019 Post-Conference</title>
    <id>urn:uuid:c2d83688-0df1-3f2d-9e58-23b29a5f3730</id>
    <updated>2019-07-15T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/7/15/scipy-2019-post-conference/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;It’s my last day in Austin, TX, having finished a long week of conferencing at &lt;a href=&quot;https://www.scipy2019.scipy.org/&quot;&gt;SciPy 2019&lt;/a&gt;. This trip was very fruitful and productive! At the same time, I’m ready for a quieter change - meeting and talking with people does take a drain on my brain, and I have a mildly strong preference for quiet time over interaction time.&lt;/p&gt;
&lt;h2 id=&quot;tutorials&quot;&gt;Tutorials&lt;/h2&gt;&lt;p&gt;I participated in the tutorials as an instructor for three tutorials, which I think have become my &quot;data science toolkit&quot;: &lt;a href=&quot;https://www.youtube.com/watch?v=2wvt6GPZl1U&quot;&gt;Bayesian statistical modeling&lt;/a&gt;, network analysis, and &lt;a href=&quot;https://www.youtube.com/watch?v=JPBz7-UCqRo&quot;&gt;deep learning&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Of the three, the one I had the most fun teaching was the deep learning one. The goal of that tutorial was to peel back a layer behind the frameworks and see what’s going on. To reinforce this and make it all concrete, we live coded a deep learning framework prototype, and it worked! (I didn’t plan for it, and so I was quite nervous while doing it, but we pulled it off as a class, and I think it reinforced the point about revealing what goes on underneath a framework.&lt;/p&gt;
&lt;p&gt;I also had a lot of fun teaching the Bayesian statistical modeling tutorial, which I had co-created with Hugo Bowne-Anderson, and as always, my personal &quot;evergreen&quot; tutorial on Network Analysis always brings me joy, especially when we reach the end and talk about graphs and matrices. I think the material connecting linear algebra to graph concepts is one that the crowd enjoys, and I might emphasize it more going forth at the SciPy tutorials.&lt;/p&gt;
&lt;h2 id=&quot;talks&quot;&gt;Talks&lt;/h2&gt;&lt;p&gt;This year, &lt;a href=&quot;https://www.youtube.com/watch?v=sSIT0rJh2OM&quot;&gt;I delivered a talk on &lt;code&gt;pyjanitor&lt;/code&gt;&lt;/a&gt;. Excluding lightning talks, this is probably the first time I’ve started my slides one day before having to deliver it (yikes!). Granted, I’ve had the outline in my head for a long time now, I guess having to do the talk was good impetus to actually get it done.&lt;/p&gt;
&lt;p&gt;Apart from that, there’s a rich selection of talks at SciPy from which I think we can screen at work over lunches (Data Science YouTube). I particularly like &lt;a href=&quot;https://www.youtube.com/watch?v=J_aymk4YXhg&quot;&gt;the talk&lt;/a&gt; on &lt;a href=&quot;https://optuna.org/&quot;&gt;Optuna&lt;/a&gt;, a framework for hyperparameter optimization, and I think I’ll be using this tool going forwards.&lt;/p&gt;
&lt;h2 id=&quot;sprints&quot;&gt;Sprints&lt;/h2&gt;&lt;p&gt;I did a sprint on &lt;code&gt;pyjanitor&lt;/code&gt; with my colleague Zach Barry. This sprint, we had about 20+ sprinters join us, the vast majority of them being first-time sprinters.&lt;/p&gt;
&lt;p&gt;One thing that stuck for me, this time round, is how even first-timers have different degrees of experience. Some know &lt;code&gt;git&lt;/code&gt; while most others don’t; most don’t have any prior experience with &lt;a href=&quot;https://www.atlassian.com/git/tutorials/comparing-workflows/gitflow-workflow&quot;&gt;Gitflow&lt;/a&gt;. I had an interaction that led me to realize it’s very important to state meaningfully what &quot;beginner&quot; means in concrete terms. For example, a &quot;beginner&quot; &lt;code&gt;pyjanitor&lt;/code&gt; contributor is probably a &lt;code&gt;pandas&lt;/code&gt; user, may or may not have used &lt;code&gt;git&lt;/code&gt; before, probably doesn’t know GitFlow. A common prerequisite quality amongst contributors would probably be that they would have the patience to&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Read the documentation,&lt;/li&gt;
&lt;li&gt;Attempt at least one pass digesting the documentation, and&lt;/li&gt;
&lt;li&gt;Ask questions regarding the intent behind something before asking for a change.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In terms of the things accomplished at this sprint, contributions mainly revolved around:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Improving language in the docs,&lt;/li&gt;
&lt;li&gt;New functions, and&lt;/li&gt;
&lt;li&gt;New example notebooks.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In addition to &lt;code&gt;pyjanitor&lt;/code&gt; sprinting, special thanks goes to &lt;a href=&quot;https://twitter.com/ocefpaf&quot;&gt;Felipe Fernandes&lt;/a&gt;, who helped me &lt;a href=&quot;https://anaconda.org/conda-forge/jax&quot;&gt;get &lt;code&gt;jax&lt;/code&gt; up onto conda-forge&lt;/a&gt;! SciPy is really the place where we can get to meet people &lt;em&gt;and&lt;/em&gt; get things done.&lt;/p&gt;
&lt;h2 id=&quot;career-advice-learned&quot;&gt;Career advice learned&lt;/h2&gt;&lt;p&gt;While at SciPy, I had a chance to talk with Eric Jones, CEO of Enthought. Having described my current role at work, he mentioned how having a team like the one I’m on parked inside IT gives us a very unique position to connect data science work across the organization to the consumers of our data products. When I raised to him my frustrations regarding our infatuation with vendors &lt;em&gt;when FOSS alternatives clearly exist&lt;/em&gt;, his advice in return was essentially this:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Focus on leveling-up your colleagues skills and knowledge, keep pushing the education piece at work, and don’t worry about the money that gets spent on tooling.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Having thought about this, I agree. Over time, we should let the results speak. At the same time, I want to help create the environment that I would like to work in: where my colleagues use the same tooling stack, are hacker-types, aren’t afraid to dig deep into the &quot;computer stuff&quot; and into the biology/chemistry, and have the necessary skill + desire to design machine learning systems to systematically accelerate discovery science.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/7/7/order-of-magnitude-is-more-than-accurate-enough/">
    <title type="text">Order of magnitude is more than accurate enough</title>
    <id>urn:uuid:c54c9f1a-17cd-3db2-83f7-740b261e796b</id>
    <updated>2019-07-07T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/7/7/order-of-magnitude-is-more-than-accurate-enough/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;When I was in &lt;a href=&quot;https://scienceone.ubc.ca/&quot;&gt;Science One&lt;/a&gt; at UBC in 2006, our Physics professor, &lt;a href=&quot;https://www.phas.ubc.ca/users/mark-halpern&quot;&gt;Mark Halpern&lt;/a&gt;, said a quotable statement that has stuck for many years.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Order of magnitude is more than accurate enough.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Mark Halpern&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;At the time, that statement rocked the class, myself included. We were classically taught that significant digits are significant, and that we have to keep track of them. But Mark’s quote seemed to throw all of that caution and precision in Physics into the wind. Did what we learn in Physics lab class not matter?&lt;/p&gt;
&lt;p&gt;Turns out, there was one highly instructive activity that still hasn’t left my mind. We were asked, during a recitation, to estimate how many days the city of Vancouver could be powered for if we took a piece of chalk and converted its entire mass into energy. This clearly required estimation of chalk mass and Vancouver daily energy consumption, both of which we had no way of accurately knowing.&lt;/p&gt;
&lt;p&gt;Regardless, I took it upon myself to carry significant digits in our calculation, while my recitation partner, Charles Au, was fully convinced that this wasn’t necessary, and so did all calculations order-of-magnitude. We debated and agreed upon what assumptions we needed to arrive at a solution, and then proceeded to do the same calculations, one with significant digits, the other without.&lt;/p&gt;
&lt;p&gt;We reached the same conclusion.&lt;/p&gt;
&lt;p&gt;More precisely, I remember obtaining a result along the lines of $6.2 \cdot 10^3$ days, while Charles obtained $10^4$ days. On an order of magnitude, more or less equivalent.&lt;/p&gt;
&lt;p&gt;In retrospect, I shouldn’t have been so surprised. Mark is an astrophysicist, and at that scale, 1 or 2 significant digits might not carry the most importance; rather, getting into the right ballpark might be more important. At the same time, the recitation activity was a powerful first-hand experience of that last point: getting into the right ballpark first.&lt;/p&gt;
&lt;p&gt;At the same time, I was also missing a second perspective, which then explains my surprise at Mark’s quote. Now that I’ve gone the route of more statistics-oriented work, I see a similar theme showing up. John Tukey said something along these lines:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Far better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question, which can always be made precise.&lt;/p&gt;
&lt;p&gt;John Tukey&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The connection to order of magnitude estimates should be quite clear here. If we’re on an order of magnitude correct on the right questions, we can always refine the answer further. If we’re precisely answering the wrong question, God help us.&lt;/p&gt;
&lt;p&gt;What does this mean for a data scientist? For one, it means that means approximate methods are usually good enough practically to get ourselves into the right ballpark; we can use pragmatic considerations to decide whether we need a more complicated model or not. It also means that when we’re building data pipelines, minimum viable products, which help us test whether we’re answering the right question, matter more than the fanciest deep learning model.&lt;/p&gt;
&lt;p&gt;So yes, to mash those two quotes together:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Order of magnitude estimates on the right question are more useful than precise quantifications on the wrong question.&lt;/p&gt;
&lt;p&gt;Mashup&lt;/p&gt;
&lt;/blockquote&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/7/7/scipy-2019-pre-conference/">
    <title type="text">SciPy 2019 Pre-Conference</title>
    <id>urn:uuid:148bd441-7938-36c3-ab2c-b643de40f34c</id>
    <updated>2019-07-07T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/7/7/scipy-2019-pre-conference/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;For the 6th year running, I’m out at UT Austin for &lt;a href=&quot;https://www.scipy2019.scipy.org/&quot;&gt;SciPy 2019&lt;/a&gt;! It’s one of my favorite conferences to attend, because the latest in data science tooling is well featured in the conference program, and I get to meet in-person a lot of the GitHub usernames that I interact with online.&lt;/p&gt;
&lt;p&gt;I will be involved in three tutorials this year, which I think have become my data science toolkit: Bayesian stats, network science, and deep learning. Really excited to share my knowledge; my hope is that at least a few more people find the practical experience I’ve gained over the years useful, and that they can put it to good use in their own work too. This year is also the first year I’ve submitted a talk on &lt;code&gt;pyjanitor&lt;/code&gt;, which is a package that I have developed with others for cleaning data, also excited to share this with the broader SciPy community!&lt;/p&gt;
&lt;p&gt;I’m also looking forward to meeting the conference scholarship recipients. Together with Scott Collis and Celia Cintas, we’ve been managing the FinAid process for the past three years, and each year it heartens me to see the scholarship recipients in person.&lt;/p&gt;
&lt;p&gt;Finally, this year’s SciPy is quite unique for me, as it is the first year that I’ll be here with colleagues at work! (In prior years, I came alone, and did networking on my own.) I hope they all have as much of a fun time as I have at SciPy!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/6/30/bone-marrow-donations/">
    <title type="text">Bone Marrow Donations</title>
    <id>urn:uuid:bc446949-9164-3bef-8203-3cbff6478270</id>
    <updated>2019-06-30T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/6/30/bone-marrow-donations/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;A friend of mine just reached out to me, saying that he’s been diagnosed with leukemia. Thankfully, he’s not subject to the abysmal state of US healthcare (as he lives in a place where healthcare coverage is great), and so he’s on treatment, progressing, and hopefully has a great shot at beating this cancer.&lt;/p&gt;
&lt;p&gt;He definitely knows how to speak to a data scientist: using data. The odds of a match for a patient who needs a bone marrow transplant are 500:1. That means on average, only about 1 donor in 500 will be a match. On the other hand, under certain assumptions, every 500 donors who registers will mean one life, on average, can be saved. I did some digging myself: According to the &lt;a href=&quot;https://bloodcell.transplant.hrsa.gov/research/registry_donor_data/index.html&quot;&gt;US Health Resources and Services Administration&lt;/a&gt;, nearly every single minority ethnic group is underrepresented in donor registry databases.&lt;/p&gt;
&lt;p&gt;As things turn out, signing up to be a donor is quite lightweight. Genetic information - specifically, only Human Leukocyte Antigen (HLA) type - is needed, and that can be obtained in a non-invasive fashion. If a match is found, the donor still has the option to withdraw if they have any objections. As such, the process is completely voluntary for the donor. There are two types of donations possible: peripheral blood stem cells (PBSC) and bone marrow, with PBSC donations being lightweight and bone marrow donations being more involved. Digging a bit deeper, it seems like the only sacrifice a donor has to make is that of time and some discomfort.&lt;/p&gt;
&lt;p&gt;I’m putting this blog post up as a reminder to myself to register, and to encourage others to do so as well. If you’re in the United States, &lt;a href=&quot;https://bethematch.org/&quot;&gt;Be The Match&lt;/a&gt; is the organization to get in touch with; if you’re from my home country of Canada, the &lt;a href=&quot;https://blood.ca/en&quot;&gt;Canadian Blood Services&lt;/a&gt; manages the process.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/6/15/graphs-and-matrices/">
    <title type="text">Graphs and Matrices</title>
    <id>urn:uuid:e41b8fff-0a7a-350c-acb8-e445f4b2ad7c</id>
    <updated>2019-06-15T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/6/15/graphs-and-matrices/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;Once again, I’m reminded through my research how neat and useful it is to be able to think of matrices as graphs and vice-versa.&lt;/p&gt;
&lt;p&gt;I was constructing a symmetric square matrix of values, in which multiple cells of the matrix were empty (i.e. no values present). (Thankfully, the diagonal is guaranteed dense.) From this matrix, I wanted the largest set of rows/columns that formed a symmetric, densely populated square matrix of values, subject to a second constraint that the set of rows/columns also maximally intersected with another set of items.&lt;/p&gt;
&lt;p&gt;Having thought about the requirements of the problem, my prior experience with graphs reminded me that every graph has a corresponding adjacency matrix, and that finding the densest symmetric subset of entries in the matrix was equivalent to finding cliques in a graph! My intern and I proceeded to convert the matrix into its graph representation, and a few API calls in &lt;code&gt;networkx&lt;/code&gt; later, we found the matrix we needed.&lt;/p&gt;
&lt;p&gt;The key takeaway from this experience? Finding the right representation for a problem, we can computationally solve them quickly by using the appropriate APIs!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/6/14/mobile-working-on-the-ipad/">
    <title type="text">Mobile Working on the iPad</title>
    <id>urn:uuid:5fa1e5d1-cef0-3174-8a67-dfd804b2e17e</id>
    <updated>2019-06-14T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/6/14/mobile-working-on-the-ipad/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;A few years ago, I test-drove mobile work using my thesis as a case study, basically challenging myself with the question: how much of my main thesis paper could I write on iOS (specifically, an iPad Mini)? Back then, iOS turned out to be a superb tool for the writing phase (getting ideas into a text editor), and a horrible one for final formatting before submitting a paper to a journal (inflexible). Now, don’t get me wrong, though - I would still use it as part of my workflow if I were to do it again!&lt;/p&gt;
&lt;p&gt;Fast-forward a few years, I now do more programming in a data science context than I do formal writing, and the tooling for software development and data analysis on iOS has improved greatly. I thought I’d challenge myself with an experiment: how much of development and analytics could I do on an iPad, especially the Mini?&lt;/p&gt;
&lt;p&gt;This time round, armed with an iPad Pro (11&quot;), I decided to test again how much one can do on iOS, once again.&lt;/p&gt;
&lt;h2 id=&quot;software-development&quot;&gt;Software Development&lt;/h2&gt;&lt;p&gt;I develop &lt;a href=&quot;https://pyjanitor.readthedocs.io&quot;&gt;&lt;code&gt;pyjanitor&lt;/code&gt;&lt;/a&gt; as a tool that I use in my daily work, and as part of my open source software portfolio. When I’m on my MacBook or Pro, or on my Linux desktop at home, I usually work with VSCode for it’s integrated terminal, superb syntax highlighting, git integration, code completion with Kite, and more.&lt;/p&gt;
&lt;p&gt;Moving to iOS, VSCode is not available, and that immediately means to rely on Terminal-based tools to get my work done. I ponied up for &lt;a href=&quot;https://www.blink.sh/&quot;&gt;Blink Shell&lt;/a&gt;, and found it to pay off immediately. Having enabled remote access on my Linux tower at home, I was thrilled to learn that Blink supports &lt;a href=&quot;https://mosh.org/&quot;&gt;&lt;code&gt;mosh&lt;/code&gt;&lt;/a&gt;, and when paired with &lt;a href=&quot;https://github.com/tmux/tmux&quot;&gt;&lt;code&gt;tmux&lt;/code&gt;&lt;/a&gt;, it is a superb solution for maintaining persistent shells across spotty internet conditions.&lt;/p&gt;
&lt;p&gt;A while ago, I also configured &lt;code&gt;nano&lt;/code&gt; with syntax highlighting. As things turned out, syntax highlighting has the biggest effect on my productivity compared to other text editor enhancements (e.g. code completion, git integration, etc.). After I mastered most of &lt;code&gt;nano&lt;/code&gt;'s shortcut keys, I found I could be productive at coding in just &lt;code&gt;nano&lt;/code&gt; itself. Even though missing out on the usual assistive tools meant I was coding somewhat slower, the pace was still acceptable; moreover, relying less on those tools helped me develop a muscle memory for certain API calls. I also found myself becoming more effective because the single window idioms of iOS meant I was focusing on the programming task at hand, rather than getting distracted while looking at docs in a web browser (a surprisingly common happening for me!).&lt;/p&gt;
&lt;h2 id=&quot;data-analysis&quot;&gt;Data Analysis&lt;/h2&gt;&lt;p&gt;For data analysis, Jupyter notebooks are the tool of my choice, for their interactive nature, and the ability to weave a narrative throughout the computation. Jupyter Lab is awesome for this task, but it’s poorly supported on mobile Safari. To use Jupyter notebooks in iOS, the best offering at the moment is &lt;a href=&quot;https://juno.sh&quot;&gt;Juno&lt;/a&gt;, with its ability to connect to a Jupyter server accessible through an IP address or URL. This does require payment, though, and I gladly ponied up for that as well.&lt;/p&gt;
&lt;p&gt;I run a Jupyter server on my Linux tower at home. Because it has a GPU installed on it, when I am accessing the machine through Juno, my iPad suddenly has access to a full-fledged, fully-configured GPU as part of the compute environment! Coupled with the responsiveness of Juno, this makes for a fairly compelling setup to do Python programming on an iPad.&lt;/p&gt;
&lt;h2 id=&quot;pros-and-cons-of-ipad-based-development&quot;&gt;Pros and Cons of iPad-based Development&lt;/h2&gt;&lt;h3 id=&quot;cons&quot;&gt;Cons&lt;/h3&gt;&lt;p&gt;Overall, the experience has been positive, but there have been some challenges, which I would like to detail here.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Remote server required:&lt;/strong&gt; Firstly, because we are essentially using the iPad as a thin client to a remote server, one must either pay for a remote development server in the cloud, or go through the hassle of setting up a development machine that one can SSH into. This may turn off individuals who either are loathe to rent a computer, or don’t have the necessary experience to setup a remote server on their own.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;iOS Multi-Windowing:&lt;/strong&gt; It’s sometimes non-trivial to check up source code or function signatures (API docs, really) on the sidebar browser window in iOS. Unlike macOS, in which I have a number of shortcut keys that will let me launch and/or switch between apps, the lack of this capability on iOS means I find myself slowed down because I have to use a bunch of swiping gestures to get to where I need to be. (&lt;code&gt;Cmd-tab&lt;/code&gt; seems to be the only exception, for it activates the app switcher, but the number of apps in the app switcher remembers is limited.)&lt;/p&gt;
&lt;h3 id=&quot;pros&quot;&gt;Pros&lt;/h3&gt;&lt;p&gt;Even with the issues detailed above, there’s still much to love about doing mobile development work on an iOS device.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;iOS Speed:&lt;/strong&gt; On the latest hardware, iOS is speedy. Well, even that is a bit of an understatement. I rarely get lags while typing in Blink and Juno, and even when I do, I can usually pin it down to network latency more than RAM issues.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Focus:&lt;/strong&gt; This is the biggest win. Because iOS makes it difficult to switch contexts, this is actually an upside for work that involves creating things. Whether it’s someone who is drawing, producing videos, editing photos, writing blog posts, or creating code, the ability to focus on the context at hand is tremendous for producing high quality work. Here, iOS is actually a winner for focused productivity.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Mobility:&lt;/strong&gt; The second upside would be that of battery life, and hence mobility, by extension. My 12&quot; MacBook is super mobile, yes, but macOS appears to have issues restraining battery drainage when the lid is closed. By contrast, iOS seems to have fewer issues with this. The battery life concerns mean I’m carrying my mouse, charger and dongle with me all the time, and I’ll get the equivalent of range anxiety when I take only my laptop.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Keyboard Experience:&lt;/strong&gt; The keyboard experience on the Smart Keyboard Folio is surprisingly good! It’s tactile, and is fully covered, so we won’t have issues arise due to dust getting underneath the keys, like my little MacBook had.&lt;/p&gt;
&lt;h2 id=&quot;concluding-thoughts&quot;&gt;Concluding Thoughts&lt;/h2&gt;&lt;p&gt;This test has been quite instructive. As usual, tooling is superbly important for productivity; investing in the right tools makes it worthwhile. Granted, none of this comes cheap or for free, naturally. Given the future directions of iOS, I think it’s shaping up to be a real contender for productivity!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/5/29/reasoning-about-shapes-and-probability-distributions/">
    <title type="text">Reasoning about Shapes and Probability Distributions</title>
    <id>urn:uuid:7d251bbe-af45-3bb0-8b08-346e8011341e</id>
    <updated>2019-05-29T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/5/29/reasoning-about-shapes-and-probability-distributions/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;I’m here with the PyMC4 dev team and Tensorflow Probability developers Rif, Brian and Chris in Google Montreal, and have found the time thus far to be an amazing learning opportunity.&lt;/p&gt;
&lt;p&gt;Prior to this summit, it never dawned on me how interfacing tensors with probability distributions could be such a minefield of overloaded ideas and terminology. Yet, communicating clearly about tensors is important, because if problems can be cast into a tensor-space operation, vectorization can help speed up many operations that we wish to handle. I wanted to share a bit about something new about tensors that I learned here: the different types of shapes involved in a probabilistic programming language.&lt;/p&gt;
&lt;p&gt;Let’s start by thinking about a few questions involving the most venerable distribution of them all: the Gaussian, also known as the Normal distribution.&lt;/p&gt;
&lt;p&gt;Let’s start by thinking about a single draw from a standard Gaussian. Drawing one number from the standard Gaussian yields a scalar. In tensor space, a scalar is a rank 0 tensor, and this colloquially means that there’s no dimensions involved. If we drew out the distribution, and drew out the process of drawing numbers from the distribution, it might look like the following:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;../../../../../blog/2019/5/29/reasoning-about-shapes-and-probability-distributions/one-draw-one-normal.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The distribution we are drawing from is on the left, and a draw is represented by a line (on the same numerical axis as the probability distribution), and the &lt;/em&gt;event shape&lt;em&gt;, &lt;/em&gt;batch shape&lt;em&gt; and &lt;/em&gt;sample shape&lt;em&gt; shown to their right, followed by a &quot;plain English&quot; description. Over the course of this blog post, the shape concepts will be disambiguated; sit tight and enjoy the ride!&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;What if we were to draw two numbers from this one Gaussian? We could use a vector with two slots to represent those draws. This might look like the following:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;../../../../../blog/2019/5/29/reasoning-about-shapes-and-probability-distributions/two-draws-one-normal.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;However, the elementary &lt;em&gt;event&lt;/em&gt; of drawing a single number did not fundamentally change when we drew two numbers, as we merely repeated the same &lt;em&gt;event&lt;/em&gt; to draw two. With my hands waving in the air, I will claim that this holds true even with K &lt;em&gt;samples&lt;/em&gt; drawn from the distribution.&lt;/p&gt;
&lt;p&gt;Now, what if I had a second Gaussian, say, with a different mean and/or variance? If I were to draw one number from the first Gaussian alongside one number from the second Gaussian, and then concatenate them into a vector, we can represent this as us drawing numbers from independent Gaussians. The illustration below should help clarify how this is different from the first.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;../../../../../blog/2019/5/29/reasoning-about-shapes-and-probability-distributions/one-draw-two-normals.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;In this case, we may argue that per distribution, the elementary shape of the &lt;em&gt;event&lt;/em&gt; did not change. However, since we have a &lt;em&gt;batch&lt;/em&gt; of two distributions, this contributes to the final shape of the tensor. Again, with much waving of my hands in the air, this should extend to more than two distributions.&lt;/p&gt;
&lt;p&gt;Now, what if we had a multivariate Gaussian, with two variates? This makes for a very interesting case! The elementary &lt;em&gt;event&lt;/em&gt; drawn from this multivariate Gaussian is a two-element vector, not a scalar, which means that its shape is apparently identical to the case where we have a single pair of numbers drawn from a batch of two independent Gaussians! This looks like the following:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;../../../../../blog/2019/5/29/reasoning-about-shapes-and-probability-distributions/one-draw-bivariate-normal.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;This is interesting, because a single draw from a bivariate Gaussian has the same overall shape as two draws from one Gaussian, which also has the same shape as one draw from a batch of two Gaussians. Yet, these apparently same-shaped draws are shaped differently semantically! In particular, the two independent Gaussians individually have elementary &lt;em&gt;event&lt;/em&gt; shapes that are scalar, but when drawn as a &lt;em&gt;batch&lt;/em&gt; of two, that is when their shape of &lt;code&gt;(2,)&lt;/code&gt; forms. On the other hand, the multivariate Gaussian cannot have its two numbers drawn independent of one another (unless this is the special case of diagonal-only covariance - in which case, this is equivalent to independent Gaussians). Hence, the elementary &lt;em&gt;event&lt;/em&gt; shape is not scalar, but vector (or more generally, same rank tensor as the mean tensor), but the &lt;em&gt;batch&lt;/em&gt; has only a single distribution, hence it has a scalar batch shape.&lt;/p&gt;
&lt;p&gt;To summarize, here are the various kinds of shapes, defined:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Event shape:&lt;/strong&gt; The atomic shape of a single event/observation from the distribution (or batch of distributions of the same family).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Batch shape:&lt;/strong&gt; The atomic shape of a single sample of observations from one or more distributions &lt;em&gt;of the same family&lt;/em&gt;. As an example, we can’t have a batch of a Gaussian and a Gamma distribution together, but we can have a batch of more than one Gaussians.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sample shape:&lt;/strong&gt; The shape of a bunch of samples drawn from the distributions.&lt;/p&gt;
&lt;p&gt;And finally, here’s the full spread of possibilities, using one or two draws, uni- or bi-variate Gaussians, and one or two batches of distributions as an illustration.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;../../../../../blog/2019/5/29/reasoning-about-shapes-and-probability-distributions/shapes-blog-post-md.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;Special thanks goes to fellow PyMC devs, Ravin Kumar, Brandon Willard, Colin Carroll, and Peadar Coyle, who provided feedback on the figure over a late-night tea/dinner/bar session at the end of Day 2.&lt;/p&gt;
&lt;h2 id=&quot;why-shapes-matter:-broadcasting&quot;&gt;Why Shapes Matter: Broadcasting&lt;/h2&gt;&lt;p&gt;Why do these different shapes matter? Well, it matters most when we are thinking about broadcasting in a semantically-consistent fashion, particularly when considering batches and events. When it comes to implementing a tensor library with probability distributions as first-class citizens, reasoning about these shapes properly can really help with implementing an API that end-users can grok in a reasonable fashion.&lt;/p&gt;
&lt;p&gt;Let’s return to the simple case where we have two different types of shape &lt;code&gt;(2,)&lt;/code&gt; Gaussians declared: a batch of two Gaussians, and a bivariate Gaussian. One useful thought experiment is to think about the computation of the log probability of a vector of two numbers, &lt;code&gt;X = (x1, x2)&lt;/code&gt;, where &lt;code&gt;x1&lt;/code&gt; and &lt;code&gt;x2&lt;/code&gt; are not necessarily the same numbers.&lt;/p&gt;
&lt;p&gt;In the case of the bivariate Gaussians, how many log probabilities should we return? In this case, it makes semantic sense to return only one number, because in a bivariate Gaussian, the two numbers could not have been drawn independent of each other, and hence the log probability has to be computed with consideration to the full joint distribution.&lt;/p&gt;
&lt;p&gt;In the case of the batch of two Gaussians, how many log probabilities should we return? Is it one number, or is it two? Semantically, it makes sense to return two numbers, because we are evaluating &lt;code&gt;x1&lt;/code&gt; against the first Gaussian, and &lt;code&gt;x2&lt;/code&gt; against the second Gaussian in the batch of Gaussians. Most crucially, this differs from the bivariate case, because by structuring our Gaussians in a batch, we are essentially declaring our intent to evaluate their log probabilities independent of each other.&lt;/p&gt;
&lt;p&gt;Borrowing from the above master figure, here's the a figure that shows how likelihood computations happen:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;../../../../../blog/2019/5/29/reasoning-about-shapes-and-probability-distributions/likelihood-computation.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;To compute the likelihood over the data, we multiply the likelihoods of each of the individual data points (or since we're doing computation on a computer, we sum the log probabilities). You will noticed that essentially, in each case, the dimension we intend to collapse is the &lt;code&gt;sample&lt;/code&gt; dimension - and that means keeping track of the sample dimension is extremely important! Also important to note is that we do &lt;/em&gt;not&lt;em&gt; intend to collapse the &lt;code&gt;batch&lt;/code&gt; dimension, as it does not carry the same statistical meaning as a &lt;code&gt;sample&lt;/code&gt; from a distribution, but is a tensor computation construct.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&quot;other-scenarios&quot;&gt;Other Scenarios&lt;/h2&gt;&lt;p&gt;There are more scenarios where reasoning about shapes in a &lt;em&gt;semantic&lt;/em&gt; manner becomes super important! Here’s a sampling of them, posed as questions and then maybe some suggested answers or further questions.&lt;/p&gt;
&lt;p&gt;If I now asked to evaluate the log probability of &lt;code&gt;x1&lt;/code&gt; only, how should broadcasting of &lt;code&gt;x1&lt;/code&gt; happen on the bivariate Gaussian, and on the batch of two Gaussians? Perhaps in this trivial case, it would be tempting to automatically broadcast the same scalar number... but wait! In the case of the bivariate Gaussian, how do we know that the end-user has not forgotten to supply the second number?&lt;/p&gt;
&lt;p&gt;If I have a batch of two bivariate Gaussians, hence effectively creating a (batch shape = 2, event shape = 2) vector of bivariate distributions, and I ask to evaluate the log probability of a matrix of values &lt;code&gt;((x1, x2), (x3, x4))&lt;/code&gt;, in which way do we orient the values? Do we assume that &lt;code&gt;(x1, x2)&lt;/code&gt; are to be evaluated against the first Gaussian, or &lt;code&gt;(x1, x3)&lt;/code&gt; are to be evaluated against the first Gaussian? (We don’t have to worry about &lt;code&gt;(x1, x4)&lt;/code&gt;, because to the average user, it is unreasonable whichever way we look.)&lt;/p&gt;
&lt;p&gt;Both these examples illustrate an inherent difficulty to thinking about tensor shapes without reference to what each of the dimensions mean.&lt;/p&gt;
&lt;h2 id=&quot;improving-shape-semantics&quot;&gt;Improving Shape Semantics&lt;/h2&gt;&lt;p&gt;What could we do, then, to improve the semantic understandability of tensor shapes?&lt;/p&gt;
&lt;p&gt;One solution would be to name tensor axes by what they mean. The &lt;code&gt;xarray&lt;/code&gt; project does exactly that! However, it can’t be used in differential computing, because (to the best of my knowledge), there is no automatic differentiation system that works with it.&lt;/p&gt;
&lt;p&gt;In addition, there’s &lt;code&gt;namedtensor&lt;/code&gt; from Harvard NLP that aims to provide an analogous solution to the problem, though I find it a pity that they chose to implement it against &lt;code&gt;pytorch&lt;/code&gt; rather than to create an extension to the idiomatic &lt;code&gt;numpy&lt;/code&gt; instead.&lt;/p&gt;
&lt;p&gt;The TensorFlow Probability team also has a solution, in which they separate the three types of shapes explicitly, though no naming happens on a per-axis basis.&lt;/p&gt;
&lt;p&gt;I think there are great ideas in all three, and when I take a birds-eye view of the scientific computing ecosystem in Python as both a developer and end-user, I’d love to see the NumPy API, which is idiomatic and widely used and built on top of, become aware of each of these types of designs, something akin to NEP-18, the array function dispatching protocol that allows NumPy APIs to be called on other tensor libraries.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/5/11/pycon-2019-sprints/">
    <title type="text">PyCon 2019 Sprints</title>
    <id>urn:uuid:a42b3f69-a28c-3ea5-945b-64486982c78e</id>
    <updated>2019-05-11T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/5/11/pycon-2019-sprints/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;This year was the first year that I decided to lead a sprint! The sprint I led was for &lt;code&gt;pyjanitor&lt;/code&gt;, a package that I developed with my colleague, Zach Barry, and a remote collaborator in NYC, Sam Zuckerman (whom I've never met in person!). This being the first sprint I've ever led, I think I was lucky to stumble upon a few ideas that made for a productive, collaborative, and most importantly, fun sprint.&lt;/p&gt;
&lt;h2 id=&quot;pyjanitor?&quot;&gt;pyjanitor?&lt;/h2&gt;&lt;p&gt;I'm going to deliver a talk on &lt;code&gt;pyjanitor&lt;/code&gt; later in the year, so I'll save the details for that talk. The short description of &lt;code&gt;pyjanitor&lt;/code&gt; is that if you have &lt;code&gt;pandas&lt;/code&gt; one-liners that are difficult to remember, they should become a function in &lt;code&gt;pyjanitor&lt;/code&gt;; if you have a 10-liner that you always copy/paste from another source, they should become a function in &lt;code&gt;pyjanitor&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&quot;sprints?&quot;&gt;Sprints?&lt;/h2&gt;&lt;p&gt;Code sprints are a part of PyCon, and it's basically one to four days of intense and focused software development on a single project.&lt;/p&gt;
&lt;p&gt;Project sprint leads first pitch their projects at the &quot;&lt;em&gt;Sprintros&lt;/em&gt;&quot;, where they indicate what days they will be sprinting, and at what times. The next day, we indicate on which rooms our projects will be sprinting. Volunteers from the conference, who would like to make an open source project contribution, then identify projects they would like to come sprint with.&lt;/p&gt;
&lt;p&gt;In some senses, there's no way for a sprint leader to know how popular their sprint will be &lt;em&gt;a priori&lt;/em&gt;. We have to be prepared to handle a range of scenarios from sprinting with just one other person to sprinting with a crowd.&lt;/p&gt;
&lt;h2 id=&quot;structure&quot;&gt;Structure&lt;/h2&gt;&lt;h3 id=&quot;preparation&quot;&gt;Preparation&lt;/h3&gt;&lt;p&gt;In preparation for the sprint, I absorbed many lessons learned over the years of sprinting on others' projects.&lt;/p&gt;
&lt;p&gt;The most obvious one was to ensure that every sprinter had something to do right from the get-go. Having a task from the get-go keeps sprinters, especially newcomers, engaged right from the beginning. This motivated the requirement to make a doc fix before making a code fix. (You can read more below on how we made this happen.) I wrote out this requirement in a number of places, and by the time the sprint day rolled by, this rolled off pretty naturally.&lt;/p&gt;
&lt;p&gt;The second thing that I did to prep was to triage existing issues and label them as being beginner- or intermediate-friendly, and whether they were doc, infrastructure, or code contributions.&lt;/p&gt;
&lt;p&gt;Those two things were the my highest priority preparation for the sprint, and I think that helped a ton.&lt;/p&gt;
&lt;h3 id=&quot;doc-fixes&quot;&gt;Doc Fixes&lt;/h3&gt;&lt;p&gt;This sprint, I gave the structure some thought, and settled on the following: Before making a code contribution, I required a docs contribution.&lt;/p&gt;
&lt;p&gt;Docs contributions could be of any scale:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A typographical, grammatical, or spelling error.&lt;/li&gt;
&lt;li&gt;A docstring that was unclear.&lt;/li&gt;
&lt;li&gt;Installation/setup instructions that are unclear.&lt;/li&gt;
&lt;li&gt;A sentence/phrase/word choice that didn't make sense.&lt;/li&gt;
&lt;li&gt;New example/tutorial notebooks using the library.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I think this worked well for the following reasons:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;New contributors must read the docs before developing on the project, and hence become familiar with the project.&lt;/li&gt;
&lt;li&gt;There's always something that can be done better in the docs, and hence, there is something that can be immediately acted on.&lt;/li&gt;
&lt;li&gt;The task is a pain point personally uncovered by the contributor, and hence the contributor has the full context of the problem. &lt;/li&gt;
&lt;li&gt;The docs don't break the code/tests, and hence doc contributions are a great way make a contribution without wrestling with more complex testing.&lt;/li&gt;
&lt;li&gt;Starting &lt;em&gt;everybody&lt;/em&gt; who has never worked on &lt;code&gt;pyjanitor&lt;/code&gt; on docs is an egalitarian way of on-boarding every newcomer, beginner and experienced individuals alike. Nobody gets special treatment.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For each individual's contribution, I asked them to first raise an issue on the GitHub issue tracker describing the contribution that they would like to make, and then clearly indicate in the comments that they would like to work on it. Then, they would go through the process of doing the documentation fix, from forking the repository, cloning it locally, creating a new branch, making edits, committing, pushing, and PR-ing.&lt;/p&gt;
&lt;p&gt;If two people accidentally ended up working on the same docs issue, I would assess the effort of the later one, and if it was substantial enough, I would allow them to consider it done, and move onto a different issue.&lt;/p&gt;
&lt;p&gt;Going forth, as the group of contributors expands, I will enforce this &quot;docs-first&quot; requirement only for newcomer sprinters, and request experienced ones to help manage the process.&lt;/p&gt;
&lt;h3 id=&quot;code-contributions&quot;&gt;Code Contributions&lt;/h3&gt;&lt;p&gt;Once the docs contributions were done, sprinters were free to either continue with more docs contributions, or provide a code contribution.&lt;/p&gt;
&lt;p&gt;Code contributions could be of one of the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;New function contributions.&lt;/li&gt;
&lt;li&gt;Cleaner implementations of existing functions.&lt;/li&gt;
&lt;li&gt;Restructuring of existing functions.&lt;/li&gt;
&lt;li&gt;Identification of functions to deprecate (very important!)&lt;/li&gt;
&lt;li&gt;Infrastructural changes to docs, build system, and more.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The process for doing this was identical to docs: raise an issue, claim it, and then make a new branch with edits, and finally PR it.&lt;/p&gt;
&lt;h2 id=&quot;my-role&quot;&gt;My Role&lt;/h2&gt;&lt;p&gt;For both days, we had more than 10 people sprint on &lt;code&gt;pyjanitor&lt;/code&gt;. Many who were present on the 1st day (and didn't have a flight to catch) came back on the 2nd day. As such, I actually didn't get much coding done. Instead, I took on the following roles:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Q&amp;amp;A person: Answering questions about what would be acceptable contributions, technical (read: git) issues, and more.&lt;/li&gt;
&lt;li&gt;Issue labeller and triage-r: I spent the bulk of my downtime (and pre-/post-sprint time) tagging issues on the GitHub issue tracker and marking them as being available or unavailable for hacking on, and tagging them with whether they were docs-related, infrastructure-related, or code enhancements. &lt;/li&gt;
&lt;li&gt;Code reviewer: As PRs came in, I would conduct code reviews on each of them, and would discuss with them where to adjust the code to adhere to code style standards. &lt;/li&gt;
&lt;li&gt;Continuous integration pipeline babysitter: Because I had just switched us off from Travis CI to Azure Pipelines, I was babysitting the pipelines to make sure nothing went wrong. (Spoiler: something did!) &lt;/li&gt;
&lt;li&gt;Green button pusher: Once all tests passed, I would hit the big green button to merge PRs!&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If I get to sprint with other experienced contributors at the sprints, I would definitely like to have some help with the above.&lt;/p&gt;
&lt;h2 id=&quot;thoughts&quot;&gt;Thoughts&lt;/h2&gt;&lt;h3 id=&quot;making-sprints-human-friendly&quot;&gt;Making sprints human-friendly&lt;/h3&gt;&lt;p&gt;I tried out a few ideas, which I hope made the sprints just that little bit more human-friendly.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Used large Post-It easel pads to write out commonly-used commands at the terminal.&lt;/li&gt;
&lt;li&gt;Displayed claimed seating arrangements at the morning, and more importantly, get to know every sprinter's name.&lt;/li&gt;
&lt;li&gt;Announcing every PR to the group that was merged and what the content was, followed by a round of applause.&lt;/li&gt;
&lt;li&gt;Setting a timer for 5 minutes before lunch so that they could all get ahead in the line.&lt;/li&gt;
&lt;li&gt;I used staff privileges to move the box of leftover bananas into our sprint room. :)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I think the applause is the most encouraging part of the process. Having struggled through a PR, however big or small, and having group recognition for that effort, is super encouraging, especially for first-time contributors. I think we need to encourage this more at sprints.&lt;/p&gt;
&lt;h3 id=&quot;relinquishing-control&quot;&gt;Relinquishing control&lt;/h3&gt;&lt;p&gt;The only things about the pyjanitor project that I'm unwilling to give up on are: good documentation of what a function does, that a function should do one thing well, and that it be method-chainable. Everything else, including functionality, is an open discussion that we can have!&lt;/p&gt;
&lt;p&gt;One PR I particularly enjoyed was that from Lucas, who PR'd in a logo for the project on the docs page. He had the idea to take the hacky broomstick I drew on the big sticky note (as a makeshift logo), redraw it as a vector graphic in Inkscape, and PR it in as the (current) official logo on the docs.&lt;/p&gt;
&lt;p&gt;More broadly, I deferred to the sprinters' opinions on docs, because I recognized that I'd have old eyes on the docs, and wouldn't be able to easily identify places where the docs could be written more clearly. Eventually, a small, self-organizing squad of 3-5 sprinters ended up becoming the unofficial docs squad, rearranging the structure of the docs, building automation around it, and better organizing and summarizing the information on the docs.&lt;/p&gt;
&lt;p&gt;In more than a few places, if there were a well-justified choice for the API (which really meant naming the functions and keyword arguments), I'd be more than happy to see the PR happen. Even if it is evolved away later, the present codebase and PRs that led to it provided the substrate for better evolution of the API!&lt;/p&gt;
&lt;h3 id=&quot;a-new-microsoft&quot;&gt;A new Microsoft&lt;/h3&gt;&lt;p&gt;This year, I switched from Travis CI to Azure Pipelines. In particular, I was attracted to the ability to build on all three major operating systems, Windows, macOS, and Linux, on the free tier.&lt;/p&gt;
&lt;p&gt;Microsoft had a booth at PyCon, in which Steve Dowell led an initiative to get us set up with Azure-related tools. Indeed, as a major sponsor of the conference, this was one of the best swag given to us. Super practical, relationship- and goodwill-building. Definitely not lesser than the Adafruit lunchboxes with electronics as swag!&lt;/p&gt;
&lt;h3 id=&quot;hiccups&quot;&gt;Hiccups&lt;/h3&gt;&lt;p&gt;Naturally, not everything was smooth sailing throughout. I did find myself a tad expressing myself in an irate fashion at times with the amount of context switching that I was doing, especially switching between talking to different sprinters one after another. (I am very used to long stretches hacking on one problem.) One thing future sprinters could help with, which I will document, is to give me enough ramp-up context around their problem, so that I can quickly pinpoint what other information I might need.&lt;/p&gt;
&lt;p&gt;The other not-so-smooth-sailing thing was finding out that Azure sometimes did not catch errors in a script block! My unproven hypothesis at this point is that if I have four commands executed in a script block, and if any of the first three fail but the last one passes, the entire script block will behave as if it passes. This probably stems from the build system looking at only the last exit code to determine exit status. Eventually, after splitting each check into individual steps, linting and testing errors started getting caught automatically! (Automating this is much preferred to me running the &lt;code&gt;black&lt;/code&gt; code formatter in my head.)&lt;/p&gt;
&lt;p&gt;Though the above issue is fixed, I think I am still having issues getting &lt;code&gt;pycodestyle&lt;/code&gt; and &lt;code&gt;black&lt;/code&gt; to work on the Windows builds. Definitely looking forward to hearing from Azure devs what could be done here!&lt;/p&gt;
&lt;h2 id=&quot;suggestions&quot;&gt;Suggestions&lt;/h2&gt;&lt;p&gt;I'm sure there's ways I could have made the sprint a bit better.  I'd love to hear them if there's something I've missed! Please feel free to comment below.&lt;/p&gt;
&lt;h2 id=&quot;sprinter-acknowledgement&quot;&gt;Sprinter Acknowledgement&lt;/h2&gt;&lt;p&gt;I would like to thank all the sprinters who joined in this sprint. Their GitHub handles are below:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;@HectorM14 (who was remote!)&lt;/li&gt;
&lt;li&gt;@jekwatt&lt;/li&gt;
&lt;li&gt;@kurtispinkney&lt;/li&gt;
&lt;li&gt;@lphk92&lt;/li&gt;
&lt;li&gt;@jonnybazookatone&lt;/li&gt;
&lt;li&gt;@SorenFrohlich&lt;/li&gt;
&lt;li&gt;@dave-frazzetto&lt;/li&gt;
&lt;li&gt;@dsouzadaniel&lt;/li&gt;
&lt;li&gt;@Eidhagen&lt;/li&gt;
&lt;li&gt;@mdini&lt;/li&gt;
&lt;li&gt;@kimt33&lt;/li&gt;
&lt;li&gt;@jack-kessler-88&lt;/li&gt;
&lt;li&gt;@NapsterInBlue&lt;/li&gt;
&lt;li&gt;@jk3587&lt;/li&gt;
&lt;li&gt;@ricky-lim&lt;/li&gt;
&lt;li&gt;@catherinedevlin&lt;/li&gt;
&lt;li&gt;@StephenSchroed&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And as always, big thanks to my collaborators on the repository:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;@zbarry&lt;/li&gt;
&lt;li&gt;@szuckerman&lt;/li&gt;
&lt;/ul&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/5/10/pycon-2019-tutorial-and-conference-days/">
    <title type="text">PyCon 2019 Tutorial and Conference Days</title>
    <id>urn:uuid:e540031b-e5df-3893-bf8f-bd692ef9b95b</id>
    <updated>2019-05-10T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/5/10/pycon-2019-tutorial-and-conference-days/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;It's just been days since I got back from PyCon, and I'm already looking forward to 2020! But I thought it'd be nice to continue the recap.&lt;/p&gt;
&lt;p&gt;This year at PyCon, I co-led two tutorials, one with Hugo Bowne-Anderson on &lt;a href=&quot;https://us.pycon.org/2019/schedule/presentation/77/&quot;&gt;Bayesian Data Science by Simulation&lt;/a&gt;, and the other with Mridul Seth on &lt;a href=&quot;https://us.pycon.org/2019/schedule/presentation/70/&quot;&gt;Network Analysis Made Simple&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I always enjoy teaching with &lt;a href=&quot;../../../../../blog/2019/5/10/pycon-2019-tutorial-and-conference-days/#&quot;&gt;Hugo&lt;/a&gt;. He brought his giant sense of humour, both figuratively and literally, to this tutorial, and melded it with his deep grasp of the math behind Bayesian statistics, delivering a workshop that, by many points of feedback, is excellent. Having reviewed the tutorial feedback, we've got many ideas for our showcase of Part II at SciPy 2019!&lt;/p&gt;
&lt;p&gt;This year was the first year that Mridul and I swapped roles. In previous years, he was my TA, helping tutorial participants while I did the main lecturing. This year, the apprentice became the master, and a really good one indeed! Looking forward to seeing him shine more in subsequent tutorial iterations.&lt;/p&gt;
&lt;p&gt;During the conference days, I spent most of my time either helping out with Financial Aid, or at the Microsoft booth. As I have alluded to in multiple tweets, Microsoft's swag this year was the best of them all. Microelectronics kits in a blue lunch box from Adafruit, and getting set up with &lt;a href=&quot;https://azure.microsoft.com/en-us/&quot;&gt;Azure&lt;/a&gt;. In fact, this website is now re-built with each push on Azure pipelines! Indeed, &lt;a href=&quot;https://twitter.com/zooba&quot;&gt;Steve Dowell&lt;/a&gt; from Microsoft told me that this year's best swag was probably getting setup with Azure, and I'm 100% onboard with that! (Fun fact, Steve told me that he's never been called by his Twitter handle (&lt;code&gt;zooba&lt;/code&gt;) in real-life... until we met.)&lt;/p&gt;
&lt;p&gt;I also delivered a talk this year, which essentially amounted to a &lt;a href=&quot;https://us.pycon.org/2019/schedule/presentation/174/&quot;&gt;formal rant against canned statistical procedures&lt;/a&gt;. I had a ton of fun delivering this talk. The usual nerves still get to me, and I had to do a lot of talking-to-myself-style rehearsals to work off those nerves. For the first time, I also did office hours post-talk at an Open Space, where for one hour, we talked about all things Bayes. Happy to have met everybody who came by; I genuinely enjoyed the chat!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/5/10/context-switching/">
    <title type="text">Context Switching</title>
    <id>urn:uuid:b323afb0-df8c-3aa2-afe8-379eff18252f</id>
    <updated>2019-05-10T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/5/10/context-switching/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;Context switching is hard. I noticed this when I was at the PyCon sprints, where I was bouncing from sprinter to sprinter, trying to give them each the necessary attention to get their chosen sprint tasks done. After a while, it took a toll on my brain, and I started finding it hard to concentrate on the next problem.&lt;/p&gt;
&lt;p&gt;Under such circumstances, when one is context switching often (most hopefully out of one's own volition), how do we communicate that we need some ramp-up time, and how can others help us help them?&lt;/p&gt;
&lt;p&gt;I think one practical thing that can be done is to frequently communicate on each context switch that context ramp-up is needed. In the future, when I switch contexts, first thing I'm going to ask is something along the lines of, &quot;What context do I need to help me help you?&quot; Or, if I'm lost, I can clearly communicate what I'm missing - if it's context that I'm missing - by stating, &quot;I think I'm missing some context. Can you bring me up to speed?&quot;&lt;/p&gt;
&lt;p&gt;At least while sprinting, sprinters can definitely help me help them by providing the necessary context up-front. Perhaps this applies more generally as well: when we're asking someone for help, we may be able to help them out by asking them, &quot;What context from me would help you get up-to-speed here?&quot;&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/4/29/pycon-2019-pre-journey/">
    <title type="text">PyCon 2019 Pre-Journey</title>
    <id>urn:uuid:ccbe0426-b1a5-3655-ab5f-4005fb02089d</id>
    <updated>2019-04-29T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/4/29/pycon-2019-pre-journey/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;I'm headed out to &lt;a href=&quot;https://us.pycon.org/2019&quot;&gt;PyCon 2019&lt;/a&gt;! This year, I will be co-instructing two tutorials, one on network analysis and one on Bayesian statistics, and delivering one talk on Bayesian statistics.&lt;/p&gt;
&lt;p&gt;The first tutorial on &lt;a href=&quot;https://us.pycon.org/2019/schedule/presentation/70/&quot;&gt;network analysis&lt;/a&gt; is based on &lt;a href=&quot;https://github.com/ericmjl/Network-Analysis-Made-Simple&quot;&gt;material that I first developed&lt;/a&gt; 5 years ago, and have continually updated. I've enjoyed teaching this tutorial because it represents a different way of thinking about data - in other words, relationally. This year, I will be a co-instructor for &lt;a href=&quot;https://twitter.com/Mridul_Seth&quot;&gt;Mridul&lt;/a&gt;, who has kindly agreed to step up and teach it this year at PyCon. The apprentice has exceeded the master!&lt;/p&gt;
&lt;p&gt;The second tutorial on &lt;a href=&quot;https://us.pycon.org/2019/schedule/presentation/77/&quot;&gt;Bayesian statistics&lt;/a&gt; is based on &lt;a href=&quot;https://github.com/ericmjl/bayesian-stats-modelling-tutorial&quot;&gt;material co-developed&lt;/a&gt; with &lt;a href=&quot;https://twitter.com/hugobowne&quot;&gt;Hugo Bowne-Anderson&lt;/a&gt;. Hugo is a mathematician by training, a pedagogy master, and data science aficionado. Like myself, he is a fan of Bayesian statistical modelling methods, and we first debuted the tutorial past year at SciPy. We're super excited for this one!&lt;/p&gt;
&lt;p&gt;The talk that I will deliver is on &lt;a href=&quot;https://us.pycon.org/2019/schedule/presentation/174/&quot;&gt;Bayesian statistical analysis of case/control tests&lt;/a&gt;. In particular, I noticed a content gap in the data science talks, where case/control comparisons were limited to one case and one control. One epiphany I came to was that if we use Bayesian methods to analyze our data, there's no particular reason to limit ourselves to one case and one control; we can flexibly model multiple cases vs. one control, or even multiple cases vs multiple different controls in the same analysis, in a fashion that is flexible and principled.&lt;/p&gt;
&lt;p&gt;My final involvement with PyCon this year is as Financial Aid Chair. This is the first year that I'm leading the FinAid effort; during previous years, I had learned a ton from the previous chair Karan Goel. My co-chairs this year are Denise Williams and Jigyasa Grover; I'm looking forward to meeting them in 3D!&lt;/p&gt;
&lt;p&gt;All-in-all, I'm looking forward to another fun year at PyCon!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/3/24/variance-explained/">
    <title type="text">Variance Explained</title>
    <id>urn:uuid:0732917e-8efd-3932-8fad-e75d4d69dd52</id>
    <updated>2019-03-24T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/3/24/variance-explained/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;Variance explained, as a regression quality metric, is one that I have begun to like a lot, especially when used in place of a metric like the correlation coefficient (r&lt;sup&gt;2&lt;/sup&gt;).&lt;/p&gt;
&lt;p&gt;Here's variance explained defined:&lt;/p&gt;
&lt;p&gt;$$1 - \frac{var(y&lt;em&gt;{true} - y&lt;/em&gt;{pred})}{var(y_{true})}$$&lt;/p&gt;
&lt;p&gt;Why do I like it? It’s because this metric gives us a measure of the scale of the error in predictions relative to the scale of the data.&lt;/p&gt;
&lt;p&gt;The numerator in the fraction calculates the variance in the errors, in other words, the &lt;em&gt;scale of the errors&lt;/em&gt;. The denominator in the fraction calculates the variance in the data, in other words, the &lt;em&gt;scale of the data&lt;/em&gt;. By subtracting the fraction from 1, we get a number upper-bounded at 1 (best case), and unbounded towards negative infinity.&lt;/p&gt;
&lt;p&gt;Here's a few interesting scenarios.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If the scale of the errors is small relative to the scale of the data, then variance explained will be close to 1.&lt;/li&gt;
&lt;li&gt;If the scale of the errors is about the same scale as the data, then the variance explained will be around 0. This essentially says our model is garbage.&lt;/li&gt;
&lt;li&gt;If the scale of the errors is greater than the scale of the data, then the variance explained will be negative. This is also an indication of a garbage model.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A thing that is &lt;em&gt;really nice&lt;/em&gt; about variance explained is that it can be used to compare related machine learning tasks that have different unit scales, for which we want to compare how good one model performs across all of the tasks. Mean squared error makes this an apples-to-oranges comparison, because the unit scales of each machine learning task is different. On the other hand, variance explained is unit-less.&lt;/p&gt;
&lt;p&gt;Now, we know that single metrics can have failure points, as does the coefficient of correlation r^2^, as shown in Ansecombe's quartet and the Datasaurus Dozen:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://d2f99xq7vri1nk.cloudfront.net/Anscombe_1_0_0.png&quot; alt=&quot;Ansecombe&amp;#39;s quartet, taken from Autodesk Research&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Fig. 1: Ansecombe's quartet, taken from Autodesk Research&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://blog.revolutionanalytics.com/downloads/DataSaurus%20Dozen.gif&quot; alt=&quot;Datasaurus Dozen, taken from Revolution Analytics&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Fig. 2: Datasaurus Dozen, taken from Revolution Analytics&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;One place where the variance explained can fail is if the predictions are systematically shifted off from the true values. Let's say prediction was shifted off by 2 units.&lt;/p&gt;
&lt;p&gt;$$var(y&lt;em&gt;{true} - y&lt;/em&gt;{pred}) = var([2, 2, ..., 2]) = 0$$&lt;/p&gt;
&lt;p&gt;There's no variance in errors, even though they are systematically shifted off from the true prediction. Like r&lt;sup&gt;2&lt;/sup&gt;, variance explained will fail here.&lt;/p&gt;
&lt;p&gt;As usual, &lt;a href=&quot;https://en.wikipedia.org/wiki/Anscombe%27s_quartet&quot;&gt;Ansecombe's quartet&lt;/a&gt;, as does &lt;a href=&quot;https://www.autodeskresearch.com/publications/samestats&quot;&gt;The Datasaurus Dozen&lt;/a&gt;, gives us a pertinent reminder that visually inspecting your model predictions is always a good thing!&lt;/p&gt;
&lt;p&gt;h/t to my colleague, &lt;a href=&quot;https://www.linkedin.com/in/clayton-springer-5a48072/&quot;&gt;Clayton Springer&lt;/a&gt;, for sharing this with me.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/3/22/functools-partial/">
    <title type="text">Functools Partial</title>
    <id>urn:uuid:7059f3fb-181e-305d-840a-bf1c34b42e8b</id>
    <updated>2019-03-22T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/3/22/functools-partial/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;If you’ve done Python programming for a while, I think it pays off to know some little tricks that can improve the readability of your code and decrease the amount of repetition that goes on.&lt;/p&gt;
&lt;p&gt;One such tool is &lt;code&gt;functools.partial&lt;/code&gt;. It took me a few years after my first introduction to &lt;code&gt;partial&lt;/code&gt; before I finally understood why it was such a powerful tool.&lt;/p&gt;
&lt;p&gt;Essentially, what &lt;code&gt;partial&lt;/code&gt; does is it wraps a function and sets a keyword argument to a constant. That’s it. What do we mean?&lt;/p&gt;
&lt;p&gt;Here’s a minimal example. Let’s say we have a function &lt;code&gt;f&lt;/code&gt;, not written by me, but provided by someone else.&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# do something with a and b.&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In my code, let’s say that I know that the value that &lt;code&gt;b&lt;/code&gt; takes on in my app is always the tuple &lt;code&gt;(1, 'A')&lt;/code&gt;. I now have a few options. The most obvious is assign the tuple &lt;code&gt;(1, 'A')&lt;/code&gt; to a variable, and pass that in on every function call:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;A&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# do some stuff.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# do more stuff.&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# ad nauseum&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# set value of N&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;resultN&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The other way I could do it is use &lt;code&gt;functools.partial&lt;/code&gt; and just set the keyword argument &lt;code&gt;b&lt;/code&gt; to equal to the tuple directly.&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;functools&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;partial&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;f_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;partial&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;A&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, I can repeat the code above, but now only worrying about the keyword argument &lt;code&gt;a&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;result1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# do some stuff.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# do more stuff.&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# ad nauseum&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# set value of N&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;resultN&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And there you go, that’s basically how &lt;code&gt;functools.partial&lt;/code&gt; works in a nutshell.&lt;/p&gt;
&lt;p&gt;Now, where have I used this in real life?&lt;/p&gt;
&lt;p&gt;The most common place I have used it is in Flask. I have built Flask apps where I need to dynamically keep my Bokeh version synced up between the Python and JS libraries that get called. To ensure that my HTML templates have a consistent Bokeh version, I use the following pattern:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;bokeh&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__version__&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bkversion&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;flask&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;render_template&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Flask&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;functools&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;partial&lt;/span&gt; 

&lt;span class=&quot;n&quot;&gt;render_template&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;partial&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;render_template&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bkversion&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bkversion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Flask app boilerplate&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;app&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Flask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;vm&quot;&gt;__name__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;nd&quot;&gt;@app&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;route&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;/&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;home&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;render_template&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;index.html.j2&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, because I always have &lt;code&gt;bkversion&lt;/code&gt; pre-specified in &lt;code&gt;render_template&lt;/code&gt;, I never have to repeat it over every &lt;code&gt;render_template&lt;/code&gt; function call.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/3/20/how-i-work/">
    <title type="text">How I Work</title>
    <id>urn:uuid:ecb1119f-e770-31bd-bddd-4da7130485e2</id>
    <updated>2019-03-20T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/3/20/how-i-work/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;I was inspired to write this because of Will Wolf’s interview with DeepLearning.AI, in which I found a ton of similarities between how both of us work. As such, I thought I’d write down what I use at work to get things done.&lt;/p&gt;
&lt;h2 id=&quot;tooling&quot;&gt;Tooling&lt;/h2&gt;&lt;p&gt;For a data scientist, I think tooling is of very high importance: mastery over our tools keeps us productive. Here’s a sampling of what I use at work:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Compute: I have my own MacBook, but I prefer freeloading off my colleague’s workstation, which is connected to our HPC compute cluster, allowing me to do parallelization with Dask!&lt;/li&gt;
&lt;li&gt;Editors/IDEs: VSCode + Jupyter Lab (JLab). Lots of plugins for VSCode!&lt;/li&gt;
&lt;li&gt;Terminal: iTerm, with my &lt;a href=&quot;https://github.com/ericmjl/dotfiles&quot;&gt;&lt;code&gt;dotfiles&lt;/code&gt;&lt;/a&gt; providing a high degree of customization. I also use the VSCode and JLab terminals where convenient.&lt;/li&gt;
&lt;li&gt;General Purpose: Python, Dask, git&lt;/li&gt;
&lt;li&gt;ML/Stats: &lt;code&gt;scikit-learn&lt;/code&gt;, &lt;code&gt;jax&lt;/code&gt;, &lt;code&gt;pymc3&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Data wrangling: &lt;code&gt;pandas&lt;/code&gt;, &lt;code&gt;pyjanitor&lt;/code&gt; (a package I wrote to provide convenience APIs for data cleaning)&lt;/li&gt;
&lt;li&gt;Data visualization: &lt;code&gt;matplotlib&lt;/code&gt;, &lt;code&gt;seaborn&lt;/code&gt;, &lt;code&gt;holoviews&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;App development: &lt;code&gt;flask&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As you probably can see, I’m a very Python-centric person!&lt;/p&gt;
&lt;h2 id=&quot;daily/weekly-routines&quot;&gt;Daily/Weekly Routines&lt;/h2&gt;&lt;p&gt;Most of my work necessitates long stretches of thinking and hacking time. Without that, I’m unable to get into &quot;the zone&quot; to do anything productive. Hence, I have a habit of packing meetings onto Mondays (a.k.a. &quot;Meeting Mondays&quot;). Backup times for meetings, which I prefer to not do, are 11 am and 1 pm, bookending lunch time so that I don’t end up with a fragmented morning/afternoon. The only exceptions I make are for my two high-priority team meetings, for which I defer to the rest of the team. I’m glad that my managers understand the need for long stretches of hacking time, and have stuck to Monday one-on-one meetings.&lt;/p&gt;
&lt;p&gt;Hence, almost every day from Tuesday through to Friday, I have long stretches of pre-allocated time for hacking. It’s data science scheduling bliss! It also means I turn down a lot of &quot;can I meet you to chat&quot; invites - unless we can pack them on Monday!&lt;/p&gt;
&lt;p&gt;On Friday, I make a point to try to work remotely. It helps with sanity, particularly in the winter, when the commute gets harsh and I can’t bike. Fridays also are the days on which I try to do my open source work.&lt;/p&gt;
&lt;h2 id=&quot;pair-coding&quot;&gt;Pair Coding&lt;/h2&gt;&lt;p&gt;Pair coding with others on mutual projects has been a very productive endeavor, &lt;a href=&quot;https://ericmjl.github.io/blog/2019/3/1/pair-coding-why-and-how-for-data-scientists/&quot;&gt;which I have written about before&lt;/a&gt;. Unlike weekly update meetings, I plan for pair coding on an as-needed basis. We have a pre-defined goal for what we want to accomplish, including a conceivably achievable goal and a stretch goal; achieving the easier one keeps us motivated. It follows the &quot;no agenda, no meeting&quot; rule of thumb by which I protect my time.&lt;/p&gt;
&lt;p&gt;I found that a good setup is really necessary for pair coding to be successful. A minimum is a dual-monitor setup, with one extra keyboard + mouse for my coding partner.&lt;/p&gt;
&lt;p&gt;One thing I didn’t mention in my previous blog post was how knowledge transfer happens. Here’s how I think it works. We have one in the &quot;driver’s seat&quot;, and the other in the observer role. Knowledge transfer generally happens from the more experienced person to the less experienced one, and the driver doesn’t necessarily have to be the more experienced one. For example, when pair coding with my intern, I play the role of observer and may dictate code or outline what needs to be done, but I don’t actively take over on my keyboard unless there’s a situation that shows up that is irrelevant to the coding session goals. On the other hand, if there’s a codebase I’ve developed for which I need to play the tour guide role, I will be in the driver’s seat, while the observer will help me catch peripheral errors that I’m making.&lt;/p&gt;
&lt;h2 id=&quot;learning-new-things&quot;&gt;Learning New Things&lt;/h2&gt;&lt;p&gt;Pair coding has been one way I learn new things. For example, with my colleague Zach as the observer, we hacked together a simple dashboard project using Flask, Holoviews and Panel.&lt;/p&gt;
&lt;p&gt;I’m not very mathematically-savvy, in that algebra is difficult for me to follow. (I’m mildly algebra-blind, but getting better now.) Ironically, code, which is algebraic in nature too, but works with plain English names, works much better for me. Implementing algorithms and statistical methods using &lt;code&gt;jax&lt;/code&gt; (for things that involve differential computing) and &lt;code&gt;PyMC3&lt;/code&gt; (for all things Bayesian) has served to be very educational. While implementing, I also impose some software abstractions on the math, and this also forces me to organize my knowledge, which also helps learning. Implementing things on the computer is also the perfect way to learn by teaching: The computer is the ultimately dumb student, as it will execute exactly as you tell it, mistakes included!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/3/1/pair-coding-why-and-how-for-data-scientists/">
    <title type="text">Pair Coding: Why and How for Data Scientists</title>
    <id>urn:uuid:639416f0-072e-3e1a-bb09-6bbec1ce20e0</id>
    <updated>2019-03-01T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/3/1/pair-coding-why-and-how-for-data-scientists/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;&lt;p&gt;While at work, I've been experimenting with pair coding with other data science-oriented colleagues. My experiences tell me that this is something extremely valuable to do. I'd like to share here the &quot;why&quot; and the &quot;how&quot; on pair coding, but focused towards data scientists.&lt;/p&gt;
&lt;h2 id=&quot;what-is-pair-coding?&quot;&gt;What is pair coding?&lt;/h2&gt;&lt;p&gt;Pair coding is a form of programming where two people work together on a single code base together. It usually involves one person on the keyboard and another talking through the problem and observing for issues, such as syntax, logic, or code style. Occasionally, they may swap who is on the keyboard. In other words, one is the &quot;creator&quot;, and the other is the &quot;critic&quot; (but in a positive, constructive fashion).&lt;/p&gt;
&lt;h2 id=&quot;what-s-your-history-with-pair-coding?&quot;&gt;What's your history with pair coding?&lt;/h2&gt;&lt;p&gt;I was inspired by a few places. Firstly, there are a wealth of blog posts detailing the potential benefits and pitfalls of pair coding, in a software developer's context. (A quick Google search will lead you to them.) Secondly, I had, at work, experimented with &quot;pair hacking&quot; sessions, which involved more than coding, including white-boarding a problem to get a feel for its scope, and it turned out to be pretty productive. Thirdly, I was inspired by &lt;a href=&quot;https://www.newyorker.com/magazine/2018/12/10/the-friendship-that-made-google-huge&quot;&gt;a New Yorker article on Jeff and Sanjay&lt;/a&gt;, in which part of it chronicled how they worked as a pair to solve the toughest problems at Google.&lt;/p&gt;
&lt;p&gt;Now, because I'm not a software engineer by training, and because don't have extensive experience beforehand, and because there are no data-science-oriented resources for pair coding that I have read before (I'd love to read them if you know of any!), I've had to be adapt what I read for software development to a data science context.&lt;/p&gt;
&lt;h2 id=&quot;what-are-the-potential-benefits-of-pair-coding?&quot;&gt;What are the potential benefits of pair coding?&lt;/h2&gt;&lt;p&gt;I can see at least the following benefits, if not more that I have yet to discover:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Instant peer review over data science logic and code. Because we are talking through a problem while coding it up, we can instantly check whether our logic is correct against each other.&lt;/li&gt;
&lt;li&gt;Knowledge transfer. In my experience, I've had productive pair-coding sessions with another colleague who has a better grasp of the project than I do. Hence, I contribute &amp;amp; teach the technical component, while I also learn the broader project context better.&lt;/li&gt;
&lt;li&gt;Building trust. We all know that the more closely you work with someone, the more rough corners get rubbed off.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;what-pre-requisites-do-you-see-for-a-productive-pair-programming-session?&quot;&gt;What pre-requisites do you see for a productive pair programming session?&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;A long, continuous, and uninterrupted time slot (at least 2-3 hours in length) to maintain continuity.&lt;/li&gt;
&lt;li&gt;A defined goal or question that we are seeking to answer - keeps us focused on what needs to be done.&lt;/li&gt;
&lt;li&gt;That goal should also be plausibly achievable within the 2-3 hour timeframe.&lt;/li&gt;
&lt;li&gt;Large monitors for both parties to look at, or a code-sharing platform where both can see the code without needing to physically huddle.&lt;/li&gt;
&lt;li&gt;A place where we can talk without feeling hindered.&lt;/li&gt;
&lt;li&gt;No impromptu interruptions from other individuals.&lt;/li&gt;
&lt;li&gt;Complementary and intersecting skillsets.&lt;/li&gt;
&lt;li&gt;Open-minded individuals who are willing to learn. (Ego-free.)&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;where-does-pair-coding-differ-for-data-scientists-vs.-software-engineers?&quot;&gt;Where does pair coding differ for data scientists vs. software engineers?&lt;/h2&gt;&lt;p&gt;I think the differences at best are subtle, not necessarily overt.&lt;/p&gt;
&lt;p&gt;The biggest difference that I can think of might be in clarity. To the best of my knowledge, software engineers work with pretty well-defined requirements. The only hiccups that I can imagine that may occur are in unforeseen logic/code blockers. Data scientists, on the other hand, often are exploring and defining the requirements as things go along. In other words, we are working with more unknowns than a software engineer might.&lt;/p&gt;
&lt;p&gt;An example is a model I built with a colleague at work that involved groups of groups of samples. We weren't able to envision the final model right at the beginning, and code towards it. Rather, we built the model iteratively, starting with highly simplifying assumptions, discussing which ones to refine, and iteratively building the model as we went forward.&lt;/p&gt;
&lt;p&gt;Perhaps a related difference is that as data scientists, because of potentially greater uncertainty surrounding the final product, we may end up talking more about project direction than one would as a software engineer. But that's probably just a minor detail.&lt;/p&gt;
&lt;h2 id=&quot;do-you-have-any-memorable-quotes-from-the-new-yorker-article?&quot;&gt;Do you have any memorable quotes from the New Yorker article?&lt;/h2&gt;&lt;p&gt;Yes, a number of them.&lt;/p&gt;
&lt;p&gt;One on scaling things up.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Alan Eustace became the head of the engineering team after Rosing left, in 2005. &quot;To solve problems at scale, paradoxically, you have to know the smallest details,&quot; Eustace said.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Another on pair programming as an uncommon practice:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&quot;I don’t know why more people don’t do it,&quot; Sanjay said, of programming with a partner.&lt;/p&gt;
&lt;p&gt;&quot;You need to find someone that you’re gonna pair-program with who’s compatible with your way of thinking, so that the two of you together are a complementary force,&quot; Jeff said.&lt;/p&gt;
&lt;/blockquote&gt;
</content>
  </entry>
</feed>
