<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title type="text">Eric Ma's Blog</title>
  <id>urn:uuid:8e5496e4-8606-3632-a35c-1d9694b4313d</id>
  <updated>2018-02-20T00:00:00Z</updated>
  <link href="http://www.ericmjl.com/blog/" />
  <link href="http://www.ericmjl.com/blog.xml" rel="self" />
  <author>
    <name></name>
  </author>
  <generator uri="https://github.com/ajdavis/lektor-atom" version="0.2">Lektor Atom Plugin</generator>
  <entry xml:base="http://www.ericmjl.com/blog/2018/2/20/deep-learning-and-the-importance-of-a-good-teacher/">
    <title type="text">Deep Learning and the Importance of a Good Teacher</title>
    <id>urn:uuid:4688e245-0c9f-34a6-9768-ac175abdb43a</id>
    <updated>2018-02-20T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2018/2/20/deep-learning-and-the-importance-of-a-good-teacher/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;I can’t emphasize this enough - someone who teaches well can really open their student’s minds.&lt;/p&gt;
&lt;p&gt;On my journey in to deep learning and now graph convolutions, &lt;a href=&quot;../../../../../blog/2018/2/20/deep-learning-and-the-importance-of-a-good-teacher/www.cs.toronto.edu/~duvenaud/&quot;&gt;David Duvenaud&lt;/a&gt; (currently a professor at the University of Toronto) simultaneously taught me, a newcomer to deep learning, the basics of deep learning and the mechanics behind the graph convolutional neural network he and his colleagues had just published. The key insight he passed on to me was that deep learning was nothing more than chaining differentiable functions together. Many times I’d ask him, “so does this mean I can do that operation?”, and the answer would usually be “yeah, why not?”.&lt;/p&gt;
&lt;p&gt;Knowing this point has made me realize how flexible deep learning really is. Once I got under the hood of what deep learning really was, then I realized that actually, DL is all about chaining together math functions one after another. Best part is, we get to define what those math functions are!&lt;/p&gt;
&lt;p&gt;Knowing this has also helped me when I read new DL papers. It’s now a lot easier to for me to tell when a research group has come up with something very different from the rest of the pack as opposed to advancing existing methods.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2018/2/13/data-scientists-need-to-write-good-apis/">
    <title type="text">Data scientists need to write good APIs</title>
    <id>urn:uuid:f0b881e1-0e2c-322f-8491-acc1e432af0c</id>
    <updated>2018-02-13T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2018/2/13/data-scientists-need-to-write-good-apis/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;I had a breakthrough in my work today. This was not some scientific epiphany, but just breaking through a wall in my progress. Today's breakthrough  was totally enabled by writing my class definitions in a way that made sense, and by writing class methods that enabled me to express my ideas in a literate fashion.&lt;/p&gt;
&lt;p&gt;Logical class definitions and methods, refactored functions... these should be reflexive habits, but unfortunately, this isn't always the case with data science. We get so caught up in writing the code to make that plot that we forget to refactor out so that the block of code isn't brittle. But that brittle code means that my future self will loathe my current self for not writing that code robustly.&lt;/p&gt;
&lt;p&gt;In other words, write good APIs.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2018/2/7/bayesian-inference-and-testing-sets/">
    <title type="text">Bayesian Inference &amp; Testing Sets</title>
    <id>urn:uuid:0f1e7591-6cec-3f8e-bb94-727e0ca6d5b6</id>
    <updated>2018-02-07T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2018/2/7/bayesian-inference-and-testing-sets/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;This topic recently came up again on the &lt;a href=&quot;https://discourse.pymc.io/t/do-we-need-a-testing-set/759/5&quot;&gt;PyMC3 discourse&lt;/a&gt;. I had an opportunity to further clarify what I was thinking about when I first uttered the train/test split comment &lt;a href=&quot;https://www.youtube.com/watch?v=s0S6HFdPtlA&quot;&gt;at PyData NYC&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;After a little while, my thoughts for a layperson are a bit clearer, and I thought I'd re-iterate them here.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Model specification uncertainty: Did we get the conditional relationships correct? Did we specify enough of the explanatory variables? &lt;/li&gt;
&lt;li&gt;Model parameter uncertainty: Given a model, can we quantify the uncertainty in the parameter values?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These are different uncertainties to deal with. We must be clear: where we are pretty sure about the model spec, Bayesian inference is about quantifying the uncertainty in the parameter values. Under this paradigm, if we use more data, we get narrower posterior distributions, and if we use less data, we get wider posterior distributions. If we split the data, we're just feeding in fewer data points to the model; if we don't, then we're just feeding in more data points.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2018/2/6/pycon-program-committee-review/">
    <title type="text">PyCon Program Committee Review</title>
    <id>urn:uuid:fc577a7a-0eb1-373b-a2fd-cbf7c9d77449</id>
    <updated>2018-02-06T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2018/2/6/pycon-program-committee-review/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;This year, I participated in the PyCon 2018 program committee (ProgCom). Though it was my second time doing it, it nonetheless was an eye-opening experience. This was because in contrast to last year, when I was writing my thesis and thus couldn't follow through on both rounds of review, this year I was. I’d like to write a bit about my thoughts on the process, with three-fold goals:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;To document my experience reviewing this year's PyCon talks.&lt;/li&gt;
&lt;li&gt;To bring a little bit more transparency to the process. (Proposal authors, understandably, might feel like the process is opaque.)&lt;/li&gt;
&lt;li&gt;To indirectly encourage others to participate in the process by demystifying what goes on in the mind of a reviewer.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Before I go on, though I do want to make a few points clear on what I will not be doing in this blog post.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;I will not be commenting on any particular proposal.&lt;/li&gt;
&lt;li&gt;I will not be mentioning specific reviewers' names and what they commented on specific proposals.&lt;/li&gt;
&lt;li&gt;I will not be offering tips for making your proposal succeed next year - that will depend on what next year’s program committee is looking for.&lt;/li&gt;
&lt;li&gt;I will not be describing the review process in detail, as this will be dealt with by an &quot;official&quot; blog post from the PSF.&lt;/li&gt;
&lt;li&gt;This blog post is definitely not an invitation to review your current round or next round proposal, as with a full-time job, I currently don't have the bandwidth for that.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Ready to read on? Let's go!&lt;/p&gt;
&lt;h2 id=&quot;stage-1:-scoring&quot;&gt;Stage 1: Scoring&lt;/h2&gt;&lt;p&gt;The process for selecting talks is a two-stage process. In the first stage, we are rating the talks qualitatively (ordinally) on six criteria (for which I will defer to the official PSF blog post for details when it comes out). &lt;strong&gt;At this stage, we are blinded to a speaker's name and prior experience&lt;/strong&gt;, as we want to review only the content on its merits. I'd like to give my thoughts on a few of the criteria below.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;On &quot;code of conduct&quot;&lt;/strong&gt;, this criteria ensures that there's no disparaging of sub-communities. I generally ding'ed talks that carried any hints of negativity, as I'd like to see PyCon be a positive force in the community. A few proposals were easily misconstrued as being negative, even though they weren't in substance; we tried our best to communicate this concern to proposal authors. That said, most talks are technical in nature, thus this criteria was not really an issue.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;On &quot;completeness&quot;&lt;/strong&gt;, it's really as about seeing whether an author demonstrated the meticulousness in making it easy for us to review. Timings were super important to me, to gauge whether I thought the talk proposer had over- or under-proposed content. I also sought detail to evaluate the accuracy of content and connect the sub-points into the author's overall message. The additional detail made it easier for me to champion a talk in the later stages.&lt;/p&gt;
&lt;p&gt;A few authors, while in communication with the ProgCom, flat-out refused to add in details after I had messaged them requesting details, citing other conferences' practices. Unfortunately, each conference is going to be slightly different in how they operate, and that ongoing dialogue can indirectly influence reviewers' perception and therefore the proposal's score. We're all human, and if a talk proposer comes off as uncooperative, especially when we provide an ongoing opportunity to engage in dialogue, it just makes it tougher to justify their elevated presence as a speaker at a conference that emphasizes cooperation and community.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;On &quot;coherence&quot;&lt;/strong&gt;, this point somewhat overlaps with &quot;completeness&quot;, in that the accuracy of content can help boost this criteria. A good talk would cover one sufficiently focused topic for 30 minutes (or 45 minutes if request in sufficient depth.&lt;/p&gt;
&lt;p&gt;I think that what constitutes &quot;sufficient&quot; depends on our state of knowledge. For example, in data science, I would view it as not sufficient to speak on &quot;how to do a data analysis&quot;, as it is now quite clear that each analysis is quite different, and the generalizable principles are too vague to be of use for a listener. On the other hand, speaking about solving a particular (data) science problem can be illuminating with solid take-aways for an audience members.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Other non-explicit criteria can affect the perception of a proposal&lt;/strong&gt;. At this stage, we're doing an ongoing dialogue with proposal authors, up till the submission deadline. Thus, as mentioned above, an authors' cooperativeness can affect our perception of the proposal, particularly on the &quot;code of conduct&quot; criteria - it'll affect whether we can trust an author's ability to adhere to the code of conduct. Additionally, having good English grammar can affect how readable the proposal is. Finally, there is something qualitatively different about a proposal by an author who is deeply passionate about their topic and believes they have something important to say, in contrast to a proposal author who is merely trying to fulfill PyCon selection criteria. I want to hear from the former, not the latter.&lt;/p&gt;
&lt;h2 id=&quot;stage-2:-selection&quot;&gt;Stage 2: Selection&lt;/h2&gt;&lt;p&gt;At this stage, we looked at the composition of talks, and proposed &quot;buckets&quot; of topics that the talks covered. This means that the &quot;topics&quot; are defined by what the community puts together. In total, there were &amp;gt;60+ groups of talks.&lt;/p&gt;
&lt;p&gt;In this stage, we are basically looking for one talk to emerge from each group. The unfortunate reality is that there will be some groups that are small (3 talks), and some groups that are large (&amp;gt;15 talks), and many groups in between, meaning not every talk has an &quot;equal&quot; chance of making it through. This is entirely dependent on what the community has submitted, though, so there's no easy way to control for this.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;At this stage, we are also debating what kind of conference we want.&lt;/strong&gt; This is where it gets super interesting, and the idiosyncracies of each ProgCom member will show through. Here's a sampling of questions that went through my mind.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Do we want one in which mature talks are rehashed from the three other conferences it's already been at, or do we want new talks to come to prominence?&lt;/strong&gt; It's not an easy decision - for some topics we favoured new ones, and for others we favoured mature talks.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Do we want experienced speakers or do we want to encourage new ones to come up?&lt;/strong&gt; This one is relatively easy - we hard-limited speakers to one talk; some speakers proposed 5 or 6 of them, but we only take one, to enable other speakers to be present. This gives more room for newcomers to speak.&lt;/p&gt;
&lt;p&gt;For me, there were some experienced speakers giving new talks, and I knew they'd be able to pull it off given their track record; I favoured them on both merits: experience and topical novelty. On the other hand, there were experienced speakers taking a single talk around the conference circuit - for these talks, if they were publicly available online, I didn't favour them, and communicated that to other members of the ProgCom team.&lt;/p&gt;
&lt;p&gt;Amongst new speakers (and relatively unknown speakers) at PyCon, I was looking for slide decks to evaluate their message, and recordings of other talks that they had done. Knowing that there's a catch-22 problem (new speaker approximately means no recordings), I also tried looking in greater detail at the proposal for hints and clues that the speaker knew in great depth what they were speaking about, and were confident about delivering it. (Generally, good amounts of non-jargony detail highlight a speaker's capacity for mastery and communication skills simultaneously.)&lt;/p&gt;
&lt;p&gt;My advocacy for new speakers and new talks, and advocacy against talks that had been given at other conferences, particularly those which had a public recording available online, stems from my desire to see PyCon be complementary to other conferences. We're programmers, and I thInk Don't Repeat Yourself (DRY) is a good general principle to adhere to. Other ProgCom members were also free to disagree with my advocacy, as are you, the reader.&lt;/p&gt;
&lt;p&gt;In deciding on whether to include some evergreen topics at a beginners level or not, I looked to history to help me decide. For example, we hadn't had a beginner-friendly live talk on testing for the past few years, so I advocated in favour of that, even though one other program committee member disagreed and preferred to have more advanced testing talks in the program.&lt;/p&gt;
&lt;p&gt;One topic that I was super torn by when voting was in the Bayesian Statistics category. Two talks, one extremely topical and beginner friendly, the other deeply technical but extremely useful in a variety of domains, both by speakers whom I've learned from in the past. I couldn't bring myself to pick one, so I voted for both and communicated this guidance to others on the team, and let them cast the deciding vote.&lt;/p&gt;
&lt;p&gt;Finally, as a machine learner, I have been frustrated by new libraries that don't respect existing community idioms, however idiosyncratic those idioms are. One particular pet peeve is libraries that reinvent similar-yet-slightly-different APIs. There are a myriad of DataFrame APIs out there, yet I've only seen Dask do its best to explicitly implement the Pandas API. Likewise, there are a myriad of GPU tensor libraries out there, but I've only seen CuPy explicitly implement the NumPy API, which is idiomatic in the Python community. I thus strongly advocated for talks that described projects that explicitly adhered to and built on top of community idioms.&lt;/p&gt;
&lt;h2 id=&quot;gratitude-towards-the-team&quot;&gt;Gratitude towards the team&lt;/h2&gt;&lt;p&gt;I'm not ProgCom's fearless leader (Jason Myers, whose name will be public anyways, is our fearless leader (and spreadsheet maestro) – and yes, I know I mentioned a second name here), but I nonetheless feel a ton of gratiutde towards the team. We worked asynchronously, distributed around globe in a myriad of time zones. We gave incisive insight into topics, and educated each other on our respective areas of expertise. I learned and got excited about new topics in Python. We debated and advocated for a PyCon that we'd all be proud of presenting back to the community.&lt;/p&gt;
&lt;h2 id=&quot;encouragement-for-speakers-accepted-and-turned-down&quot;&gt;Encouragement for speakers, accepted and turned down&lt;/h2&gt;&lt;p&gt;As part of PyCon's program committee in 2018, I'm proud to congratulate speakers accepted to this year's PyCon talk lineup!&lt;/p&gt;
&lt;p&gt;To those who were turned down (myself included), I would like to offer up a picture of the reality we faced: some talks were decided by a single vote; at other times, we had to decide between two proposals submitted independently that paralleled each other; yet at other times, we saw such a big cluster of talks that we knew we wanted to hear, yet could only pick a handful because we didn't have an Education Summit-like dedicated track to accommodate all of them. Tough choices left and right. Don't be discouraged, you have important things to say, and there are many awesome Python-related venues (SciPy &amp;amp; PyData) to present at.&lt;/p&gt;
&lt;p&gt;For those whose data science talks were turned down, ping me on Twitter @ericmjl: I'd love to organize a Data Science Summit with you at PyCon 2019!&lt;/p&gt;
&lt;h2 id=&quot;addressing-potential-lingering-questions&quot;&gt;Addressing potential lingering questions&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Does everybody on the ProgCom have the qualifications to review every talk proposal submitted?&lt;/strong&gt; Definitely not. I have a knowledge bias towards the data science talks, and could handle some of the web talks, but I was completely unqualified to review talks on security and Python internals. Thus, for the data science talks, I offered my guidance to the rest of the ProgCom on what would be useful to speak about, but deferred to the expertise of others for talks I could not intelligently comment on. More than once I found myself re-voting based on other expert opinions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Without a fixed criteria on hand, how can talk proposers maximize their chances of getting their talk accepted?&lt;/strong&gt; This “criteria” (if you want to call it that) develops organically over time. This is intentional, as PyCon is a community conference, not a topical conference. By not setting explicit topical criteria, we can solicit talks from the community that range from timely to evergreen, from specialized to broad, and beyond, allowing the community to speak for itself (pun, ahem, not intended).&lt;/p&gt;
&lt;p&gt;This is not to say that PyCon couldn't become a topical conference, in which the ProgCom solitics proposals in particular pre-defined categories. If this changes, I'd love for next year's ProgCom to be explicit about this change, so that proposal authors have enough time to prepare for it.&lt;/p&gt;
&lt;p&gt;Moreover, new topical areas can be proposed: if I am remembering history correctly, this is how the Education Summit came into being (though I'm happy to be corrected if I'm wrong). If anybody's up for it, I'd love for a topical &quot;Data Science Summit&quot; at PyCon to come to life as well! Let's propose it together next year through the Hatchery Program.&lt;/p&gt;
&lt;p&gt;Back to the question, though: if you're thinking of this question, your proposal is probably not the one I would vote in favour of. I personally would like to hear from speakers who are deeply technical and can inject passion into a room through their technical talk, rather than from someone who was trying to tick checkboxes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If you're on the ProgCom, does this mean you can't propose a talk?&lt;/strong&gt; Of course you can propose a talk! :) Our previous fearless leader, Ned Jackson Lovely, wrote an open source app that hides our own talks from our own review, thus enabling us to remain impartial. For what it's worth, my own talk was not accepted by the ProgCom, but I have no hard feelings about it - it was placed in a category with (I think) 21 talks, making that category super competitive.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2018/1/29/refactor-notebook-code/">
    <title type="text">Refactor Notebook Code</title>
    <id>urn:uuid:7c009a1d-1d8f-3965-b5ee-6e1a1b8dab85</id>
    <updated>2018-01-29T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2018/1/29/refactor-notebook-code/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;Jupyter notebooks that are filled with complex analyses can get unwieldy. Refactoring repeated code out into functions placed in modules should be standard practice, but from the sampling of Jupyter notebooks I've seen, I don't think this is standard practice.&lt;/p&gt;
&lt;p&gt;When should code be refactored? As soon as we start copying/pasting it! Making sure I have self-contained functions ensures that lingering state in my notebook doesn't cause unexpected behaviour. (Side note: learning the &quot;functional&quot; programming mindset can be very useful here!)&lt;/p&gt;
&lt;p&gt;But won't this slow down my pace? Isn't it faster to just copy and paste the code, and tweak what I need? Yes, but a small speed hit is going to be traded for a massive bump in rigour. Just today, I saw the effects of &quot;lingering state&quot; in my notebooks causing my plots to display different things before and after refactoring. It's not a good sign for any analysis if this happens.&lt;/p&gt;
&lt;p&gt;In short, refactor your code.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2018/1/18/pymc3-docs-weibull-patches-merged/">
    <title type="text">PyMC3 docs + Weibull patches merged!</title>
    <id>urn:uuid:bf58dfdb-811c-3ae2-a502-f789c2867530</id>
    <updated>2018-01-18T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2018/1/18/pymc3-docs-weibull-patches-merged/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;I recently had a few PRs merged into the PyMC3 codebase. Really happy about it, and just like my previous bug fix, I thought I'd share a bit about how those PRs came about.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&quot;https://github.com/pymc-devs/pymc3/pull/2789&quot;&gt;first PR&lt;/a&gt; was an update to the docs on when to specify precision and when to specify standard deviation. They're related, so only one has to be specified, but I sometimes am sloppy when reading the docs and didn't pick up on that. Thus, I added a few lines to make sure this was crystal clear to sloppy readers like me.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&quot;https://github.com/pymc-devs/pymc3/pull/2799&quot;&gt;next PR&lt;/a&gt; was an update to the Mixture model docs, in which I added an example of the new API for specifying components of mixture models. It previously wasn't clear how to do this, as there were no examples provided, so I put in a documentation PR specifying examples.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&quot;https://github.com/pymc-devs/pymc3/pull/2804&quot;&gt;final PR&lt;/a&gt; was a patch to the Weibull distribution. I wanted to play around with trying mixture Weibulls at work, but mixture Weibulls wouldn't work because it didn't have a mode specified. I checked on Wikipedia, and found that Weibull's mode is conditional on the value of its parameters, and thus put in a PR to make this happen. &lt;a href=&quot;https://github.com/ericmjl/bayesian-analysis-recipes/blob/master/notebooks/mixture-model.ipynb&quot;&gt;Trying it out on some simulated/toy data&lt;/a&gt;, it worked! Thus, the devs allowed it to be merged.&lt;/p&gt;
&lt;p&gt;A few lessons I've learned along the way:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;(1) Docs are an awesome place to start.&lt;/strong&gt; In fact, I made a few formatting mistakes in my first and second PRs that gave an opportunity for another guy to fix! Nothing is too small to be made as a contribution. FWIW, my first contribution to open source software were documentation fixes for &lt;code&gt;matplotlib&lt;/code&gt;, and that was a superb learning journey!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;(2) Friendly maintainers are crucial.&lt;/strong&gt; The PyMC dev team can basically be described as, &quot;generally super nice!&quot; From the online and in-person interactions I've had with them, there's little in the way of egos, they're always learning, always being generally helpful. If they weren't that way, I very likely would have second thoughts trying putting in a PR there.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;(3) Open source lets me fix bugs I find.&lt;/strong&gt; This lets me work at the pace that I need to, without having to wait for commercial vendors to provide update patches. If the patch that I find turns out to be useful for others, then the work I did can possibly save a ton of people's time as well. Win-win scenario!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2018/1/10/offline-time/">
    <title type="text">Offline Time</title>
    <id>urn:uuid:9068e2bf-6f5e-3c23-a72f-c28b96d31b22</id>
    <updated>2018-01-10T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2018/1/10/offline-time/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;I was locked out of my work computer due to password reasons. (It's not human error - something about corporate management tools locked me out. Okay, well, that's human error too.) That said, I inadvertently gained a full two hours of offline time on Monday.&lt;/p&gt;
&lt;p&gt;Those two hours turned out to be pretty productive. I spent some time sketching out my work projects, trying to make better sense of how the project could fit in a disease area researcher's workflow, and figuring out derivative analyses that could enhance the value to them. This was something I probably wouldn't be able to accomplish if I had the regular distractions of my computer nearby.&lt;/p&gt;
&lt;p&gt;Seems like Cal Newport's &quot;digital distraction de-cluttering&quot; is a good thing to do. I must do more of it.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2018/1/8/bayesian-uncertainty-a-more-nuanced-view/">
    <title type="text">Bayesian Uncertainty: A More Nuanced View</title>
    <id>urn:uuid:ac107438-c56c-3961-9fb0-b41c8a66858a</id>
    <updated>2018-01-08T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2018/1/8/bayesian-uncertainty-a-more-nuanced-view/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;The following thought hit my mind just last night.&lt;/p&gt;
&lt;p&gt;Bayesian inference requires the computation of uncertainty. Computing that uncertainty is computationally expensive compared to simply computing point estimates/summary statistics. But when exactly is uncertainty useful, and more importantly, actionable? That's something I've not really appreciated in the past. It's probably not productive to be dogmatic about always computing uncertainty if that uncertainty is not actionable.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2017/12/13/visual-studio-code-a-new-microsoft/">
    <title type="text">Visual Studio Code: A New Microsoft?</title>
    <id>urn:uuid:edf0c31f-74d9-39eb-a09f-a61e861ff480</id>
    <updated>2017-12-13T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2017/12/13/visual-studio-code-a-new-microsoft/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;During my week attending PyData NYC 2017, which was effectively a mini-mini-sabbatical from work, I got a chance to try out Visual Studio Code. Part of it was curiosity, having seen so many PyData participants using it; part of it was because of Steve Dowell, a core CPython contributor who works at Microsoft, who mentioned about the Python-friendly tools they added into VSCode.&lt;/p&gt;
&lt;p&gt;I think VSCode is representative of a new Microsoft.&lt;/p&gt;
&lt;p&gt;But first, let me describe what using it is like.&lt;/p&gt;
&lt;h2 id=&quot;user-interface&quot;&gt;User Interface&lt;/h2&gt;&lt;p&gt;First off, the UI is beautiful. It's impossible to repeat enough how important the UI is. With minimal configuration, I made it basically match Atom's UI, which I had grown used to. It has an integrated terminal, and the colours are... wow. That shade of green, blue and red are amazing, ever just so slightly muted compared to the Terminal or iTerm. The background shade of black matches well with the rest of VSCode, and the colour scheme is changeable to match that of Atom's. The design feels... just right. Wow!&lt;/p&gt;
&lt;h2 id=&quot;git-integration&quot;&gt;Git Integration&lt;/h2&gt;&lt;p&gt;Secondly, the integration with Git rivals Atom; in fact, there's a one-click &quot;sync&quot; button! It also has nice &lt;code&gt;git commit -am&lt;/code&gt; analog where I can add and commit all of the files simultaneously.&lt;/p&gt;
&lt;h2 id=&quot;intellisense&quot;&gt;Intellisense&lt;/h2&gt;&lt;p&gt;Thirdly, IntelliSense is just amazing! I like how I can use it to look up a function signature just by mousing over the function name.&lt;/p&gt;
&lt;h2 id=&quot;open-source&quot;&gt;Open Source&lt;/h2&gt;&lt;p&gt;Finally, it’s fully open source and back able, in the same vein as Atom, minus the bloat that comes from building on top of electron. Impressive stuff!&lt;/p&gt;
&lt;h2 id=&quot;other-thoughts&quot;&gt;Other Thoughts&lt;/h2&gt;&lt;p&gt;Now, on the new Microsoft.&lt;/p&gt;
&lt;p&gt;Only at the recent PyData NYC did I learn that Microsoft has hired almost half of the core CPython developers! Not only that, they are encouraged to continue their contributions into the CPython code base. In my view, that’s a pretty awesome development! It means the Python programming language will continue to have a strong corporate backing while also enjoying community support. Its a sign of a healthy ecosystem, IMO, and also a sign of Microsoft’s support for Open Source Software!&lt;/p&gt;
&lt;p&gt;I’m more and more impressed by what Microsoft is doing for the Open Source community. I’m hoping they’ll continue up with this!!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2017/11/30/pydata-nyc-2017-recap/">
    <title type="text">PyData NYC 2017 Recap</title>
    <id>urn:uuid:08109ae3-b5f2-3035-a564-d3ca4a21ae25</id>
    <updated>2017-11-30T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2017/11/30/pydata-nyc-2017-recap/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;With that, we’ve finished PyData NYC! Here's some of my highlights of the conference.&lt;/p&gt;
&lt;h2 id=&quot;keynotes&quot;&gt;Keynotes&lt;/h2&gt;&lt;p&gt;There were three keynotes, one each by Kerstin Kleese van Dam, Thomas Sargent, and Andrew Gelman. Interestingly enough, they didn't do what I would expect most academics to do -- give talks highlighting the accomplishments of their research groups. Rather, Kerstin gave a talk that highlighted the use of PyData tools at Brookhaven National Labs. Thomas Sargent gave a philosophical talk on what economic models really are (they're &quot;games&quot;, in a mathematical sense), and I took back the importance of being able to implement models, otherwise, &quot;you're just bull*****ing&quot;.&lt;/p&gt;
&lt;p&gt;Andrew Gelman surprised me the most - he gave a wide-ranging talk about the problems we have in statistical analysis workflows. He emphasized that &quot;robustness checks&quot; are basically scams, because they're basically methods whose purpose is reassurance. He had a really cool example that highlighted that we need to understand our models by modifying our models, perhaps even using a graph of models to identify perturbations to our model that will help us understand our model. He also peppered his talk with anecdotes about how he made mistakes in his analysis workflows. I took home a different philosophy of data analysis: when we evaluate how &quot;good&quot; a model is, the operative question is, &quot;compared against what?&quot;&lt;/p&gt;
&lt;h2 id=&quot;talks&quot;&gt;Talks&lt;/h2&gt;&lt;p&gt;The talks were, for me, the highlight of the conference. A lot of good learning material around. Here's the talks from which I learned actionable new material.&lt;/p&gt;
&lt;h3 id=&quot;analyzing-nba-foul-calls-using-python&quot;&gt;Analyzing NBA Foul Calls using Python&lt;/h3&gt;&lt;p&gt;This talk by the prolific PyMC blogger Austin Rochford is one that I really enjoyed. The take-home that I got from him was towards the end of his talk, in which I picked up three ways to diagnose probabilistic programming models.&lt;/p&gt;
&lt;p&gt;The first was the use of residuals - which I now know can be used for classification problems as well as regression problems.&lt;/p&gt;
&lt;p&gt;The second was the use of the energy plots in PyMC3, where if the &quot;energy transition&quot; and &quot;marginal energy distribution&quot; plots match up (especially in the tails), then we know that the NUTS sampler did a great job.&lt;/p&gt;
&lt;p&gt;The third was the use of the Gelman-Rubin statistic to measure the in-chain vs. between-chain variation; measures close to 1 are generally considered good.&lt;/p&gt;
&lt;p&gt;Check out the talk slides &lt;a href=&quot;http://austinrochford.com/resources/talks/nba-fouls-pydata-nyc-2017.slides.html#/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&quot;scikit-learn-compatible-model-stacking&quot;&gt;&lt;code&gt;scikit-learn&lt;/code&gt;-compatible model stacking&lt;/h3&gt;&lt;p&gt;This talk was a great one because it shows how to use model stacking (also known as &quot;ensembling&quot;, a technique commonly used in Kaggle competitions) to enable better predictions.&lt;/p&gt;
&lt;p&gt;Conceptually, model stacking works like this: I train a set of model individually on a problem, and use the predictions from those models as features for a meta-model. The meta-model should perform, at worst, on par with the best model inside the ensemble, but may also perform better. This idea was first explored in Polley and van der Laan's work, available &lt;a href=&quot;http://biostats.bepress.com/ucbbiostat/paper266&quot;&gt;online&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Civis Analytics has released their implementation of model stacking in their &lt;a href=&quot;https://github.com/civisanalytics/civisml-extensions&quot;&gt;GitHub repository&lt;/a&gt;, and it's available on PyPI. The best part of it? They didn't try inventing a new API, they kept a &lt;code&gt;scikit-learn&lt;/code&gt;-compatible API. Kudos to them!&lt;/p&gt;
&lt;p&gt;Check out the repository &lt;a href=&quot;https://github.com/civisanalytics/civisml-extensions&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&quot;stream-processing-with-dask&quot;&gt;Stream Processing with Dask&lt;/h3&gt;&lt;p&gt;Matthew Rocklin, as usual, gave an entertaining and informative talk on the use of Streamz, a lightweight library he built, to explore the use of Dask for streaming applications. The examples he gave were amazing showcases of the library's capabilities. Given the right project, I'd love to try this out!&lt;/p&gt;
&lt;p&gt;Check out his slides &lt;a href=&quot;http://matthewrocklin.com/slides/pydata-nyc-2017.html#/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&quot;asynchronous-python:-a-gentle-introduction&quot;&gt;Asynchronous Python: A Gentle Introduction&lt;/h3&gt;&lt;p&gt;This talk, delivered by James Cropcho, defined what asynchronous programming was all about. For me, things finally clicked at the end when I asked him for an example of how asynchronous programming would be done in data analytics workflows -- to which he responded, &quot;If you're querying for data and then performing a calculation, then async is a good idea.&quot;&lt;/p&gt;
&lt;p&gt;The idea behind this is as such: web queries written serially are often &quot;blocking&quot;, meaning we can't do anything while we wait for the web query to return. If we want to do a calculation on the returned data point, we have to wait for it to return first. On the other hand, if written asynchronously, we could potentially do a calculation on the previous data point while waiting for the current result to return, shaving the total time off potentially by some considerable fraction.&lt;/p&gt;
&lt;h3 id=&quot;turning-pymc3-into-scikit-learn&quot;&gt;Turning PyMC3 into &lt;code&gt;scikit-learn&lt;/code&gt;&lt;/h3&gt;&lt;p&gt;This talk was by Nicole Carlson, and she did a tremendously great job delivering this talk. In it, she walked through how to wrap a PyMC3 model inside a &lt;code&gt;scikit-learn&lt;/code&gt; estimator, including details on how to implement the &lt;code&gt;.fit()&lt;/code&gt;, &lt;code&gt;.predict()&lt;/code&gt;, and &lt;code&gt;.predict_proba()&lt;/code&gt; methods. The code in &lt;a href=&quot;https://github.com/parsing-science/ps-toolkit/blob/master/ps_toolkit/pymc3_models/HLR.py&quot;&gt;her repository&lt;/a&gt; provides a great base example to copy from.&lt;/p&gt;
&lt;p&gt;One thing I can't emphasize enough is that from a user experience standpoint, it's super important to follow idioms that people are used to. What Nicole did in this talk is to show how we can provide such idioms to end-users, rather than inventing a slightly modified wheel. Props to her for that!&lt;/p&gt;
&lt;p&gt;Her slides are online &lt;a href=&quot;../../../../../blog/2017/11/30/pydata-nyc-2017-recap/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&quot;an-attempt-at-demystifying-bayesian-deep-learning&quot;&gt;An Attempt at Demystifying Bayesian Deep Learning&lt;/h3&gt;&lt;p&gt;This talk was my own, put at the end of the 2nd day. The title definitely contributed to the hype. I popped into the room early, but then left for the restroom. When I got back, there was a lineup in the front door and in the back door. Totally unexpected. That said, big credit to the Boston Bayesians organizers Jordi and Colin, who let me do the talk as a rehearsal for PyData, so I felt very grounded.&lt;/p&gt;
&lt;p&gt;The talk went mostly smoothly. I think I was channeling my colleague, Brant Peterson, with his sense of humour during that time. There was one really hilarious hiccup - right after mentioning that I wouldn't overdo the &quot;math&quot; and &quot;equations&quot;, I accidentally opened an adjacent tab with an alternate version of the slides... with, surprise surprise, a math equation on it! During the Q&amp;amp;A, when I shared the point of not needing to do train/test splits in Bayesian analysis, I could sense the jaws dropping and eyes widening in disbelief; more than just a handful of people came up and asked for the reference later on.&lt;/p&gt;
&lt;p&gt;Having done the talk, I now realize how much people will appreciate a lighthearted and lightweight introduction to a topic that's very dense and filled with jargon. Conference speakers, we need to do more of this!&lt;/p&gt;
&lt;p&gt;From an emotional standpoint, many people brought me joy with their positive comments on the visuals and structure of the talk. Others put out positive comments on Twitter, which I collected together in a &lt;a href=&quot;https://twitter.com/i/moments/924970479869448193?ref_src=twsrc%5Etfw&quot;&gt;Twitter Moment&lt;/a&gt;. It was very encouraging, especially on this deep learning journey that I'm on right now.&lt;/p&gt;
&lt;h2 id=&quot;tutorials&quot;&gt;Tutorials&lt;/h2&gt;&lt;h3 id=&quot;interactive-matplotlib-figures&quot;&gt;Interactive &lt;code&gt;matplotlib&lt;/code&gt; Figures&lt;/h3&gt;&lt;p&gt;This was something I totally didn't realize was possible before - we can create interactive &lt;code&gt;matplotlib&lt;/code&gt; figures very easily! I have cloned the repository, and I think it'll be neat to hack on some projects at work to use this.&lt;/p&gt;
&lt;p&gt;The tutorial repository can be found &lt;a href=&quot;https://github.com/tacaswell/interactive_mpl_tutorial&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&quot;linear-regression-three-ways&quot;&gt;Linear Regression Three Ways&lt;/h3&gt;&lt;p&gt;This one was by Colin Carroll, a software engineer at the MIT Media Lab (previously at Kensho). I sat in and learned a good deal of math from him. One thing new I learned was how we can specify a model without requiring the use of observed variables. Any sampling we do will take into account the hierarchical and mathematical relationships we've done. This makes it neat to implement Bayesian nets to test how things will look under different scenarios!&lt;/p&gt;
&lt;p&gt;His tutorial repository can be found &lt;a href=&quot;https://github.com/ColCarroll/pydata_nyc2017&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&quot;top-to-bottom-line-by-line&quot;&gt;Top-To-Bottom, Line-By-Line&lt;/h3&gt;&lt;p&gt;This one was led by the ever-entertaining, ever-surprising James Powell. I wish the tutorial was recorded, because even though this is a &quot;novice&quot; tutorial, it nonetheless was still an eye-opening talk. Anybody who thinks they know Python should go listen to James' talks, whenever he gives them live - it's bound to be entertaining and eye-opening!&lt;/p&gt;
&lt;h2 id=&quot;overall-thoughts&quot;&gt;Overall Thoughts&lt;/h2&gt;&lt;p&gt;I'm glad I made it to PyData NYC 2017 this year. Made many new friends and connections, and caught up with old friends in the PyData community. As always, learned a ton as well!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2017/11/16/bayesian-learning-and-overfitting/">
    <title type="text">Bayesian Learning and Overfitting</title>
    <id>urn:uuid:aa1fbb3d-1db8-3f41-afd8-02927a0fb440</id>
    <updated>2017-11-16T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2017/11/16/bayesian-learning-and-overfitting/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;Yesterday, after I did my Boston Bayesians dry run talk, there was a point raised that I had only heard of once before: Bayesian learning methods don't overfit. Which means we're allowed to use all the data on hand. The point holds for simple Bayesian networks, and for more complicated deep neural nets.&lt;/p&gt;
&lt;p&gt;Though I believe it, I wasn't 100% convinced of this myself, so I decided to check it up. I managed to get my hands on Radford Neal's book, Bayesian Learning for Neural Networks, and found the following quotable paragraphs:&lt;/p&gt;
&lt;blockquote class=&quot;blockquote blockquote-success&quot;&gt;
&lt;p&gt;It is a common belief, however, that restricting the complexity of the models used for such tasks is a good thing, not just because of the obvious computational savings from using a simple model, but also because it is felt that too complex a model will overfit the training data, and perform poorly when applied to new cases. This belief is certainly justified if the model parameters are estimated by maximum likelihood. I will argue here that concern about overfitting is not a good reason to limit complexity in a Bayesian context.&lt;/p&gt;
&lt;/blockquote&gt;&lt;p&gt;A few paragraphs later, after explaining the frequentist procedure:&lt;/p&gt;
&lt;blockquote class=&quot;blockquote&quot;&gt;
&lt;p&gt;From a Bayesian perspective, adjusting the complexity of the model based on the amount of training data makes no sense. A Bayesian defines a model, selects a prior, collects data, computes the posterior, and then makes predictions. There is no provision in the Bayesian framework for changing the model or the prior depending on how much data was collected. If the model and prior are correct for a thousand observations, they are correct for ten observations as well (though the impact of using an incorrect prior might be more serious with fewer observations). In practice, we might sometimes switch to a simpler model if it turns out that we have little data, and we feel that we will consequently derive little benefit from using a complex, computationally expensive model, but this would be a concession to practicality, rather than a theoretically desirable procedure.&lt;/p&gt;
&lt;/blockquote&gt;&lt;p&gt;Finally, in the following section after describing how neural networks are built:&lt;/p&gt;
&lt;blockquote class=&quot;blockquote&quot;&gt;
&lt;p&gt;In a Bayesian model of this type, the role of the hyperparameters controlling the priors for weights is roughly analogous to the role of a weight decay constant in conventional training. With Bayesian training, values for these hyperparameters (more precisely, a distribution of values) can be found without the need for a validation set.&lt;/p&gt;
&lt;/blockquote&gt;&lt;p&gt;This seems to dovetail well with the following convoluted intuition that I've had: &lt;strong&gt;if I fit a Bayesian model on the &quot;training&quot; set of the data, then update it with the &quot;test&quot; set, it's equivalent to just training with the whole dataset. With wide priors, if I fit with a smaller dataset, my posterior distribution will be wider than if I fit with the entire dataset. So... where possible, just train with the entire dataset.&lt;/strong&gt; That said, I've not had sufficient grounding in Bayesian stats (after all, still a newcomer) to justify this.&lt;/p&gt;
&lt;p&gt;I certainly have more reading/learning to do here. Looks like something neat to explore in the short-term.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2017/11/14/the-value-of-thinking-simply/">
    <title type="text">The Value of Thinking Simply</title>
    <id>urn:uuid:896b86f0-01f9-38dc-85f4-a4cc7bd04e5d</id>
    <updated>2017-11-14T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2017/11/14/the-value-of-thinking-simply/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;Einstein has a famous quote that most people don't hear about.&lt;/p&gt;
&lt;blockquote class=&quot;blockquote&quot;&gt;
&lt;p class=&quot;mb-0&quot;&gt;It can scarcely be denied that the supreme goal of all theory is to make the irreducible basic elements as simple and as few as possible without having to surrender the adequate representation of a single datum of experience.&lt;/p&gt;
&lt;footer class=&quot;blockquote-footer&quot;&gt;Albert Einstein&lt;/footer&gt;
&lt;/blockquote&gt;&lt;p&gt;It instead, most people hear the misquote:&lt;/p&gt;
&lt;blockquote class=&quot;blockquote text&quot;&gt;
&lt;p&gt;Everything should be made as simple as possible, but no simpler. &lt;/p&gt;
&lt;footer class=&quot;blockquote-footer&quot;&gt;Misquoted version&lt;/footer&gt;
&lt;/blockquote&gt;&lt;p&gt;Though a misquote, it's still a fair (though lopsided -- missing a sufficient translation of the latter half) simplification of the original.&lt;/p&gt;
&lt;p&gt;In my work, I'm reminded of this point. I can choose to go for the complex fancy thing, but if I don't start from first principles, or start with simplistic approximations, I will struggle to have a sufficiently firm grasp on a problem to start tackling it. And therein lies the key, I think, in making progress on creative, intellectual work.&lt;/p&gt;
&lt;p&gt;The past week, I've noticed myself not wasting time on mindless coding (which usually amounts to re-running code with tweaks), and instead devoting more time to strategic thinking. As an activity, strategic thinking isn't just sitting there and thinking. For me, it involves writing and re-writing what I'm thinking, drawing and re-drawing what I'm seeing, and arranging and composing the pieces that are floating in my mind. During that time of writing, drawing, arranging and composing, I'm questioning myself, &quot;What if I didn't have this piece?&quot;. Soon enough, the &quot;simplest complex version&quot; (SCV) of whatever I'm working on begins to emerge -- but it never really is the final version! I go back and prototype it in code, and then get stuck on something, and realize I left something out in that SCV, and re-draw the entire SCV from scratch.&lt;/p&gt;
&lt;p&gt;Here's my misquote, then, offered up:&lt;/p&gt;
&lt;blockquote class=&quot;blockquote&quot;&gt;
&lt;p&gt;Sufficiently simple, and only necessarily complex.&lt;/p&gt;
&lt;footer class=&quot;blockquote-footer&quot;&gt;A further mutated version.&lt;/footer&gt;
&lt;/blockquote&gt;</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2017/11/3/boston-bayesians-talk-an-attempt-at-demystifying-bayesian-deep-learning/">
    <title type="text">Boston Bayesians Talk: An Attempt at Demystifying Bayesian Deep Learning</title>
    <id>urn:uuid:a10c1307-f36e-3897-9151-cc7e7857a8ed</id>
    <updated>2017-11-03T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2017/11/3/boston-bayesians-talk-an-attempt-at-demystifying-bayesian-deep-learning/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;It's confirmed! I will be rehearsing my PyData NYC talk &lt;a href=&quot;https://www.meetup.com/Boston-Bayesians/events/244731222&quot;&gt;at Boston Bayesians&lt;/a&gt;, held at McKinsey's office.&lt;/p&gt;
&lt;p&gt;This time round, I've challenged myself with making the slides without using PowerPoint or Keynote, and I think I've successfully done it! Check them out:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/ericmjl/bayesian-deep-learning-demystified&quot;&gt;GitHub repository&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://ericmjl.github.io/bayesian-deep-learning-demystified&quot;&gt;Online slides&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Side note, I'm starting to really love what we can do with the web!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2017/10/31/always-check-your-data/">
    <title type="text">Always Check Your Data</title>
    <id>urn:uuid:8098bba8-6083-3f03-92b7-c6eaea1049d5</id>
    <updated>2017-10-31T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2017/10/31/always-check-your-data/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;True story, just happened today. I was trying to fit a Poisson likelihood to estimate event cycle times (in discreet weeks). For certain columns, everything went perfectly fine. Yet for other columns, I was getting negative infinity’s likelihoods, and was banging my head over this problem for over an hour and a half.&lt;/p&gt;
&lt;p&gt;As things turned out, those columns that gave me negative infinity likelihood initializations were doing so because of negative values in the data. Try fitting a Poisson likelihood, which only has positive support, on that!&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://i.imgflip.com/1yl6ki.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;This lost hour and a half was a good lesson in data checking/testing: &lt;strong&gt;always be sure to sanity check basic stats associated with the data - bounds (min/max), central tendency (mean/median/mode) and spread (variance, quartile range) - always check!&lt;/strong&gt;&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2017/10/27/random-forests-a-good-default-model/">
    <title type="text">Random Forests: A Good Default Model?</title>
    <id>urn:uuid:5c17eda1-cdac-3da7-8384-651ed275c348</id>
    <updated>2017-10-27T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2017/10/27/random-forests-a-good-default-model/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;I've been giving this some thought, and wanted to go out on a limb to put forth this idea:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;I think Random Forests (RF) are a good &quot;baseline&quot; model to try, after establishing a &quot;random&quot; baseline case.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;(Clarification: I'm using RF as a shorthand for &quot;forest-based ML algorithms&quot;, including XGBoost etc.)&lt;/p&gt;
&lt;p&gt;Before I go on, let me first provide some setup.&lt;/p&gt;
&lt;p&gt;Let's say we have a two-class classification problem. Assume everything is balanced. One &quot;dumb baseline&quot;&quot; case is a coin flip. The other &quot;dumb baseline&quot; is predicting everything to be one class. Once we have these established, we can go to a &quot;baseline&quot; machine learning model.&lt;/p&gt;
&lt;p&gt;Usually, people might say, &quot;go do logistic regression (LR)&quot; as your first baseline model for classification problems. It sure is a principled choice! Logistic regression is geared towards classification problems, makes only linear assumptions about the data, and identifies directional effects as well. From a practical perspective, it's also very fast to train.&lt;/p&gt;
&lt;p&gt;But I've found myself more and more being oriented towards using RFs as my baseline model instead of logistic regression. Here are my reasons:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Practically speaking, any modern computer can train a RF model with ~1000+ trees in not much more time than it would need for an LR model.&lt;/li&gt;
&lt;li&gt;By using RFs, we do not make linearity assumptions about the data.&lt;/li&gt;
&lt;li&gt;Additionally, we don't have to scale the data (one less thing to do).&lt;/li&gt;
&lt;li&gt;RFs will automatically learn non-linear interaction terms in the data, which is not possible without further feature engineering in LR.&lt;/li&gt;
&lt;li&gt;As such, the out-of-the-box performance using large RFs with default settings is often very good, making for a much more intellectually interesting challenge in trying to beat that classifier.&lt;/li&gt;
&lt;li&gt;With &lt;code&gt;scikit-learn&lt;/code&gt;, it's a one-liner change to swap out LR for RF. The API is what matters, and as such, drop-in replacements are easily implemented!&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Just to be clear, I'm not advocating for throwing away logistic regression altogether. There are moments where interpretability is needed, and is more easily done by using LR. In those cases, LR can be the &quot;baseline model&quot;, or even just back-filled in after training the baseline RF model for comparison.&lt;/p&gt;
&lt;p&gt;Random Forests were the darling of the machine learning world before neural networks came along, and even now, remain the tool-of-choice for colleagues in the cheminformatics world. Given how easy they are to use now, why not just start with them?&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2017/10/11/pypy-impressive/">
    <title type="text">PyPy: Impressive!</title>
    <id>urn:uuid:e634bb3d-8f4b-3088-b8c8-69cc85fc6c13</id>
    <updated>2017-10-11T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2017/10/11/pypy-impressive/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;A few years on after trying out PyPy for the first time and wrestling with it, I still find it to be pretty awesome.&lt;/p&gt;
&lt;p&gt;Now that PyPy officially supports &lt;code&gt;numpy&lt;/code&gt;, I'm going to profile a few simple statistical simulation tasks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Computing the mean of a number of random number draws.&lt;/li&gt;
&lt;li&gt;Simulating many coin flips&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I'll profile each of the tasks four ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pure Python implementation running from the CPython and PyPy interpreters&lt;/li&gt;
&lt;li&gt;&lt;code&gt;numpy&lt;/code&gt; implementation running from the CPython and PyPy interpreters.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So, how do PyPy and CPython fare? Let's show the results up front first.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;../../../../../blog/2017/10/11/pypy-impressive/profile.png&quot;&gt;&lt;img src=&quot;../../../../../blog/2017/10/11/pypy-impressive/profile-sm.png&quot; alt=&quot;Profiling results.&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Click on the image to view a higher resolution chart. The raw recorded measurements can be found &lt;a href=&quot;https://docs.google.com/spreadsheets/d/1QB1hF7Z8SGYjvll8sYCjVYEYAgzL4pjqGt1dbO6B2Co/edit?usp=sharing&quot;&gt;on Google Sheets&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Here's a description of what's happening:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(top-left): PyPy is approx. 10X faster than CPython at computing the mean of 10 million random numbers.&lt;/li&gt;
&lt;li&gt;(top-right): When both are running &lt;code&gt;numpy&lt;/code&gt;, the speed is identical.&lt;/li&gt;
&lt;li&gt;(bottom-left): When simulating coin flips, PyPy with a custom &lt;code&gt;binomial()&lt;/code&gt; function is about 3X faster than CPython.&lt;/li&gt;
&lt;li&gt;(bottom-right): When using &lt;code&gt;numpy&lt;/code&gt; instead, there is a bottleneck, and PyPy fails badly compared to CPython.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It's pretty clear that when PyPy is dealing with &quot;pure&quot; data (i.e. not having to pass data between Python and C), PyPy runs very, very fast, and, at least in the scenarios tested here, it performs faster than the CPython interpreter. This is consistent with my previous observations, and probably explains why PyPy is very good for code that is very repetitive; the JIT tracer really speeds things up.&lt;/p&gt;
&lt;p&gt;That last plot (bottom-right) is a big curiosity. Using the code below, I measured the random number generation is actually just as fast as it should be using CPython, but that PyPy failed badly when I was passing in a &lt;code&gt;numpy&lt;/code&gt; array to the &lt;code&gt;Counter()&lt;/code&gt; object (from the standard library). I'm not sure what is happening behind-the-scenes, but I have reached out to the PyPy developers to ask what's going on, and will update this post at a later date.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;UPDATE:&lt;/strong&gt; I heard back from the PyPy devs &lt;a href=&quot;https://bitbucket.org/pypy/pypy/issues/2680/slow-speed-going-from-numpy-data-structure&quot;&gt;on BitBucket&lt;/a&gt;, and this is indeed explainable by data transfer between the C-to-PyPy interface. It's probably parallel to the latency that arises from transferring data between the CPU and GPU, or between compute nodes.&lt;/p&gt;
&lt;p&gt;So, what does this mean? It means that for pure Python code, PyPy can be a very powerful way to accelerate your code. One example I can imagine is agent-based simulations using Python objects. Another example that comes to mind is running a web server that only ever deals with strings, floats and JSONs (in contrast to matrix-heavy scientific computing).&lt;/p&gt;
&lt;p&gt;Now, for those who are curious, here's the source code for the &lt;strong&gt;pure Python implementation of the mean of random numbers&lt;/strong&gt;.&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Mean of 10 million random number draws.&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;time&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;random&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1E7&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;rnds&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;rnds&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rnds&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;{} seconds&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And here's the source code for the &lt;strong&gt;&lt;code&gt;numpy&lt;/code&gt; implementation of the mean of random numbers&lt;/strong&gt;.&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;time&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1E7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;{} seconds&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Next, here's the source code for &lt;strong&gt;coin flips in pure Python&lt;/strong&gt;:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Simulate 10 million biased coin flips with p = 0.3&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;random&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;time&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;collections&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Counter&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;bernoulli&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;rnd&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rnd&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;


&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bernoulli&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1E7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Counter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;{} seconds&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And finally, source code for &lt;strong&gt;coin flips using &lt;code&gt;numpy&lt;/code&gt;&lt;/strong&gt;:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy.random&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;binomial&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;time&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;collections&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Counter&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;coinflips&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;binomial&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1E7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Time for numpy coinflips: {} seconds&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Counter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;coinflips&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;{} seconds&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2017/10/10/pydata-nyc-2017/">
    <title type="text">PyData NYC 2017</title>
    <id>urn:uuid:68ffb19b-9905-3852-8f3f-a1a3e04a705c</id>
    <updated>2017-10-10T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2017/10/10/pydata-nyc-2017/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;I'm seriously looking forward to PyData NYC this year -- there's a great lineup of talks that I'm particularly looking forward to hearing! The theme for my set of must-see talks this year is &quot;Bayesian machine learning&quot; - there's much for me to learn!&lt;/p&gt;
&lt;p&gt;The first is by my fellow Boston Bayesian &lt;strong&gt;&lt;a href=&quot;https://colindcarroll.com/&quot;&gt;Colin Caroll&lt;/a&gt;&lt;/strong&gt; with his talk titled &lt;a href=&quot;https://pydata.org/nyc2017/schedule/presentation/12/&quot;&gt;Two views on regression with PyMC3 and scikit-learn&lt;/a&gt;. Colin is a mathematician at heart, even though he does software engineering for living now, and I can't wait to hear about regularization strategies!&lt;/p&gt;
&lt;p&gt;The second is by &lt;strong&gt;&lt;a href=&quot;https://pydata.org/nyc2017/speaker/profile/6/&quot;&gt;Nicole Carlson&lt;/a&gt;&lt;/strong&gt;, with her talk titled &lt;a href=&quot;https://pydata.org/nyc2017/schedule/presentation/24/&quot;&gt;Turning PyMC3 into scikit-learn&lt;/a&gt;. Nicole's talk is of interest to me because I've implemented models in PyMC3 before, and now would like to know how to make them reusable!&lt;/p&gt;
&lt;p&gt;The third talk is by &lt;strong&gt;&lt;a href=&quot;https://pydata.org/nyc2017/speaker/profile/118/&quot;&gt;Chaya Stern&lt;/a&gt;&lt;/strong&gt;, with her talk titled &lt;a href=&quot;https://pydata.org/nyc2017/schedule/presentation/53/&quot;&gt;Bayesian inference in computational chemistry&lt;/a&gt;. Super relevant to my work at Novartis!&lt;/p&gt;
&lt;p&gt;The fourth is by my fellow Boston Pythonista &lt;strong&gt;&lt;a href=&quot;https://pydata.org/nyc2017/speaker/profile/34/&quot;&gt;Joe Jevnik&lt;/a&gt;&lt;/strong&gt;, who will be speaking on the first day about his journey into deep learning on some really cool time-series data. He works at Quantopian, BUT the spoiler here is that his talk is NOT about financial data! (I've heard his talk outline already.)&lt;/p&gt;
&lt;p&gt;The fifth is a tutorial by &lt;strong&gt;&lt;a href=&quot;https://pydata.org/nyc2017/speaker/profile/29/&quot;&gt;Jacob Schrieber&lt;/a&gt;&lt;/strong&gt;, with his talk titled &lt;a href=&quot;https://pydata.org/nyc2017/schedule/presentation/30/&quot;&gt;pomegranate: fast and flexible probabilistic modeling in python&lt;/a&gt;. &lt;code&gt;pomegranate&lt;/code&gt;'s API models after the &lt;code&gt;scikit-learn&lt;/code&gt;'s API; with the API being the user-facing interface, and &lt;code&gt;scikit-learn&lt;/code&gt; being the &lt;em&gt;de facto&lt;/em&gt; go-to library for machine learning, I'd be interested to see how much more &lt;code&gt;pomegranate&lt;/code&gt; adds to the ecosystem, particularly w.r.t. Bayesian models.&lt;/p&gt;
&lt;p&gt;There are a swathe of other good talks that I'm expecting to be able to catch online later on. &lt;strong&gt;&lt;a href=&quot;https://matthewrocklin.com/&quot;&gt;Matt Rocklin&lt;/a&gt;&lt;/strong&gt;, who is the lead developer of Dask, has done a ton of work on speeding Python up through parallelism. His talk will be on &lt;a href=&quot;https://pydata.org/nyc2017/schedule/presentation/22/&quot;&gt;the use of Cython &amp;amp; Dask to speed up GeoPandas&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Also, &lt;strong&gt;&lt;a href=&quot;https://pydata.org/nyc2017/speaker/profile/80/&quot;&gt;Thomas Caswell&lt;/a&gt;&lt;/strong&gt;, one of the &lt;a href=&quot;http://matplotlib.org/&quot;&gt;&lt;code&gt;matplotlib&lt;/code&gt;&lt;/a&gt; lead devs who helped guide my first foray into open source contributions, is giving a tutorial on &lt;a href=&quot;https://pydata.org/nyc2017/schedule/presentation/3/&quot;&gt;developing interactive figures in matplotlib&lt;/a&gt;. Highly recommended if you're into the visualization world!&lt;/p&gt;
&lt;p&gt;Finally, the always-interesting, always entertaining &lt;strong&gt;&lt;a href=&quot;https://pydata.org/nyc2017/schedule/presentation/25/&quot;&gt;en zyme&lt;/a&gt;&lt;/strong&gt; will be speaking on an &lt;a href=&quot;https://pydata.org/nyc2017/schedule/presentation/25/&quot;&gt;interesting topic&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Looking forward to being at the conference, and meeting old and new friends there!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2017/10/10/recursive-programming-and-dags/">
    <title type="text">Recursive Programming and DAGs</title>
    <id>urn:uuid:651bdb43-752a-3283-a522-17cdc7a276c7</id>
    <updated>2017-10-10T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2017/10/10/recursive-programming-and-dags/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;Over the past few days, I've found myself using recursive programming to implement a &quot;model specification&quot; system with inheritance for deep learning. The goal here is to enable reproducible computational experiments for particular deep learning hyperparameter sets. Reproducibility is something I learned from the Software/Data Carpentry initiative, thus I wanted to ensure that my own work was reproducible, even if it's not (because of corporate reasons) open-able, because it's the right thing to do.&lt;/p&gt;
&lt;p&gt;So, how do these &quot;model spec&quot; files work? I call them &quot;experiment profiles&quot;, and they specify a bunch of things: &lt;strong&gt;model architecture&lt;/strong&gt;, &lt;strong&gt;training parameters&lt;/strong&gt;, and &lt;strong&gt;data tasks&lt;/strong&gt;. These experiment profiles are stored in YAML files on disk. A profile essentially looks like the following (dummy examples provided, naturally):&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Name: default.yaml&lt;/span&gt;
&lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;parent&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;null&lt;/span&gt;
&lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;data_tasks&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;tasks&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p p-Indicator&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;task1&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;task2&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;task3&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;model_architecture&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;hidden_layers&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p p-Indicator&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;hidden_dropouts&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p p-Indicator&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;training_parameters&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;sgd&amp;quot;&lt;/span&gt;
    &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;optimizer_options&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;n_epochs&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;20&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In this YAML file, the key-value pairs essentially match the API of the tooling I've built on top of Keras' API to make myself more productive. (From the example, it should be clear that we're dealing with only feed-forward neural networks and nothing else more complicated.) The key here (pun unintended) is that I have a &lt;code&gt;parent&lt;/code&gt; key-value pair that specifies another experiment profile that I can inherit from.&lt;/p&gt;
&lt;p&gt;Let's call the above example &lt;code&gt;default.yaml&lt;/code&gt;. Let's say I want to run another computational experiment that uses the &lt;code&gt;adam&lt;/code&gt; optimizer instead of plain vanilla &lt;code&gt;sgd&lt;/code&gt;. Instead of re-specifying the entire YAML file, by implementing an inheritance scheme, I can re-specify only the optimizer and optimizer_options.&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Name: adam.yaml&lt;/span&gt;
&lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;parent&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;default.yaml&amp;quot;&lt;/span&gt;
&lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;training_parameters&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;adam&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Finally, let's say I find out that 20 epochs (inherited from &lt;code&gt;default.yaml&lt;/code&gt;) is too much for Adam - after all, Adam is one of the most efficient gradient descent algorithms out there - and I want to change it to 3 epochs instead. I can do the following:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Name: adam-3.yaml&lt;/span&gt;
&lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;parent&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;adam.yaml&amp;quot;&lt;/span&gt;
&lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;training_parameters&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;optimizer_options&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;n_epochs&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;3&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Okay, so specifying YAML files with inheritance is all good, but how do I ensure that I get the entire parameter set out correctly, without writing verbose code? This is where the power of recursive programming comes in. Using recursion, I can solve this problem with &lt;strong&gt;a single function that calls itself on one condition, and returns a result on another condition&lt;/strong&gt;. That's a recursive function in its essence.&lt;/p&gt;
&lt;p&gt;The core of this problem is traversing the inheritance path, from &lt;code&gt;adam-3.yaml&lt;/code&gt; to &lt;code&gt;adam.yaml&lt;/code&gt; to &lt;code&gt;default.yaml&lt;/code&gt;. Once I have the inheritance path specified, loading the YAML files as a dictionary becomes the easy part.&lt;/p&gt;
&lt;p&gt;How would this look like in code? Let's take a look at an implementation.&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;yaml&lt;/span&gt; 

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;inheritance_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yaml_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    :param str yaml_file: The path to the yaml file of interest.&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    :param list path: A list specifying the existing inheritance path. First&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        entry is the file of interest, and parents are recursively appended to&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        the end of the list.&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yaml_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;r+&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yaml&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;parent&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;parent&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inheritance_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;parent&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The most important part of the function is in the &lt;code&gt;if&lt;/code&gt;/&lt;code&gt;else&lt;/code&gt; block. If I have reached the &quot;root&quot; of the inheritance path, (that is, I have hit &lt;code&gt;default.yaml&lt;/code&gt; which has no parent), then I return the &lt;code&gt;path&lt;/code&gt; traversed. Otherwise, I return into the &lt;code&gt;inheritance_path&lt;/code&gt; function call again, but with an updated &lt;code&gt;path&lt;/code&gt; list, and a different &lt;code&gt;yaml_file&lt;/code&gt; to read. It's a bit like doing a &lt;code&gt;while&lt;/code&gt; loop, but in my opinion, a bit more elegant aesthetically.&lt;/p&gt;
&lt;p&gt;Once I've gotten the path list, I can finally load the parameters using a single function that calls on &lt;code&gt;inheritance_path&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;load_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yaml_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inheritance_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yaml_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yaml_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_tasks&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; 
             &lt;span class=&quot;n&quot;&gt;model_architecture&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; 
             &lt;span class=&quot;n&quot;&gt;training_parameters&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fn&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[::&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# go in reverse!&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;r+&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yaml&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;items&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;update&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This is the equivalent of traversing a Directed Acyclic Graph (DAG), or in some special cases, a tree data structure, but in a way where we don't have to know the entire tree structure ahead of time. The goal is to reach the root from any node:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;root
    |- A
        |- B
        |- C
            |- D
            |- E
    |- F
        |- G
        |- H
        |- I 
            |- J
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Also, because we only have one pointer in each YAML file to its parent, we have effectively created a &quot;Linked List&quot; that we can use to trace a path back to the &quot;root&quot; node, along the way collecting the information that we need together. By using this method of traversal, we only need to know the neighbors, and at some point (however long it takes), we will reach the root.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;D -&amp;gt; C -&amp;gt; A -&amp;gt; root
E -&amp;gt; C -&amp;gt; A -&amp;gt; root
J -&amp;gt; I -&amp;gt; F -&amp;gt; root
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you were wondering why linked lists, trees and other data structures might be useful as a data scientist, I hope this illustrates on productive example!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2017/10/7/a-data-scientists-guide-to-environment-variables/">
    <title type="text">A Data Scientist's Guide to Environment Variables</title>
    <id>urn:uuid:3a3bceb7-58a5-3816-8a72-3cbffe8f4742</id>
    <updated>2017-10-07T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2017/10/7/a-data-scientists-guide-to-environment-variables/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;You might have encountered a piece of software asking you for permission to modify your &lt;code&gt;PATH&lt;/code&gt; variable, or another program's installation instructions cryptically telling you that you have to &quot;set your &lt;code&gt;LD_LIBRARY_PATH&lt;/code&gt; variable correctly&quot;.&lt;/p&gt;
&lt;p&gt;As a data scientist, you might encounter other environment variable issues when interacting with your compute stack (particularly if you don't have full control over it, like I do). This post is meant to demystify what an environment variable is, and how it gets used in a data science context.&lt;/p&gt;
&lt;h2 id=&quot;what-is-an-environment-variable?&quot;&gt;What Is An Environment Variable?&lt;/h2&gt;&lt;p&gt;First off, let me explain what an environment variable is, by going in-depth into the &lt;code&gt;PATH&lt;/code&gt; environment variable. I'd encourage you to execute the commands here inside your bash terminal (with appropriate modifications -- read the text to figure out what I'm doing!).&lt;/p&gt;
&lt;p&gt;When you log into your computer system, say, your local computer’s terminal or your remote server via SSH, your bash interpreter needs to know where to look for particular programs, such as &lt;code&gt;nano&lt;/code&gt; (the text editor), or &lt;code&gt;git&lt;/code&gt; (your version control software), or your Python executable. This is controlled by your PATH variable. It specifies the paths to folders where your executable programs are found.&lt;/p&gt;
&lt;p&gt;By historical convention, command line programs, such as &lt;code&gt;nano&lt;/code&gt;, &lt;code&gt;which&lt;/code&gt;, and &lt;code&gt;top&lt;/code&gt;, are found in the directory &lt;code&gt;/usr/bin&lt;/code&gt;. (By historical convention, the &lt;code&gt;/bin&lt;/code&gt; folder is for software binaries, which is why they are named &lt;code&gt;/bin&lt;/code&gt;.) These are the ones that are bundled with your operating system, and as such, need special permissions to upgrade.&lt;/p&gt;
&lt;p&gt;Try it out in your terminal:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ which which
/usr/bin/which
$ which top
/usr/bin/top
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Other programs are installed (for whatever reason) into &lt;code&gt;/bin&lt;/code&gt; instead. &lt;code&gt;ls&lt;/code&gt; is one example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ which ls
/bin/ls
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Yet other programs might be installed in other special directories:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ which nano
/usr/local/bin/nano
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How does your Bash terminal figure out where to go to look for stuff? It uses the &lt;code&gt;PATH&lt;/code&gt; environment variable. It looks something like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ echo $PATH
/usr/bin:/bin:/usr/local/bin
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The most important thing to remember about the &lt;code&gt;PATH&lt;/code&gt; variable is that it is &quot;colon-delimited&quot;. That is, each directory path is separated by the next using a &quot;colon&quot; (&lt;code&gt;:&lt;/code&gt;) character. The order in which your bash terminal is looking for programs goes from left to right:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;/usr/bin&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/bin&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/usr/local/bin&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;On my particular computer, when I type in &lt;code&gt;ls&lt;/code&gt;, my bash interpreter will look inside the &lt;code&gt;/usr/bin&lt;/code&gt; directory first. It'll find that &lt;code&gt;ls&lt;/code&gt; doesn't exist in &lt;code&gt;/usr/bin&lt;/code&gt;, and so it'll move to the next directory, &lt;code&gt;/bin&lt;/code&gt;. Since my &lt;code&gt;ls&lt;/code&gt; exists under &lt;code&gt;/bin&lt;/code&gt;, it'll execute the &lt;code&gt;ls&lt;/code&gt; program from there.&lt;/p&gt;
&lt;p&gt;You can see, then, that this is simultaneously super flexible for customizing your compute environment, yet also potentially super frustrating if a program modified your &lt;code&gt;PATH&lt;/code&gt; variable without you knowing.&lt;/p&gt;
&lt;p&gt;Wait, you can actually modify your &lt;code&gt;PATH&lt;/code&gt; variable? Yep, and there's a few ways to do this.&lt;/p&gt;
&lt;h2 id=&quot;how-to-modify-the-path-variable&quot;&gt;How To Modify the &lt;code&gt;PATH&lt;/code&gt; variable&lt;/h2&gt;&lt;h3 id=&quot;using-a-bash-session&quot;&gt;Using a Bash Session&lt;/h3&gt;&lt;p&gt;The first way is transient, or temporary, and only occurs for your particular bash session. You can make a folder have higher priority than the existing paths by &quot;pre-pending&quot; it to the &lt;code&gt;PATH&lt;/code&gt; variable:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ export PATH=/path/to/my/folder:$PATH
$ echo $PATH
/path/to/my/folder:/usr/bin:/bin:/usr/local/bin
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or I can make it have a lower priority than existing paths by &quot;appending&quot; it to the &lt;code&gt;PATH&lt;/code&gt; variable:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ export PATH=$PATH:/path/to/my/folder
$ echo $PATH
/usr/bin:/bin:/usr/local/bin:/path/to/my/folder
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The reason this is temporary is because I only export it during my current bash session.&lt;/p&gt;
&lt;h3 id=&quot;bashrc-or-.bash_profile-file&quot;&gt;&lt;code&gt;bashrc&lt;/code&gt; or &lt;code&gt;.bash_profile&lt;/code&gt; File&lt;/h3&gt;&lt;p&gt;If I wanted to make my changes somewhat more permanent, then I would include inside my &lt;code&gt;.bashrc&lt;/code&gt; or &lt;code&gt;.bash_profile&lt;/code&gt; file. (I recommend using the &lt;code&gt;.bashrc&lt;/code&gt; file.) The &lt;code&gt;.bashrc&lt;/code&gt;/&lt;code&gt;.bash_profile&lt;/code&gt; file lives inside your home directory (your &lt;code&gt;$HOME&lt;/code&gt; environment variable specifies this), and is a file that your bash interpreter will execute first load. It will execute all of the commands inside there. This means, you can change your PATH variable by simply putting inside your &lt;code&gt;.bashrc&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;...other stuff above...
# Make /path/to/folder have higher priority
export PATH=/path/to/folder:$PATH

# Make /path/to/other/folder have lower priority
export PATH=$PATH:/path/to/folder
...other stuff below...
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&quot;data-science-and-the-path-environment-variable&quot;&gt;Data Science and the &lt;code&gt;PATH&lt;/code&gt; environment variable&lt;/h2&gt;&lt;p&gt;Now, &lt;strong&gt;how is this relevant to data scientists?&lt;/strong&gt; Well, if you're a data scientist, chances are that you use Python, and that your Python interpreter comes from the Anaconda Python distribution (a seriously awesome thing, go get it!). What the Anaconda Python installer does is prioritize the &lt;code&gt;/path/to/anaconda/bin&lt;/code&gt; folder in the &lt;code&gt;PATH&lt;/code&gt; environment variable. You might have other Python interpreters installed on your system (that is, Apple ships its own). However, this &lt;code&gt;PATH&lt;/code&gt; modification ensures that each time you type &lt;code&gt;python&lt;/code&gt; into your Bash terminal, you execute the Python interpreter shipped with the Anaconda Python distribution. In my case, after installing the Anaconda Python distribution, my &lt;code&gt;PATH&lt;/code&gt; looks like:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ echo $PATH
/Users/ericmjl/anaconda/bin:/usr/bin:/bin:/usr/local/bin
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Even better, what conda environments do is prepend the path to the conda environment binaries folder while the environment is activated. For example, with my blog, I keep it in an environment named &lt;code&gt;lektor&lt;/code&gt;. Thus...&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ echo $PATH
/Users/ericmjl/anaconda/bin:/usr/bin:/bin:/usr/local/bin
$ which python
/Users/ericmjl/anaconda/bin/python
$ source activate lektor
$ echo $PATH
/Users/ericmjl/anaconda/envs/lektor/bin:/Users/ericmjl/anaconda/bin:/usr/bin:/bin:/usr/local/bin
$ which python
/Users/ericmjl/anaconda/envs/lektor/bin/python
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice how the bash terminal now preferentially picks the Python inside the higher-priority &lt;code&gt;lektor&lt;/code&gt; environment.&lt;/p&gt;
&lt;p&gt;If you've gotten to this point, then you'll hopefully realize there's a few important concepts listed here. Let's recap them:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;PATH&lt;/code&gt; is an environment variable stored as a plain text string used by the bash interpreter to figure out where to find executable programs.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;PATH&lt;/code&gt; is colon-delimited; higher priority directories are to the left of the string, while lower priority directories are to the right of the string.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;PATH&lt;/code&gt; can be modified by prepending or appending directories to the environment variable. It can be done transiently inside a bash session by running the &lt;code&gt;export&lt;/code&gt; command at the command prompt, or it can be done permanently across bash sessions by adding an &lt;code&gt;export&lt;/code&gt; line inside your &lt;code&gt;.bashrc&lt;/code&gt; or &lt;code&gt;.bash_profile&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;other-environment-variables-of-interest&quot;&gt;Other Environment Variables of Interest&lt;/h2&gt;&lt;p&gt;Now, what other environment variables might a data scientist encounter? These are a sampling of them that you might see, and might have to fix, especially in contexts where your system administrators are off on vacation (or taking too long to respond).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;For general use&lt;/strong&gt;, you'll definitely want to know where your &lt;code&gt;HOME&lt;/code&gt; folder is -- on Linux systems, it's often &lt;code&gt;/home/username&lt;/code&gt;, while on macOS systems, it's often &lt;code&gt;/Users/username&lt;/code&gt;.  You can figure out what &lt;code&gt;HOME&lt;/code&gt; is by doing:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ echo $HOME
/Users/ericmjl
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;If you're a Python user&lt;/strong&gt;, then the &lt;code&gt;PYTHONPATH&lt;/code&gt; is one variable that might be useful. It is used by the Python interpreter, and specifies where to find Python modules/packages.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If you have to deal with C++ libraries&lt;/strong&gt;, then knowing your &lt;code&gt;LD_LIBRARY_PATH&lt;/code&gt; environment variable is going to be very important. I'm not well-versed enough in this to espouse on it intelligently, so I would defer to &lt;a href=&quot;http://xahlee.info/UnixResource_dir/_/ldpath.html&quot;&gt;this website&lt;/a&gt; for more information on best practices for using the &lt;code&gt;LD_LIBRARY_PATH&lt;/code&gt; variable.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If you're working with Spark&lt;/strong&gt;, then the &lt;code&gt;PYSPARK_PYTHON&lt;/code&gt; environment variable would be of interest. This essentially tells Spark which Python to use for both its driver and its workers; you can also set the &lt;code&gt;PYSPARK_DRIVER_PYTHON&lt;/code&gt; to be separate from the &lt;code&gt;PYSPARK_PYTHON&lt;/code&gt; environment variable, if needed.&lt;/p&gt;
&lt;h3 id=&quot;hack-your-environment-variables&quot;&gt;Hack Your Environment Variables&lt;/h3&gt;&lt;p&gt;This is where the most fun happens! Follow along for some stuff you might be able to do by hacking your environment variables.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Hack #1: Enable access to PyPy.&lt;/strong&gt; I occasionally keep up with the development of PyPy, but because PyPy is not yet the default Python interpreter, and is not yet &lt;code&gt;conda install&lt;/code&gt;-able, I have to put it in its own &lt;code&gt;$HOME/pypy/bin&lt;/code&gt; directory. To enable access to the PyPy interpreter, I have to make sure that my &lt;code&gt;/path/to/pypy&lt;/code&gt; is present in the &lt;code&gt;PATH&lt;/code&gt; environment variable, but at a lower priority than my regular CPython interpreter.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Hack #2: Enable access to other language interpreters/compilers.&lt;/strong&gt; This is analogous to PyPy. I once was trying out Lua's JIT interpreter to use Torch for deep learning, and needed to add a path to there in my &lt;code&gt;.bashrc&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Hack #3: Install Python packages to your home directory.&lt;/strong&gt; On shared Linux compute systems that use the &lt;code&gt;modules&lt;/code&gt;  system rather than &lt;code&gt;conda&lt;/code&gt; environments, a &lt;code&gt;modulefile&lt;/code&gt; that you load might be configured with a virtual environment that &lt;em&gt;you don't have permissions to modify&lt;/em&gt;. If you need to install a Python package, you might want to &lt;code&gt;pip install --user my_pkg_name&lt;/code&gt;. This will install it to &lt;code&gt;$HOME/.local/lib/python-[version]/site-packages/&lt;/code&gt;. Ensuring that your &lt;code&gt;PYTHONPATH&lt;/code&gt; includes &lt;code&gt;$HOME/.local/lib/python-[version]/site-packages&lt;/code&gt; at a high enough priority is going to be important in this case.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Hack 4: Debugging when things go wrong.&lt;/strong&gt; In case something throws an error, or you have unexpected behaviour -- something I encountered before was my Python interpreter not being found correctly after loading all of my Linux modules -- then a way to debug is to temporarily set your PATH environment variable to some sensible &quot;defaults&quot; and sourcing that, effectively &quot;resetting&quot; your PATH variable, so that you can manually prepend/append while debugging.&lt;/p&gt;
&lt;p&gt;To do this, place the following line inside a file named &lt;code&gt;.path_default&lt;/code&gt;, inside your home directory:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;export PATH=&quot;&quot;  # resets PATH to an empty string.
export PATH=/usr/bin:/bin:/usr/local/bin:$PATH  # this is a sensible default; customize as needed.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After something goes wrong, you can reset your PATH environment variable by using the &quot;source&quot; command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ echo $PATH
/some/complicated/path:/more/complicated/paths:/really/complicated/paths
$ source ~/.path_default
$ echo $PATH
/usr/bin:/bin:/usr/local/bin
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note - you can also execute the exact same commands inside your bash session; the interactivity may also be helpful.&lt;/p&gt;
&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;&lt;p&gt;I hope you enjoyed this article, and that it'll give you a, ahem, path forward whenever you encounter these environment variables!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2017/10/3/new-habits/">
    <title type="text">New Habits</title>
    <id>urn:uuid:b6183a04-fde9-32ea-8fad-fdefbc3bd4c1</id>
    <updated>2017-10-03T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2017/10/3/new-habits/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;Ever since &quot;going corporate&quot;, it's meant picking up more new productivity/coding habits. Here's a sampling of what I've learned.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;(1) Living by my calendar&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Basically, the &quot;work calendar&quot; defines everything about the day. I've had to make sure that if I am not going to be pencilled in for a meeting, I have to block out time on the calendar first.&lt;/p&gt;
&lt;p&gt;Also, sending invites to people + rooms -- the latter being the newest habit I've had to pick up.&lt;/p&gt;
&lt;p&gt;Finally, setting informative titles for calendar events - if I want to have coffee or lunch with X, I can't just write &quot;Coffee with X&quot; - it literally shows up as &quot;Coffee with X&quot; on X's calendar, which is super awkward, as if they're having coffee with themselves! Something more informative, like, &quot;Eric &amp;lt;&amp;gt; X coffee&quot; really helps the other person, who might be super busy and thus only glances at their calendar once in a while.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;(2) Flagging emails and applying rules&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Emails fly everywhere. It gets super overwhelming after a while.&lt;/p&gt;
&lt;p&gt;If there's stuff that needs to be followed-up on, it stays in my Inbox until it's done. It also gets flagged, which automatically creates a Todo on my task list. (If this sounds like Outlook - yes, it's Outlook. On macOS. With no &quot;email snoozing&quot; feature...)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;(3) Hacking through legacy code and shared compute resources&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Working on a compute cluster with a code base that's built for legacy versions of programming languages is super frustrating! Thankfully I know enough about the differences between Python 2 and Python 3 to hack my way through.&lt;/p&gt;
&lt;p&gt;Shared compute resources means using &lt;code&gt;modules&lt;/code&gt;, but not everybody sets up &lt;code&gt;modules&lt;/code&gt; with the same set of assumptions as others. Some create virtual environments inside a module, others append to &lt;code&gt;$PATH&lt;/code&gt;, and getting the right combinations in a modular way is really tricky. It means I have some really painful one-off &lt;code&gt;$PATH&lt;/code&gt; hacks to make stuff work. Documentation is paramount - without putting in docs, I'll never remember what I did...&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;(4) Adapting to others' coding styles&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Not everybody is a Python programmer, and not everybody is a Pythonic programmer. The usual Python idioms that I'm used to (whether functional or object-oriented) sometimes get thrown out in favour of some other style (globals, anybody?), and I have to adapt to figure out what's going on. Thankfully my colleagues are open to me modifying their code, as long as I can demonstrate that the new version works fine, and I've been working hard to bring in Pythonic code style.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;(5) Performance reviews&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Gotta start getting used to this. I had a taste of it while volunteering as part of Tang Hall's student leadership, but now it's for real.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2017/9/14/visualize-large-datasets-by-sampling/">
    <title type="text">Visualize Large Datasets by Sampling</title>
    <id>urn:uuid:cdb3d3cb-bfc1-32a6-80cf-a71b1ee56b14</id>
    <updated>2017-09-14T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2017/9/14/visualize-large-datasets-by-sampling/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;Just a little tip, putting it here for myself and others in case it helps.&lt;/p&gt;
&lt;p&gt;Sometimes, you need to visualize a large dataset, but it takes a ton of time to render it or compute the necessary transforms.&lt;/p&gt;
&lt;p&gt;If your samples are statistically sampled independently of one another (i.e. basically not timeseries), and the goals are to do some statistical visualizations, then it's basically valid to visualize a downsampled set of the dataset.&lt;/p&gt;
&lt;p&gt;I recently encountered this point at work. After running a clustering analysis, I wanted to see a pair plot of the distribution of features in each cluster. However, with cluster sizes ranging from 200-2 million, rendering times were unreasonably long (making things non-interactive) for the large sized clusters. I thus decided to downsample the large clusters to a maximum of 2,000 data points. Instantly, render times improved, and I could start interacting with my data again.&lt;/p&gt;
&lt;p&gt;Little things matter!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2017/9/11/nano-text-editor-hacks/">
    <title type="text">nano text editor hacks</title>
    <id>urn:uuid:6027e651-57d0-305a-a2d4-cf3162beaa87</id>
    <updated>2017-09-11T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2017/9/11/nano-text-editor-hacks/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;Much as I've embraced the &lt;a href=&quot;https://atom.io/&quot;&gt;Atom text editor&lt;/a&gt;, there are times when the GUI isn't accessible to us, and we are forced to use a Terminal-based text editor.&lt;/p&gt;
&lt;p&gt;Now, I'm not one of those crazy types who use emacs or vim - those are the real seasoned pros. (I still don't know how to exit vim, btw.) As such, my terminal editor of choice remains the venerable &lt;code&gt;nano&lt;/code&gt;. Here's some hacks that I recently figured out, to make text editing much easier in &lt;code&gt;nano&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;(1) Syntax highlighting&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is such a big one! Syntax highlighting seriously helps a ton. If you're on a Mac, make sure you install &lt;code&gt;homebrew&lt;/code&gt;'s version of &lt;code&gt;nano&lt;/code&gt; - you can look at my &lt;a href=&quot;https://github.com/ericmjl/dotfiles/blob/master/install.sh#L41&quot;&gt;dotfiles&lt;/a&gt; or run the command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ brew install nano
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, edit your &lt;code&gt;~/.nanorc&lt;/code&gt; file to look something like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;include /usr/local/share/nano/python.nanorc  # gives you Python syntax highlighting
include /usr/local/share/nano/sh.nanorc  # gives you bash shell syntax highlighting
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next time you use &lt;code&gt;nano&lt;/code&gt; (from your user account), syntax highlighting should be enabled!&lt;/p&gt;
&lt;p&gt;You can find a sample &lt;a href=&quot;https://github.com/ericmjl/dotfiles/blob/master/.nanorc-mac&quot;&gt;.nanorc&lt;/a&gt; file on my GitHub &lt;a href=&quot;https://github.com/ericmjl/dotfiles/&quot;&gt;dotfiles&lt;/a&gt; repository&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;(2) Keyboard Shortcuts&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Here's a laundry list of keyboard shortcuts I've muscle-memorized:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Ctrl-x&lt;/code&gt;: quits. There will be a prompt to save the file if it's been modified.&lt;ul&gt;
&lt;li&gt;I usually end up doing &lt;code&gt;Ctrl-x-y-Enter&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Ctrl-v&lt;/code&gt; scrolls down a page&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Ctrl-y&lt;/code&gt; scrolls up a page&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Ctrl-w&lt;/code&gt; searches the document for a term that you type in (think &quot;where&quot;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Ctrl-k&lt;/code&gt; cuts the line&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Ctrl-u&lt;/code&gt; pastes a cut line&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Ctrl-^&lt;/code&gt; (i.e. &lt;code&gt;Ctrl-Shift-6&lt;/code&gt; on macOS keyboards) starts a &quot;select&quot; cursor.&lt;ul&gt;
&lt;li&gt;You can use arrow keys to expand or shrink the selection, which can then be cut and pasted.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Ctrl-c&lt;/code&gt; cancels any commands that are 'active'.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Ctrl-o&lt;/code&gt; activates the &quot;save file&quot; dialogue - lets you save your state without quitting.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;(3) Persistence&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;nano&lt;/code&gt;, being not as fancy as &lt;code&gt;vim&lt;/code&gt; or &lt;code&gt;emacs&lt;/code&gt;, means it doesn't have the concept of sessions. Doesn't matter - use &lt;a href=&quot;https://github.com/tmux/tmux/wiki&quot;&gt;&lt;code&gt;tmux&lt;/code&gt;&lt;/a&gt; to persist!&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;All-in-all, the biggest one that aids in writing on a terminal editor is syntax highlighting. I wrote this blog post in &lt;code&gt;nano&lt;/code&gt;, and being able to visually see different parts of my text highlighted according to their meaning has made writing much easier.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2017/8/31/what-would-be-useful-for-aspiring-data-scientists-to-know/">
    <title type="text">What would be useful for aspiring data scientists to know?</title>
    <id>urn:uuid:7512a55f-c44f-3d25-b874-e1656a22e54d</id>
    <updated>2017-08-31T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2017/8/31/what-would-be-useful-for-aspiring-data-scientists-to-know/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;I originally titled this post, &quot;What you need to know to become a data scientist&quot;, but I backed off from having such an authoritative post title for I wanted to keep things opinionated without being pompous :).&lt;/p&gt;
&lt;p&gt;Data Science (DS) is a hot field, and I'm going to be starting my new role doing DS at Novartis in September. As an aside, what makes me most happy about this role is that I'm going to do DS in the context of the life sciences (one of the &quot;hard sciences&quot;)!&lt;/p&gt;
&lt;p&gt;Now that I have secured a role, some people have come to ask me questions about how I made the transition into DS and into the industry in general. I hope to provide answers to those questions in this blog post, and that you, the reader, find it useful.&lt;/p&gt;
&lt;p&gt;I will structure this blog post into two sections:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;What do I need to know and how do I go about it?&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;What do I need to do?&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Ready? Here we go :)&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;First off, let's talk about what I think you, an aspiring data scientist, needs to know, and how to go about learning it.&lt;/p&gt;
&lt;h3 id=&quot;topic-1:-statistical-learning&quot;&gt;Topic 1: Statistical Learning&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Statistical learning methods&lt;/strong&gt; are going to top the list. From the standpoint of &quot;topics to learn&quot;, there's a laundry list one can write - all of the ML methods in &lt;code&gt;scikit-learn&lt;/code&gt;, neural networks, statistical inference methods and more. It's also very tempting to go through that laundry list of terms, learn how they work underneath, and call it a day there. I think that's all good, but only if that material is learned while in the service of picking up the meta-skill of &lt;strong&gt;statistical thinking&lt;/strong&gt;. This includes:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Thinking about data as being sampled from a generative model parameterized by probability distributions (my Bayesian fox tail is revealed!), &lt;/li&gt;
&lt;li&gt;Identifying biases in the data and figuring out how to use sampling methods to help correct those biases (e.g. bootstrap resampling, downsampling), and &lt;/li&gt;
&lt;li&gt;Figuring out when your data are garbage enough that you shouldn't proceed with inference and instead think about experimental design.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;That meta-skill of statistical thinking can only come with practice. Some only need a few months, some need a few years. (I needed about a year's worth of self-directed study during graduate school to pick it up.) &lt;strong&gt;&lt;em&gt;Having a project that involves this is going to be key!&lt;/em&gt;&lt;/strong&gt; A good introduction to statistical thinking for data science can be found in a &lt;a href=&quot;https://www.youtube.com/watch?v=TGGGDpb04Yc&quot;&gt;SciPy 2015 talk by Chris Fonnesbeck&lt;/a&gt;, and working through the two-part computational statistics tutorial by him and Allen Downey (&lt;a href=&quot;https://www.youtube.com/watch?v=fMycLa1bsno&quot;&gt;Part 1&lt;/a&gt;, &lt;a href=&quot;https://www.youtube.com/watch?v=heFaYLKVZY4&quot;&gt;Part 2&lt;/a&gt;) helped me a ton.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recommendation &amp;amp; Personal Story:&lt;/strong&gt; Nothing beats practice. This means finding ways to apply statistical learning methods to projects that you already work on, or else coming up with new projects to try. I did this in graduate school: my main thesis project was not a machine learning-based project. However, I found a great &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3794897/&quot;&gt;PLoS Computational Biology paper&lt;/a&gt; implementing Random Forests to identify viral hosts from protein sequence, and it was close enough in research topic that I spent two afternoons re-implementing it using &lt;code&gt;scikit-learn&lt;/code&gt;, and presenting it during our lab's Journal Club session. I then realized the same logic could be applied to predicting drug resistance from protein sequence, and re-implemented a few other HIV drug resistance papers before finally learning and applying a fancier deep learning-based method that had been developed at Harvard to the same problem.&lt;/p&gt;
&lt;h3 id=&quot;topic-2:-software-engineering&quot;&gt;Topic 2: Software Engineering&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Software engineering&lt;/strong&gt; (SE), to the best of my observation, is about three main things: (a) learning how to abstract and organize ideas in a way that is logical and humanly accessible, (b) writing good code that is well-tested and documented, and (c) being familiar with the ever-evolving ecosystem of packages. SE is important for a data scientist, because models that are making predictions often are put into production systems and used beyond just the DS themselves.&lt;/p&gt;
&lt;p&gt;Now, I don't think a data scientist has to be a seasoned software engineer, as most companies have SE teams that a data scientist can interface with. However, having some experience building a software product can be &lt;em&gt;very helpful&lt;/em&gt; for lubricating the interaction between DS and SE teams. Having a logical structure to your code, writing basic tests for it, and providing sufficiently detailed documentation, are all things that SE types will very much appreciate, and it'll make life much easier for them when coming to code deployment and helping with maintenance. (Aside: I strongly believe a DS should take primary responsibility for maintenance, and &lt;em&gt;not&lt;/em&gt; the SE team, and only rely on the SE team as a fallback, say, when people are sick or on vacation.)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recommendation &amp;amp; Personal Story:&lt;/strong&gt; Again, nothing beats practice here. Working on your own projects, whether work-related or not, will help you get a feel for these things. I learned my software engineering concepts from participating in open source contributions. The first was a contribution to &lt;code&gt;matplotlib&lt;/code&gt; documentation, where I first got to use Git (a version control system) and Travis CI (a continuous integration system). It was there that I also got my first taste of software testing. The next year, I quickly followed it up with a small contribution to &lt;code&gt;bokeh&lt;/code&gt;, and then decided at SciPy 2016 to build &lt;code&gt;nxviz&lt;/code&gt; for my Network Analysis Made Simple tutorials. &lt;code&gt;nxviz&lt;/code&gt; became my first independent software engineering project, and also my &quot;capstone&quot; project for that year of learning. All-in-all, getting practice was instrumental for my learning process.&lt;/p&gt;
&lt;h3 id=&quot;topic-3:-industry-specific-business-cases&quot;&gt;Topic 3: Industry-Specific Business Cases&lt;/h3&gt;&lt;p&gt;This is something I learned from my time at Insight, and is non-negotiable. Data Science does not exist in a vacuum; it is primarily in the service of solving business problems. At Insight, Fellows get exposure to business case problems from a variety of industries, thanks to the Program Directors' efforts in collecting feedback from Insight alumni who are already Data Scientists in the industry.&lt;/p&gt;
&lt;p&gt;I think business cases show up in interviews as a test of a candidate's &lt;strong&gt;imaginative capacity and/or experience&lt;/strong&gt;: can the candidate demonstrate (a) the creativity needed in solving tough business problems, and (b) the passion for solving those problems? Neither of these are easy to fake when confronted with a well-designed business case. In my case, it was tough for me to get excited about data science in an advertisement technology firm, and was promptly rejected right after an on-site business case.&lt;/p&gt;
&lt;p&gt;It's important to note that these business cases are very industry specific. Retail firms will have a distinct need from marketing firms, and both will be very distinct from healthcare and pharmaceutical companies.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recommendation &amp;amp; Personal Story:&lt;/strong&gt; For aspiring data scientists, I recommend prioritizing the general industry area that you're most interested in targeting. After that, start going to meet-ups and talking with people about the kinds of problems they're solving - for example, I started going to a &lt;a href=&quot;../../../../../blog/2017/8/31/what-would-be-useful-for-aspiring-data-scientists-to-know/#&quot;&gt;Quantitative Systems Pharmacology&lt;/a&gt; meet-up to learn more about quantitative problems in the pharma research industry; I also presented a talk &amp;amp; poster at a conference organized by Applied BioMath, where I knew lots of pharma scientists would be present. I also started reading through scientific journals (while I still had access to them through the MIT Libraries), and did a lot of background reading on the kinds of problems being solved in drug discovery.&lt;/p&gt;
&lt;h3 id=&quot;topic-4:-cs-fundamentals&quot;&gt;Topic 4: CS Fundamentals&lt;/h3&gt;&lt;p&gt;CS fundamentals really means things like algorithms and data structures. I didn't do much to prepare for this. The industry I was targeting didn't have a strong CS legacy/tradition, unlike most other technology firms doing data science (think the Facebooks, Googles, and Amazons), which do. Thus, I think CS fundamentals are mostly important for cracking interviews, and while problems involving CS fundamentals certainly can show up at work, unless something changes, they probably won't occupy a central focus of data science roles for a long time.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recommendation &amp;amp; Personal Story:&lt;/strong&gt; As I don't really like &quot;studying to the test&quot;, I didn't bother with this - but that also meant I was rejected from tech firms that I did apply to (e.g. I didn't pass Google Brain's phone interview). Thus, if you're really interested in those firms, you'll probably have to spend a lot of time getting into the core data structures in computer science (not just Python). Insight provided a great environment for us Fellows to learn these topics; that said, it's easy to over-compensate and neglect the other topics. Prioritize accordingly - based on your field/industry of experience.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Now, let's talk about things you can start doing from now on that will snowball your credibility for entering into a role in data science. To be clear, these recommendations are made with a year-long time horizon in mind - these are not so much &quot;crack-the-interview&quot; tips as they are &quot;prepare yourself for the transition&quot; strategies.&lt;/p&gt;
&lt;h3 id=&quot;strategy-1:-create-novel-and-useful-material-and-share-it-freely&quot;&gt;Strategy 1: Create novel and useful material, and share it freely&lt;/h3&gt;&lt;p&gt;This is very important, as it builds a personal portfolio of projects that showcase your technical skill. A friend of mine, Will Wolf, did a self-directed &lt;a href=&quot;http://willwolf.io/2016/07/29/my-open-source-machine-learning-masters-in-casablanca-morocco/&quot;&gt;Open Source Masters&lt;/a&gt;, where he not only delved deeply into learning data science topics, but also set about &lt;a href=&quot;http://willwolf.io/&quot;&gt;writing blog posts&lt;/a&gt; that explained tough and difficult concepts for others to understand, and showcased data projects that he was hacking on while learning his stuff.&lt;/p&gt;
&lt;p&gt;Another friend of mine, Jon Charest, wrote a blog post doing a &lt;a href=&quot;http://jonchar.net/2016/05/20/exploring-metal-subgenres-with-python.html&quot;&gt;network analysis about metal bands&lt;/a&gt; and their shared genre labels - along the way producing a great Jupyter Notebook and network visualizations that yielded contributions to &lt;code&gt;nxviz&lt;/code&gt;! Starting with that project, he did a few more, and eventually landed a role as a data scientist at Mathworks.&lt;/p&gt;
&lt;p&gt;Apart from blog posts, giving technical talks is another great way to showcase your technical mastery. I had created the Network Analysis Made Simple tutorials, inspired by Allen Downey's X Made Simple series, as a way of solidifying my knowledge on graph theory and complex systems, and a very nice side product was recognition that I had capabilities in computation, resulting in more opportunities - my favourite being becoming a DataCamp instructor on Network Analysis!&lt;/p&gt;
&lt;p&gt;A key here is to create materials that are &lt;strong&gt;accessible&lt;/strong&gt;.  Academic conferences likely won't cut it for accessibility - they're often not recorded, and not published to the web, meaning people can't find it. On the other hand, blog posts are publicly accessible, as are PyCon/SciPy/JupyterCon/PyData videos. Another key is to produce &lt;strong&gt;novel&lt;/strong&gt; material - simple rehashes aren't enough; they have to bring value to someone else's. Your materials only count if people can find you and they expand someone's knowledge.&lt;/p&gt;
&lt;p&gt;A few other data scientists, I think, will concur very strongly with this point; Brandon Rorher has an &lt;a href=&quot;https://brohrer.github.io/imposter_syndrome.html&quot;&gt;excellent blog post&lt;/a&gt; on this.&lt;/p&gt;
&lt;h3 id=&quot;strategy-2:-talk-with-people-inside-and-adjacent-to-industries-that-you-re-interested-in.&quot;&gt;Strategy 2: Talk with people inside and adjacent to industries that you're interested in.&lt;/h3&gt;&lt;p&gt;The importance of learning from other people cannot be understated. If you're releasing novel and accessible material, then you'll find this one to be much easier, as your credibility w.r.t. technical mastery will already be there - you'll have opportunities to bring value to industry insiders, and you can take that opportunity to get inside information on the kinds of problems that are being solved there. That can really help you strategize the kinds of new material that you make, which feeds back into a positive cycle.&lt;/p&gt;
&lt;p&gt;Talking with people in adjacent industries and beyond is also very important. I think none put it better than Francois Chollet in his tweet:&lt;/p&gt;
&lt;p&gt;&lt;blockquote class=&quot;twitter-tweet&quot; data-lang=&quot;en&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;It&amp;#39;s better to be curious about many things beyond your field -- the more topics you&amp;#39;ve explored, the broader your inspiration in your field&lt;/p&gt;&amp;mdash; François Chollet (@fchollet) &lt;a href=&quot;https://twitter.com/fchollet/status/903103206812655621&quot;&gt;August 31, 2017&lt;/a&gt;&lt;/blockquote&gt;&lt;/p&gt;
&lt;script async src=&quot;//platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;&lt;p&gt;The main thing here is to have a breadth of ideas to draw on for inspiration when solving your own problem at hand. I had a first-hand taste of it when trying to solve the drug resistance problem (see above) - which turned out to be my introduction into the deep learning world proper!&lt;/p&gt;
&lt;h3 id=&quot;strategy-3:-learn-python&quot;&gt;Strategy 3: Learn Python&lt;/h3&gt;&lt;p&gt;Yes, I put this as a strategy rather than as a topic, mainly because programming languages are kind of arbitrary, and as such are less about whether a language is superior to others and more about whether you can get stuff done with that language.&lt;/p&gt;
&lt;p&gt;I suggest Python only because I've tasted for myself the triumphant feeling of being able to do all of the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;environment setup (&lt;code&gt;conda&lt;/code&gt;), &lt;/li&gt;
&lt;li&gt;data extraction and cleaning (&lt;code&gt;pandas&lt;/code&gt;) &lt;/li&gt;
&lt;li&gt;modelling (&lt;code&gt;scikit-learn&lt;/code&gt;, &lt;code&gt;PyMC3&lt;/code&gt;, &lt;code&gt;keras&lt;/code&gt;) &lt;/li&gt;
&lt;li&gt;visualization (&lt;code&gt;matplotlib&lt;/code&gt;, &lt;code&gt;bokeh&lt;/code&gt;, &lt;code&gt;searborn&lt;/code&gt;), &lt;/li&gt;
&lt;li&gt;deployment (&lt;code&gt;Flask&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;in one language. That's right - one language! (Sprinkling in a bit of HTML/CSS/JS in deployment, and bash in environment setup, of course.)&lt;/p&gt;
&lt;p&gt;There's very few languages with the flexibility of Python, and having a team converse in one language simply reduces that little bit of friction that comes from reading another language. There's a ton of productivity gains to be had! It's not the fastest, it's not the most elegant, but over the years, it's adopted the right ideas and built a large community developers, as such many people have built on it and used it to solve all manners of problems they're facing - heck, I even found a package that converts between traditional and simplified Chinese!&lt;/p&gt;
&lt;p&gt;It takes time to learn the language well enough to write good code with it, and nothing beats learning Python apart from actually building a project with it - I hope this idea of &quot;building stuff&quot; is now something ingrained in you after reading this post!&lt;/p&gt;
&lt;h3 id=&quot;strategy-4:-find-a-community-of-people&quot;&gt;Strategy 4: Find a community of people&lt;/h3&gt;&lt;p&gt;When it comes to building a professional network and making friends, nothing beats going through a shared experience of thick &amp;amp; thin together with other people. Data science, being a really new thing, is a growing community of people, and being plugged into the community is going to be important for learning new things.&lt;/p&gt;
&lt;p&gt;The Insight Summer 2017 class did this - we formed a closely-knit community of aspiring data scientists, cheered each other on, and coached each other on topics that were of interest. I know that this shared experience with other Insighters will give us a professional network that we can tap into in the future!&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h3&gt;&lt;p&gt;Alrighty, to conclude, here's the topics and strategies outlined above.&lt;/p&gt;
&lt;p&gt;Topics to learn:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Must-have:&lt;/strong&gt; Statistical learning &amp;amp; statistical thinking&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Good-to-have:&lt;/strong&gt; Software engineering&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Good-to-have:&lt;/strong&gt; Business case knowledge&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dependency, Optional:&lt;/strong&gt; CS Fundamentals&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Strategies:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Proven:&lt;/strong&gt; Make novel and useful materials and freely release them - teaching materials &amp;amp; projects!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Very Useful:&lt;/strong&gt; Learn from industry insiders.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Very Useful:&lt;/strong&gt; Learn Python.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Don't Forget:&lt;/strong&gt; Build community.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;All-in-all, I think it boils back down to the fundamentals of living in a society: it's still about &lt;strong&gt;creating real value for others&lt;/strong&gt;, and &lt;strong&gt;receiving commensurate recognition&lt;/strong&gt; (not always money, by the way) for what you've delivered. Tips and tricks can sometimes get us ahead by a bit, but the fundamentals matter the most.&lt;/p&gt;
&lt;p&gt;For aspiring data scientists, some parting words: build useful stuff, learn new things, demonstrate that you can deliver value using data analytics and work with others using the same tools, and good luck on your job hunt!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2017/8/24/reading-and-writing-docs-the-overlooked-programming-skill/">
    <title type="text">Reading &amp; Writing Docs: The Overlooked Programming Skill?</title>
    <id>urn:uuid:41b8966e-9391-31d1-b945-ace2eae23652</id>
    <updated>2017-08-24T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2017/8/24/reading-and-writing-docs-the-overlooked-programming-skill/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;I recently read a blog article by DataCamp's CTO (Dieter) on how to scale their projects and their engineering team - it's a &lt;a href=&quot;https://medium.com/datacamp/technical-vision-part-1-5f016c163340&quot;&gt;great read&lt;/a&gt;! In the article, Dieter states that the only way to scale an engineering team is to have well-written docs. I can see the benefits to doing it this way - we minimize the number of channels that any coder needs to use to find out information; the docs should be the place where the intent and technical detail of the code are simultaneously documented alongside usage examples.&lt;/p&gt;
&lt;p&gt;Thus, in the final weeks up to starting my new job at Novartis as a Data Scientist, I decided to make sure I have the practice of writing, reading and publishing docs as good as muscle memory. I can already envision cases where, while conducting and building analyses, I end up writing a bunch of generally-useful functions that should be documented as well. What I write may eventually need to be used by someone else, including my future self; keeping track of how exactly a function is intended to be used is going to be very useful.&lt;/p&gt;
&lt;p&gt;I think reading and writing docs is an overlooked skill in programming. It's probably because this isn't a test of &quot;creative capacity&quot; (i.e. can you build something new), which is the &quot;sexy&quot; thing. It's more a test of &quot;maintenance capacity&quot; - and this is given less value and importance in the coding world. But it's incredibly important - many basic problems can be solved by reading the docs... but also, so many problems can be avoided by writing really good docs! The onus is on both parties - package maintainers &lt;em&gt;and&lt;/em&gt; developers - to write &lt;em&gt;and&lt;/em&gt; read good docs.&lt;/p&gt;
&lt;p&gt;But writing good docs is a tough job! I absolutely agree with this. There are different styles through which developers read docs - some prefer examples, while others just want to see function definitions - and it's very difficult to cater to every style. I personally think starting off with the style one's most comfortable with, and then gradually accepting community contributions, is the right way to go.&lt;/p&gt;
&lt;p&gt;One package that I maintain, &lt;a href=&quot;http://github.com/ericmjl/nxviz&quot;&gt;&lt;code&gt;nxviz&lt;/code&gt;&lt;/a&gt;, used to not have any docs written apart from that single file in the README. Thanks to my friend &lt;a href=&quot;https://www.linkedin.com/in/rempic/&quot;&gt;Remi Picone&lt;/a&gt;, I was able to learn how to configure Sphinx to get my docs working through copying &lt;a href=&quot;https://github.com/rempic/Image-Features-Extraction&quot;&gt;his example repository&lt;/a&gt;. Through that, I configured Sphinx to build docs on my nxviz project - and finally got it going! You can find it on &lt;a href=&quot;http://nxviz.readthedocs.io/en/latest/&quot;&gt;RTFD&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Learning this was really fun - looking forward to putting up more docs!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2017/8/10/next-steps/">
    <title type="text">Next Steps</title>
    <id>urn:uuid:8a2d5f36-6b39-3dc6-b303-643463e52510</id>
    <updated>2017-08-10T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2017/8/10/next-steps/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;Signed and done! I will be joining the Novartis Institutes for Biomedical Research (NIBR) in September, as part of the Scientific Data Analysis (SDA) team under Novartis Informatics (NX).&lt;/p&gt;
&lt;p&gt;NIBR is the research arm of Novartis, and the SDA team is essentially a &quot;Data Special Ops&quot; team inside NIBR. The nature of the position involves both internal consulting and the development of new initiatives across teams.&lt;/p&gt;
&lt;p&gt;The nature of the role I'm being hired into is in statistical learning, which is a general direction I've been moving towards during my time in grad school. I picked up and implemented a number of useful and interesting deep learning algorithms back then, and over the past half a year, have finally gotten in underneath the hood of graph &amp;amp; image convolutions, variational autoencoders and gaussian processes. It's really fun stuff, at its core, and to me, it's even more fun translating biological and chemical data problems into that language, and back.&lt;/p&gt;
&lt;p&gt;After a summer learning lots and networking with industry professionals and fellow Fellows at Insight, I'm ready for a bit more structure in my life. Looking forward to starting there!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2017/8/2/open-source-software/">
    <title type="text">Open Source Software</title>
    <id>urn:uuid:1fcbcd30-7e09-3f8b-b884-21f4d7eee71d</id>
    <updated>2017-08-02T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2017/8/2/open-source-software/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;Open source software is awesome, and I've just been thoroughly convinced of why.&lt;/p&gt;
&lt;p&gt;Today, I put in a &lt;a href=&quot;https://github.com/pymc-devs/pymc3/pull/2470&quot;&gt;PR&lt;/a&gt; to PyMC3. This was a bug fix related to the PyMC3 multinomial distribution's random variates generator, which uses &lt;code&gt;numpy&lt;/code&gt;'s multinomial under the hood, which arose from floating point precision errors.&lt;/p&gt;
&lt;p&gt;I first encountered this bug last week, when I started trying out the use of PyMC3 on my GPU tower. GPU stuff is tricky. One of the issues relates to floating point precision. I'm not well-versed enough on this to write intelligently about the underlying causes, but one thing I learned is that GPUs prefer 32-bit floating point precision (&lt;code&gt;float32&lt;/code&gt;), while modern CPUs can handle 64-bit (&lt;code&gt;float64&lt;/code&gt;). (I'm sure this will change in the future.) In the vast majority of &quot;large number&quot; computations, it's no big deal, but when we deal with small numbers (decimals in the thousandths range and smaller), addition errors can crop up.&lt;/p&gt;
&lt;p&gt;This was the exact problem I was facing. I had some numbers crunching on the GPU in &lt;code&gt;float32&lt;/code&gt; space. Then, I had to pass them back to &lt;code&gt;numpy&lt;/code&gt;'s multinomial, which implicitly converts everything to &lt;code&gt;float64&lt;/code&gt;. Because multinomial takes in a list of &lt;code&gt;p&lt;/code&gt;s (probabilities) that must sum to one, I was getting issues with my list of &lt;code&gt;p&lt;/code&gt;s summing to just infinitesimally (in computation land) greater than one. I dug around on-and-off for about a week to look for a solution, but none came. Instead, I had to rely on a small hack that I didn't like, adding a 1 millionth value to the sum and renormalizing probabilities... but that felt hacky and unprincipled.&lt;/p&gt;
&lt;p&gt;The fix was inspired by someone else's problems that was discussed on &lt;code&gt;numpy&lt;/code&gt;'s repository. The trick was to convert the numbers from &lt;code&gt;float32&lt;/code&gt; to &lt;code&gt;float64&lt;/code&gt; first and re-compute the probabilities in &lt;code&gt;float64&lt;/code&gt; precision. I implemented that locally, and everything worked! I quickly ran two of the most relevant tests in the test suite, and they both passed. So I pushed up to GitHub and submitted a PR on this (after checking in with the lead devs on their issue tracker) - and it was just merged tonight!&lt;/p&gt;
&lt;p&gt;If PyMC3's and &lt;code&gt;numpy&lt;/code&gt;'s code bases were not open source, with issues discussed openly, I would not have been able to figure out a possible fix to the issues with the help of other people. Also, I wouldn't have been able to patch the codebase locally first to see if it solved my own problems. I also wouldn't have access to the test suite to check that nothing was broken. All-in-all, working with an open source codebase was instrumental to getting this fix implemented.&lt;/p&gt;
&lt;p&gt;Big shout-out to the PyMC devs I interacted with on this - Colin &amp;amp; Junpeng. Thank you for being so encouraging and helpful!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2017/7/22/bayesian-neural-networks/">
    <title type="text">Bayesian Neural Networks</title>
    <id>urn:uuid:cfb0959d-2897-34c1-8f49-bc2e71496aeb</id>
    <updated>2017-07-22T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2017/7/22/bayesian-neural-networks/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;During this week, while us Insight Fellows begin going out to interview with other companies, my &quot;side hustle&quot; has been working on my &lt;a href=&quot;https://github.com/ericmjl/bayesian-analysis-recipes&quot;&gt;Bayesian Analysis Recipes&lt;/a&gt; repository.&lt;/p&gt;
&lt;p&gt;Two particularly interesting problems I've wanted to write my own implementation for are multinomial classification and Bayesian deep learning. I finally got both of them done today, after about 2-3 days of hacking on them.&lt;/p&gt;
&lt;p&gt;Multinomial classification (&lt;a href=&quot;https://github.com/ericmjl/bayesian-analysis-recipes/blob/master/multiclass-logistic-regression-cover-type.ipynb&quot;&gt;notebook here&lt;/a&gt;) is the problem where we try to classify an item as being one of multiple classes. This is the natural extension to binary classification (done by logistic regression). To do this, I took the &lt;a href=&quot;https://archive.ics.uci.edu/ml/datasets/covertype&quot;&gt;forest cover dataset&lt;/a&gt; and used PyMC3 to implement multinomial logistic regression. Seeing how to do it with PyMC3 was the most important aspect of this; actual accuracy wasn't much of a concern for me.&lt;/p&gt;
&lt;p&gt;However, having seen the classification report (at the bottom of the notebook), and having read that the dataset was originally classified using neural networks, I immediately had the thought of doing a Bayesian neural network for multi-class classification, having seen it implemented for binary classification on the PyMC3 website.&lt;/p&gt;
&lt;p&gt;Bayesian neural networks are not hard to intuit - basically, we place priors on the weights, rather than learning point estimates. In doing so, we are able to propagate uncertainty forward to predictions. Speaking as a non-expert in the field, I think the tricky part is the sampling algorithms needed.&lt;/p&gt;
&lt;p&gt;One thing nice about the field of Bayesian deep learning is the use of variational inference to approximate the true distribution of predictions with a mathematically more tractable one (e.g. a Gaussian). In doing so, we gain a fast way towards approximately learning the uncertainty in predictions - essentially we trade a little bit of accuracy for a lot of speed. For complex models like neural nets, this can be very valuable, as the number of parameters to learn grows very, very quickly with model complexity, so anything fast can make iteration easier.&lt;/p&gt;
&lt;p&gt;Starting with the code &lt;a href=&quot;http://twiecki.github.io/blog/2016/06/01/bayesian-deep-learning/&quot;&gt;from Thomas Wiecki's website&lt;/a&gt;, I hacked together a few utility functions and boiled down the example to its essentials. Feed-forward neural nets aren't difficult to write - just a bunch of matrix ops and we're done. The &lt;a href=&quot;https://github.com/ericmjl/bayesian-analysis-recipes/blob/master/multiclass-classification-neural-network.ipynb&quot;&gt;notebook is available as well&lt;/a&gt;. One nice little feature is that by going with a deep neural network, we have additional predictive accuracy!&lt;/p&gt;
&lt;p&gt;Moving forward, I'd like to improve on that notebook a bit more, by somehow implementing/developing a visualization for multiclass classification &lt;strong&gt;uncertainty&lt;/strong&gt; which is the thing we gain from going Bayesian. Hopefully I'll be able to get to that next week - it's shaping up to look quite hectic!&lt;/p&gt;
&lt;p&gt;As a side note, I found a bug with the multinomial distribution implementation in PyMC3, and am working with one of the core developers to get it fixed in PyMC3's master branch. (Thanks a ton, Junpeng, if you ever get to read this! ) In the meantime, I simply took his patch, modified mine a little bit, and used the patched up PyMC3 for my own purposes.&lt;/p&gt;
&lt;p&gt;This is why I think open source is amazing - I can literally patch the source code to get it to do what I need correctly! Wherever I work next has to be supportive of things like this, and have to allow re-release of generally/broadly useful code that I touch - it is the right thing to do!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2017/7/17/lessons-learned-during-insight/">
    <title type="text">Lessons Learned During Insight</title>
    <id>urn:uuid:65144002-b35b-3954-9870-3db8518bed59</id>
    <updated>2017-07-17T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2017/7/17/lessons-learned-during-insight/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;&lt;strong&gt;(a) Solving healthcare goes beyond solving the science underlying it.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;At its core, healthcare delivery is essentially a human problem.  Even what we choose to optimize for is a hard problem. Do we optimize for changing human behaviour, or do we optimize for more precise treatments?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;(b) Healthcare is complex&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The biggest thing preventing a &quot;solving of healthcare&quot; is misaligned incentives.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;(c) I like scientific data&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Regardless of the lesson that healthcare needs to be solved with more than science, I still found myself naturally much more engaged with companies that were dealing with scientific data as part of their data science problems. Teams that were dealing with other types of data: insurance claims, financial, marketing, platform product analytics, click streams... these were much less engaging. I know my best fit now, though I won’t rule out other teams.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;(d) People can change the equation.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I met with some people whose intellect and grasp of knowledge I really admire! Additionally, passion is infectious. It helps to work with colleagues who energize one another, rather than drain each others’ energy.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;(e) Some Insight alumni are awesome&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;And I want to be like them when I help with mentoring for the next batch. Perhaps if I get a chance to interview others, I’d like to be able to model how I interview after the alumni mentors.&lt;/p&gt;
&lt;p&gt;Biggest shout-out to George Leung, who works for Vectra, tailored his mentoring session by first asking me about my Insight project, which involved Gaussian processes and variational auto-encoders (VAEs). George asked me first about what VAEs were, and then asked me to solve a Bayes problem on the board. I could tell he was building his questions on-the-fly.&lt;/p&gt;
&lt;p&gt;The other shout-out goes to Ramsey Kamar, who went through the “Big 4” questions: tell me about yourself, what’s your previous accomplishments, how did you face a conflict, and what’s your biggest weakness. His feedback to me was direct, positive, and most importantly, always encouraging.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;(f) Humanities tools are needed&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;On reflection, I think that if we’re going to solve the “human” portion of healthcare, we’re going to need tools from the humanities - the tools that let us qualitatively and quantitatively study human behaviour. While data science can provide a quantitative path towards a solution, the qualitative side of it will remain as important as ever.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2017/7/15/insight-week-7/">
    <title type="text">Insight Week 7</title>
    <id>urn:uuid:f74e9ac8-4b90-3bf8-9f49-2df29a025f6b</id>
    <updated>2017-07-15T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2017/7/15/insight-week-7/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;Aaand with that, week 7 of Insight is done!&lt;/p&gt;
&lt;p&gt;I had a short week because of SciPy 2017, and I'm thankful that I got a chance to head out there - had the opportunity to reconnect with many friends from the SciPy community.&lt;/p&gt;
&lt;p&gt;The two days of Week 7 that I experienced were probably the weirdest week 7 any Fellow has experienced to date. Because I had missed a demo on account of SciPy, and because the company didn't want to just watch the pre-recorded demo video, I made a trek up to Cambridge to demo on-site. What initially was a 30 minute session turned out to be a 1.5 hr demo.&lt;/p&gt;
&lt;p&gt;I have two more demo obligations to fulfill next week. Other than that, it's going to be mostly interview preparation with other fellows, and more data and coding challenges, and more studying of topics that we're not familiar with. I am trying to brush up on SQL more, as I can see it being a useful tool to have to query data out of databases.&lt;/p&gt;
&lt;p&gt;Now that we're done with Week 7, we're going to be alumni soon. As such, I've began thinking about how I could give back as an alumni. Some ideas have come to mind, inspired by what others have done.&lt;/p&gt;
&lt;p&gt;Firstly, I think I can help standardize future Fellows' coding environments by providing a set of annotated instructions for installing the Anaconda distribution of Python. Perhaps even an evening workshop on the first Thursday might be useful.&lt;/p&gt;
&lt;p&gt;Secondly, I've come to recognize that the biggest bottleneck for Fellows' projects is the web deployment and design portion. Model training to obtain an MVP is fairly fast - one of &lt;code&gt;scikit-learn&lt;/code&gt;'s models is often good enough. However, most of us didn't know HTML and Bootstrap CSS, and the deadline makes it stressful enough to pick this up on-the-fly. (The stress is probably compounded by the fact that the web app/blog post is not the most intellectually interesting portion of the project.) Perhaps a workshop at the end of Week 2 or beginning of Week 3 might be good.&lt;/p&gt;
&lt;p&gt;Thirdly, I see this trend where a lot more projects are going to start using deep learning. I think putting a workshop together with, say, Jigar, might be a useful thing to have.&lt;/p&gt;
&lt;p&gt;Finally, my interview simulator questions have become famous for being a 'hybrid' between stats, ML and CS. It's very much in the same vein as what I got when I interviewed with Verily.&lt;/p&gt;
&lt;p&gt;Until we get hired, we are allowed (and one might even say, expected) to continue coming into the office to help each other prepare for upcoming interviews. We're all looking forward to getting hired and solving data problems!&lt;/p&gt;
&lt;p&gt;With this post, I think I'll end the regular blog post series here. Hope this post series was an informative insight into Insight! Next one I'll post is going to be a summary of lessons learned from my time as an Insight Health Data Fellow.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2017/7/12/scipy-2017/">
    <title type="text">SciPy 2017</title>
    <id>urn:uuid:4b2b2af9-0ddf-3552-bc94-c7e33fbadfe5</id>
    <updated>2017-07-12T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2017/7/12/scipy-2017/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;I just finished from SciPy 2017!  This was a fruitful conference, and I'm glad I managed to make it.&lt;/p&gt;
&lt;p&gt;Monday was the first day. I wanted to get a better feel for the Jupyter widgets ecosystem, and as such I sat in on the corresponding &lt;a href=&quot;https://scipy2017.scipy.org/ehome/220975/493418/&quot;&gt;tutorial&lt;/a&gt;. That happened to be the only tutorial I sat in live.&lt;/p&gt;
&lt;p&gt;Nonetheless, one nice thing about the tutorials is that they are live recorded, and so we can watch the ones we missed on our own time when back home. These are the ones I hope to catch, partly out of interest, partly from recommendations by other conference attendees who sat in them:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Numba&lt;/li&gt;
&lt;li&gt;Holoviews&lt;/li&gt;
&lt;li&gt;Dask&lt;/li&gt;
&lt;li&gt;scikit-image&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Looking at the list, I kind of realize now how much of a Continuum Analytics fanboy I've become...&lt;/p&gt;
&lt;p&gt;On the second day, I delivered my own Network Analysis Made Simple. I collected some feedback right at the end of the tutorial, and it looked like they were overall very positive. Many liked the whiteboard illustrations that I added on. When delivering this at PyCon, I think it would benefit from having a whiteboard of sorts.&lt;/p&gt;
&lt;p&gt;The third day was the start of the conference talks. There's many, many great talks out there! I also had the opportunity to connect with new people over breakfast, lunch, coffee and dinner. I tried hosting &quot;office hours&quot;, like Matt Davis did last year, but I think I announced it a bit too late.&lt;/p&gt;
&lt;p&gt;All-in-all, I think it was great to attend SciPy 2017 this year. I'm happy to have not broken the chain of attendance. Looking forward to serving on next year's organizing committee again, and I hope to have a new tutorial in the works!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2017/7/8/insight-week-6/">
    <title type="text">Insight Week 6</title>
    <id>urn:uuid:f1f19fd6-3a2a-3c79-b089-632495a06614</id>
    <updated>2017-07-08T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2017/7/8/insight-week-6/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;We had a short week this week because of the long July 4th weekend (Happy Birthday, America!).&lt;/p&gt;
&lt;p&gt;Wednesday was my second demo day, this time at MGH. There were 8 of us demoing at MGH's Clinical DS team, and I really enjoyed the interaction with them. The team asked of me two technical questions about Flu Forecaster, both of which were analogous to other questions I had heard before. After the demo, we hung out with the team and chatted a bit about their latest projects.&lt;/p&gt;
&lt;p&gt;In the afternoon, I focused on doing the data challenge and leetcode exercises; in the evening, I (at the last minute) signed up for back-to-back behavioral and ML interview practice sessions. It was good to chat with the alumni helping with the sessions, as I learned much more about their thought process. In the future, I'll probably be called on to interview other people, and I will definitely draw on my experiences here.&lt;/p&gt;
&lt;p&gt;On Thursday we had more prep. I helped with mock interviewing by being an observer for Xi and an interviewer for Angela. The role-playing with Angela was an interesting one for me. I tried playing the role of a conversational but technically-competent interviewer. Also asked questions genuinely out of curiosity too. I think that combined with Angela's outgoing personality kept the conversation enjoyable for all three of our spectators.&lt;/p&gt;
&lt;p&gt;In the late afternoon, an NYC session alum came by and gave us a session on data challenges. The exercise he gave was quite neat - basically, given one categorical output column and a slew of other feature columns, train the best model that has the highest accuracy score. Oh, the twist? Do it in 25 minutes.&lt;/p&gt;
&lt;p&gt;The key point from this exercise was to have us get prepared for an on-site data challenge. The on-site data challenge mainly helps the hiring team check that we have the necessary coding chops to work with the team. It also lets them see how we perform under time constraints. The most important thing is to deliver a model with some form of results. Iterating fast is very important. Thus, it helps to push out fast one model that works.&lt;/p&gt;
&lt;p&gt;On Friday, we did another round of the interview simulator. I thought it was better run this time round. The mutual feedback from one another is very helpful. I was tasked with a stats question, which I melded into a hybrid stats + CS question, thus modelling what I had received when I was interviewed at Verily. FWIW, the question I asked was to define bootstrap resampling (sampling with replacement), implement it using the Python standard library, and discuss the scenarios where it becomes a useful thing.&lt;/p&gt;
&lt;p&gt;If tasked with a similar one for the next time, I will probably ask about writing a function to sample from a Bernoulli distribution using only the Python standard library. It's useful to know how to implement these statistical draws when it's not easy or impossible to use other libraries. (I had to do it when trying out the PyPy interpreter a few years back, and didn't want to mess with installing &lt;code&gt;numpy&lt;/code&gt; for PyPy.)&lt;/p&gt;
&lt;p&gt;I liked a few of the other questions asked as well - for example, the knapsack problem posed by Steve: Given a set of produce items, each with their own value and weight (in Kg), and a knapsack that can only carry a maximum weight of produce, find the set of produce that will maximize value at the market.&lt;/p&gt;
&lt;p&gt;That afternoon, we slowed things down a bit. Regardless of how much we benefit from them, the interview simulators nonetheless are tiring. But that's the key point - interviews are day-long, exhausting endeavours that test stamina and ability to switch between contexts (both technical and social). The simulator aims to simulate that.&lt;/p&gt;
&lt;p&gt;Looking forward to next week. For me it'll be a short one, because I'll be at SciPy 2017 to lead a Network Analysis tutorial. Also hoping to represent Insight well!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2017/7/1/insight-week-5/">
    <title type="text">Insight Week 5</title>
    <id>urn:uuid:1c3535fd-47ba-3bcf-a37c-79a0f2cc9e82</id>
    <updated>2017-07-01T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2017/7/1/insight-week-5/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;First off, Happy Canada Day!&lt;/p&gt;
&lt;p&gt;Week 5 is primarily focused on interview prep as a bunch of us go out for our demos.&lt;/p&gt;
&lt;p&gt;We kicked off Monday with an interview prep field day. The main areas of focus for us were CS fundamentals, machine learning, SQL, and behavioral interviewing. I found SQL to be my weakest point, and I'll definitely be focusing a lot of efforts on there. I had a chance to explain gradient descent and regularization using algebra - something I never thought I would do!&lt;/p&gt;
&lt;p&gt;On Tuesday, Fellows began going outside for demos. My first demo will be at Boston Health Economics this Thursday, followed by (in no particular order) MGH, Biogen, Merck, OM1, and Immuneering. Definitely looking forward to presenting Flu Forecaster to them!&lt;/p&gt;
&lt;p&gt;On the side, we also started thinking through computer science fundamentals problems, and doing data analytics challenges. CS fundamentals are what you think it would be, covering data structures and algorithms. I found myself to be particularly fond of recursion, and implemented a recursive algorithm for something that could be solved in linear time without recursion. It was good to see my biases, and to try my hand at implementing the same thing in fundamentally two different styles.&lt;/p&gt;
&lt;p&gt;In the evening, Nick (one of the fellows) gave us a run through on SQL. It was very useful to have his perspective, which was basically that most of the problems we will encounter involve some degree of nested searches, and that we have to work backwards from what we want. I also had a good perspective from my alumni mentor on how to approach describing my thesis to interviewers.&lt;/p&gt;
&lt;p&gt;On Wednesday, the interview prep continued with more coding challenges, demo trips, and fellow-led workshops. Together with Jeff and Jigar, we led a deep learning fundamentals workshop, in which we went through how deep learning works for feed forward neural networks and convolutions neural networks.&lt;/p&gt;
&lt;p&gt;Thursday came my first demo, which was at Boston Health Economics. Overall, I thought the demo session went well, and that Catherine, our host, kept engaged with the presentations. I very much appreciate her intellect. Additionally, I took the approach of &quot;free styling it&quot; (of course conditioned on having previously rehearsed it enough times), which resulted in a demo presentation that was overall smoother than what I had previously delivered&lt;/p&gt;
&lt;p&gt;Apart from that, we continued our interview prep. This involved more CS fundamentals for me, getting more practice with common algorithms, and finishing the coding exercises that Ivan gave us.&lt;/p&gt;
&lt;p&gt;On Friday, we did an interview simulator, in which we practiced interviewing one another. This gave me a better view into the thought process that an interviewer might be going through, particularly when conducting a technical interview. From prior experience interviewing, I remembered that my most pleasant interviews were with individuals who kept the atmosphere positive, encouraging, and provided hints along the way. Thus, I tried to conduct the mock interviews in the same way.&lt;/p&gt;
&lt;p&gt;In the afternoon, I gave a very short workshop on how to write Pythonic code, which covered &lt;a href=&quot;https://www.python.org/dev/peps/pep-0008/#introduction&quot;&gt;&lt;code&gt;PEP8&lt;/code&gt;&lt;/a&gt; (which is now check-able using &lt;a href=&quot;https://github.com/PyCQA/pycodestyle&quot;&gt;&lt;code&gt;pycodestyle&lt;/code&gt;&lt;/a&gt;). It was fun seeing everybody go, &quot;Whoa! Atom can do that?!&quot; and then promptly going ahead to clean up their code according to the &lt;code&gt;flake8&lt;/code&gt; linter's recommendations.&lt;/p&gt;
&lt;p&gt;Interspersed throughout the week, I made an effort to summarize my thesis work a bit more. I think I have a few ways/hooks to explain it to a 'recruiter without a technical background', a 'computer scientist without biology background', and a 'biologist without a computing background'. Making it concise with a good &quot;hook&quot; was the hardest part, but I think I have something good now.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2017/6/30/using-bokeh-in-fluforecaster/">
    <title type="text">Using Bokeh in FluForecaster</title>
    <id>urn:uuid:5947ebef-768b-3520-b99f-4464525e6c45</id>
    <updated>2017-06-30T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2017/6/30/using-bokeh-in-fluforecaster/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;Author: Eric J. Ma, Insight Health Data Science Fellow (Boston 2017b)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Summary&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In this blog post, I will show how Bokeh featured in my Insight project, FluForecaster.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Motivation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As a Health Data Fellow at Insight, we spend about 3 weeks executing on a data project, which we demo at companies that are hiring. I built &lt;a href=&quot;https://fluforecaster.herokuapp.com/&quot;&gt;FluForecaster&lt;/a&gt;, which was a project aimed at forecasting influenza sequence evolution using deep learning.&lt;/p&gt;
&lt;p&gt;My choice of project was strategically aligned with my goals on a few levels. Firstly, I wanted to make sure my project showcased deep learning, as it's currently one of the hottest skills to have. Secondly, I had components of the code base written in separate Jupyter notebooks prior to Insight, meaning, I could execute on it quickly within the three weeks we had. Thirdly, I had intended to join Insight primarily with the goal of networking with the Insight community, and that basically meant 'being a blessing' to others on their journey too - if I could execute fast and well on my own stuff, then there'd be time to be a team player with other Fellows in the session, and help them get their projects across the finish line.&lt;/p&gt;
&lt;p&gt;Each of us had to demo a &quot;final product&quot;. Initially, I was thinking about a &quot;forecasting dashboard&quot;, but one of our program directors, Ivan, suggested that I include more background information. As such, I decided to make the dashboard an interactive blog post instead. Thus, with FluForecaster being a web-first project, I finally had a project in which I could use Bokeh as part of the front-end.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Applying Bokeh&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Bokeh was used mainly for displaying three data panels in the browser. Firstly, I wanted to show how flu vaccine efficacy rarely crossed the 60% threshold over the years. Secondly, I wanted to show a breakdown of the number of sequences collected per year (as used in my dataset). Thirdly, I wanted to show a visual display of influenza evolution.&lt;/p&gt;
&lt;p&gt;For yearly vaccine effectiveness, it was essentially a line and scatter chart, with the Y-axis constrained between 0 and 100%. I added a hover tooltip to enable my readers to see the exact value of vaccine effectiveness as measured by the US CDC.&lt;/p&gt;
&lt;p&gt;To show the number of sequences per year in the dataset, the same kind of chart was deployed.&lt;/p&gt;
&lt;p&gt;Bokeh magic became really evident later when I wanted to show sequence evolution in 3 dimensions. Because 3D charts are generally a poor choice for a flat screen, I opted to show pairs of dimensions at a time. A nice side-effect of this is that because my &lt;code&gt;ColumnDataSource&lt;/code&gt; was shared amongst each of the three pairs of coordinates, panning and selection was automatically linked for free.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Usage Pros and Cons&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Bokeh's API is very powerful, in that it supplies many plotting primitive objects (glyphs, particularly), and that makes it a big plus for users who are experienced with the library, who are creating complex interactive charts.&lt;/p&gt;
&lt;p&gt;Most of my fellow Fellows at Insight ended up using the &lt;code&gt;bokeh.plotting&lt;/code&gt; interface, and I did too. I think the &lt;code&gt;bokeh.plotting&lt;/code&gt; interface provides the best balance between ease-of-use and flexibility. If you take a look at the code &lt;a href=&quot;https://github.com/ericmjl/flu-sequence-predictor/blob/master/utils/webplots.py#L15&quot;&gt;here&lt;/a&gt;, you'll notice that there's often a bit of boilerplate that gets repeated with variation, such as in the configuration of custom hover tools. I think this is the tradeoff we make for configurability... or I might just be not writing code most efficiently. :)&lt;/p&gt;
&lt;p&gt;There were times where I was tempted to just use the &lt;code&gt;bkcharts&lt;/code&gt;' declarative interface instead. It's a lot more easy to use. However, I did have some time on hand, and wanted to get familiar with the &lt;code&gt;bokeh.plotting&lt;/code&gt; interface, because there's a possibility that I might want to make wrappers for other visualizations that can lend themselves to a declarative API.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Embedding Visualizations&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I built my interactive blog post using a combination of Flask, hand-crafted HTML, Bootstrap CSS &amp;amp; JS, and Bokeh - which took care of the bulk of visuals. I drew static figures using Illustrator.&lt;/p&gt;
&lt;p&gt;Embedding the necessary Bokeh components wasn't difficult. Very good documentation is available on the &lt;a href=&quot;http://bokeh.pydata.org/en/latest/docs/user_guide/embed.html&quot;&gt;Bokeh docs&lt;/a&gt;. The key insight that I had learned was that I could have the &lt;code&gt;components&lt;/code&gt; passed into my Flask app functions' &lt;code&gt;return&lt;/code&gt; statements, and embed them using Jinja2 templating syntax. An example can be found &lt;a href=&quot;https://github.com/ericmjl/flu-sequence-predictor/blob/master/templates/index.html#L81&quot;&gt;here&lt;/a&gt;. Basically, &lt;code&gt;components&lt;/code&gt; returns a &lt;code&gt;div&lt;/code&gt; and a &lt;code&gt;js&lt;/code&gt; object, which are essentially just strings. To embed them in the templates, we use the syntax &lt;code&gt;{{ div|safe }}&lt;/code&gt; and &lt;code&gt;{{ js|safe }}&lt;/code&gt;. That &lt;code&gt;|safe&lt;/code&gt; is very important: it tells the Jinja2 templating engine that it's safe to render those pieces of Javascript and HTML.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Through the project, I became a lot more familiar with the Bokeh plotting library. Now I feel a bit torn! I've contributed to both the Bokeh and &lt;code&gt;matplotlib&lt;/code&gt; projects, and I love them both! I've also come to deeply respect the lead developers of both projects, having interacted with them many times. If I were to make a comment on &quot;where to use what&quot; based on my experience, it'd probably still be the conservative view of &quot;&lt;code&gt;matplotlib&lt;/code&gt; for papers, &lt;code&gt;bokeh&lt;/code&gt; for the web&quot;... but I'm sure that will be outdated soon. Who knows how the Python plotting landscape will evolve - it's exciting times ahead, and at least for now, I'm happy for the experience driving a dataviz project with Bokeh!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2017/6/24/insight-week-4/">
    <title type="text">Insight Week 4</title>
    <id>urn:uuid:d45d85a8-c226-3cd4-ae89-5e9392e16346</id>
    <updated>2017-06-24T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2017/6/24/insight-week-4/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;Week 4 has been all about demos. Polishing our demos, picking companies that we want to demo at (and possibly interview at later on). Every morning, we practice our demos, 10 minutes per person, with the goal of keeping our demo to under 5 minutes to leave time for Q&amp;amp;A. I've found that the act of rehearsing our demos makes it much easier to pick out where I need improvement. For example, I tended to have trouble with explaining the validation portion smoothly, even though I knew what I was doing there. A tool that seems useful, especially for short demos, is to write out exactly what I want to say, and that definitely helped.&lt;/p&gt;
&lt;p&gt;On the type of work that I'm interested in, here's some things I've become much clearer on.&lt;/p&gt;
&lt;p&gt;Firstly, the factors I'm considering for a company. The ideal combination is: a company that deeply values the hard sciences (in my case life sciences), and is solving very tough technical problems that requires growth in and mastery of deep technical topics, on a team that encourages experimentation, personal growth, and open source contributions on company time. We'd have to be at the innovation boundary of very powerful techniques. This is important for me, because I believe that 5-10 years down the road, I would have mastery over very foundational and broadly applicable tools with the appropriate experience applying them to real-world problems, which I could leverage to solve more cool and interesting problems. It's also a good defence against being pigeon-holed into a particular domains or tasks - autonomy in problem selection and definition is very important to me, so most of my choices aim to maximize that over money.&lt;/p&gt;
&lt;p&gt;Secondly, I've effectively ruled out companies that are dealing with non hard-science data, e.g. insurance claims, marketing &amp;amp; advertising, finance, and business data. Having applied computation to the life sciences over grad school, and being trained in the life sciences for over 10 years, I'm not ready to give up that background knowledge to work on other problems. I also believe that investing in the hard sciences means investing in the next wave of real-world innovation, and I'd like to ride that wave.&lt;/p&gt;
&lt;p&gt;Thirdly, within the next 5 years, I see myself growing as a technical person, rather than a management person. People issues, particularly conflict resolution, make it difficult to focus on being a good craftsman, and I much more enjoy craftsmanship than management.&lt;/p&gt;
&lt;p&gt;Now, on the companies that have come by...&lt;/p&gt;
&lt;p&gt;Most are using open data science tools in their toolkit, and this mostly means Python and R, Spark and a few other big DB tools. Some are still using SAS (.................) and didn't show a trend towards open data science languages, and effectively ruled themselves out of contention. (Using legacy tools signals a lack of forward-thinking and a desire to favour the status quo over pushing boundaries.)&lt;/p&gt;
&lt;p&gt;Some have given us words of wisdom. One guy basically said that healthcare has messed up (he used stronger language) incentives. Another said that to solve healthcare we need to first solve human behaviour. All very interesting points that are well-taken on my side. A non-healthcare company told us that if we're not paying for a service, then we're the product.&lt;/p&gt;
&lt;p&gt;In our session, it was basically the pharma research arms that piqued my interest the most, aside from one hospital's internal startup team. The gap in interest between #4 and #5 (for me, at least) was really big, and the gap of interest from #5 to the rest was even larger.&lt;/p&gt;
&lt;p&gt;Anyways, week 5 begins soon, and we pivot over into interview prep. Looking forward to learning lots, particularly doing deep dives on my weak spots!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2017/6/17/insight-week-3/">
    <title type="text">Insight Week 3</title>
    <id>urn:uuid:e5fc9686-7fc3-3c47-a50e-b3d2eac8f6f6</id>
    <updated>2017-06-17T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2017/6/17/insight-week-3/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;This week was a week of polishing our final products and getting them in shape for our demos. Pushing a product to final production really involves a lot of nitty-gritty tweaking. In this blog post, I'll detail some of what I had to work on.&lt;/p&gt;
&lt;p&gt;My &lt;a href=&quot;https://fluforecaster.herokuapp.com&quot;&gt;final product&lt;/a&gt; a hybrid web dashboard + blog post. Behind the dashboard is a fairly complex set of computations, which currently are run through a Jupyter notebook. The front-end, therefore, only renders the predicted flu sequences returned from the Jupyter notebooks. As part of my forecasts, I want to show the uncertainty surrounding the predictions, and how they're associated with individual forecasted sequences. This requires computing a convex hull surrounding a point cloud, and plotting it. I spent about 3-4 hours on Tuesday figuring out the code to make this part of the visualization, which I consider integral to communicating the project.&lt;/p&gt;
&lt;p&gt;Another important thing is the user experience (UX) when interacting with my hybrid blog post + dashboard. Unlike this blog or one written in Medium (the blog of choice for Insight), I have interactive elements in the post, which meant I had to hand-craft the HTML for the page. In plotting the figures on the page, there are a set of functions in the backend that are run before the page is rendered. These compute the necessary JS for interactive web plots. They have to run fast enough, otherwise Heroku will timeout. Introducing code to plot the bounding boxes above slowed the loading time of the page beyond the 30 second limit Heroku imposed. As such, I had to carefully profile my code (mostly manually, with timing statements printed to console) to isolate the slow part, rewrite the implementation for speed, and re-deploy to Heroku. This took another good 3-4 hours, all to shave off dozens of seconds. The things we do with our lives!&lt;/p&gt;
&lt;p&gt;Throughout the week, a lot of other Fellows were getting their web demos set up. A lot of questions regarding Bokeh and Flask were flying around. Because of the discussion, I think I have a much better grasp over the programming model involved in making Bokeh work with Flask. Basically there's a bunch of plotting computation that is needed to get the JavaScript computer by Bokeh, and then through Jinja2 templating and HTML divs, we can put the final plot in the HTML canvas. A few more rounds of practice and I should be able to commit it to memory.&lt;/p&gt;
&lt;p&gt;The final part is in getting the presentation overall looking polished and understandable. This involves many tasks, from tweaking the text to making static figures and more. I have spent time with column layouts and configuring modals to get my page content looking overall fresh and yet also informative. Requires a lot of thought!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2017/6/10/insight-week-2/">
    <title type="text">Insight Week 2</title>
    <id>urn:uuid:f36e273b-a937-34da-b058-04e268d1e235</id>
    <updated>2017-06-10T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2017/6/10/insight-week-2/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;This week has been intense, mostly because I knew in advance that I'd be spending two days wearing a fancy hat, funky robe, and yellow sash. Because I was missing two days of Insight, I had to get my Minimum Viable Product (MVP) out by Wednesday - thankfully, I did!&lt;/p&gt;
&lt;p&gt;If you've followed my blog post series on Insight (this is the 2nd post), my project is forecasting influenza sequences. This week, I hacked out my MVP and deployed it to &lt;a href=&quot;https://fluforecaster.herokuapp.com/&quot;&gt;Heroku&lt;/a&gt; as a hybrid HTML report + dashboard. I also picked up and incorporated a few new things along the way.&lt;/p&gt;
&lt;p&gt;The first is the use of tooltips on my Bokeh plots. Bokeh is really powerful, and in some of the exploratory analyses, I desired having tooltips as a UI element to help a reader (who might need some introduction) understand the nature of the problem and the data involved.&lt;/p&gt;
&lt;p&gt;The second is further mastery of Bootstrap CSS &amp;amp; JS. Now, Insight's Program Directors have told us clearly that we're not gunning to become front-end designers (and the likes). Keeping that in mind, I still think it's important to know at least one front-end framework well enough to produce pleasant-looking interactive tools or reports - knowing front-end elements potentiates us to communicate with front-end designers on final data products.&lt;/p&gt;
&lt;p&gt;For the MVP, I tried further experiments with the Grid layout and Modal JS. The key idea behind Grid layouts was easier to grasp - prioritize rows, then columns.&lt;/p&gt;
&lt;p&gt;With the Modal, stepping back for a moment, my goal was to display the science behind the project. However, it gets really technical. My audience is probably going to fall into one of two personas: the &quot;business person&quot; who just wants to see the final result and doesn't really care about the techniques, and the &quot;technical person&quot; who wants to dig deeper. I chose to use the Modal effect to satisfy both. The scientific methods are described at a high level on the main page, and the Modal element is used to show further information, graphics, and the likes.&lt;/p&gt;
&lt;p&gt;The third was deployment to Heroku itself! &lt;a href=&quot;https://www.davidbaumgold.com/&quot;&gt;David Baumgold&lt;/a&gt; first showed me how to use Heroku at PyCon 2016, but I could never wrap my head around it at first. I think I didn't understand how &quot;deployment&quot; worked. A year later, stuff that DB taught me came to fruition, as I hacked on deploying a minimal Flask app to Heroku with my younger brother. That gave me enough of the Heroku-specific concepts to hack together the necessary &lt;code&gt;requirements.txt&lt;/code&gt; and &lt;code&gt;Procfile&lt;/code&gt; files to deploy Flu Forecaster to the web.&lt;/p&gt;
&lt;p&gt;For next week, these are my plans:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Solve the problem of &quot;average sequences per quarter&quot; being ~10 amino acids different from actual sequences. Two approaches I'm thinking of:&lt;ul&gt;
&lt;li&gt;Use GPflow to train a Sparse Variational GP regressor on my dataset. This should allow me to scale up the forecasts beyond 67 quarters and into 700+ weeks, which is a greater time resolution and thus will give me greater sequence resolution in forecasts.&lt;/li&gt;
&lt;li&gt;Try forecasting variance in addition to mean coordinates. This will give me a full probability distribution to sample from, which may allow me to stick with PyMC3 and quarterly forecasts.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Still not sure which of the above two approaches are the better one, so I'll be sure to give each a shot.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2017/6/10/walk-the-stage-2017-edition/">
    <title type="text">&quot;Walk the Stage&quot; 2017 Edition</title>
    <id>urn:uuid:5d633f67-a1d8-3757-8d46-cf8d58935ff7</id>
    <updated>2017-06-10T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2017/6/10/walk-the-stage-2017-edition/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;Finally put on a funny hat and an oversized robe, and topped it off with a yellow hood that only six of us received last Friday. Just six in the entire Institute this year - only six of us weirdos chose the SciDoc (ScD) degree! (I chose it because I like yellow over blue, and get to have a bit of fun confusing recruiters out there.) The day was too hot, and so I couldn't be bothered to dress up - who's going to see what I'm wearing underneath the robes anyways?!&lt;/p&gt;
&lt;p&gt;Anyways, overall a good feeling to be done. A little bittersweet because I'm leaving a time where I had a ton of fun learning new things, especially in my final two years of grad school, though I also think it's nice to have a change of environment and to have a new set of problems to solve.&lt;/p&gt;
&lt;p&gt;My hope is to continue being deeply engaged with the hard sciences, even outside of the academic ivory tower, just because it's a fun thing to do. Here's to hoping I can find a good match with a company out there.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2017/6/2/insight-week-1/">
    <title type="text">Insight Week 1</title>
    <id>urn:uuid:5b5da7d1-9c07-3403-af55-a496755da518</id>
    <updated>2017-06-02T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2017/6/2/insight-week-1/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;Insight's Week 1 is done! Here's some of my thoughts so far.&lt;/p&gt;
&lt;p&gt;Firstly, the Fellows at Insight is very fast at learning things. Everybody is either a PhD or MD, some have done post-doctoral work, and even fewer have become professors, but everybody is interested in doing data stuff, and are very fast at picking up new things. I think at the same time, we're also good at thinking strategically upon being given feedback; once an idea sounds infeasible, new ideas come out of the pivot or even switch.&lt;/p&gt;
&lt;p&gt;Secondly, I see now the importance of developing a great data product. I think of a data product in terms of the &lt;strong&gt;input data&lt;/strong&gt;, the &lt;strong&gt;transformation&lt;/strong&gt; applied to the data, and the &lt;strong&gt;insight&lt;/strong&gt; returned from the data. Think of it as a Python function:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;def data_product(data):
    insight = transformation(data)
    return insight
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Most of the &quot;data products&quot; being developed are consumer-facing type projects that a user can interact with, but a small number of them, mine included, are &quot;dashboard-style&quot; products that can continually ingest continually updated data and return continually updated insights. Both are good ideas.&lt;/p&gt;
&lt;p&gt;Thirdly, I've become clear on the importance of first clearly defining the problem we want to solve, and then working backwards to define what we build, particularly for the &lt;strong&gt;m&lt;/strong&gt;inimum &lt;strong&gt;v&lt;/strong&gt;iable &lt;strong&gt;p&lt;/strong&gt;roduct (MVP). This way of thinking keeps us agile, and prevents us from being stuck in a rut.&lt;/p&gt;
&lt;p&gt;Fourthly, other fellows know lots of good stuff that I've been able to learn about. For example, in deep learning, there's been a few steps I wasn't sure about w.r.t. convolutional neural networks in autoencoders. One other fellow, a post-doc from UC Berkeley, gave me the master-class run-through on what happens at the vector/matrix level with convolutional neural networks.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Thus far, really nice. I've noticed we don't generally end up competing with one another, and the atmosphere is very collaborative. We're working with one another, talking with one another, building trust and the likes. I'm looking forward to the coming weeks!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2017/5/22/pycon-2017-highlights/">
    <title type="text">PyCon 2017 Highlights</title>
    <id>urn:uuid:fa889d0d-31b1-34c9-87ae-7481b1b435e1</id>
    <updated>2017-05-22T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2017/5/22/pycon-2017-highlights/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;Last post was about thoughts on past PyCons, having attended PyCon 2017. This post is on PyCon 2017's highlights for me.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;(1) Serving as part of the organizing committee.&lt;/strong&gt; I had the privilege of serving on the FinAid committee this year, and spent a large fraction of time in the staff room preparing to disburse FinAid cheques. I have very vivid memories of how slow the line was when I was receiving my cheques back in the day, and so I wanted to make sure FinAid recipients could receive their reimbursements as fast as possible, without wasting time in line (when they could instead be listening on talks).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;(2) Teaching two tutorials.&lt;/strong&gt; This year, I submitted two tutorial proposals, and both were accepted. In the three years that I've been teaching it, &lt;a href=&quot;https://www.youtube.com/watch?v=E4VKzFmByhE&quot;&gt;Network Analysis Made Simple&lt;/a&gt; has always been popular, and I think it's because it gives participants a different way of thinking about data, thus making it an intellectually stimulating topic. I also developed a new material on &lt;a href=&quot;https://www.youtube.com/watch?v=yACtdj1_IxE&quot;&gt;Best Testing Practices for Data Science&lt;/a&gt;. This one, in retrospect, was much fresher, and thus in need of more battle-testing and polish compared to Network Analysis. I have some ideas, including modifications to the workshop format, narrowing the target audience and more, to make it more useful for future iterations.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;(3) First talk at PyCon!&lt;/strong&gt; I also gave a talk at PyCon on doing &lt;a href=&quot;https://www.youtube.com/watch?v=p1IB4zWq9C8&quot;&gt;Bayesian Statistical Analysis with PyMC3&lt;/a&gt;! This was my first PyCon talk ever. It was so nice to have a tweet-commendation by PyMC3's creator Chris Fonnesbeck too:&lt;/p&gt;
&lt;p&gt;&lt;blockquote class=&quot;twitter-tweet&quot; data-lang=&quot;en&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;. &lt;a href=&quot;https://twitter.com/ericmjl&quot;&gt;@ericmjl&lt;/a&gt; is giving the &lt;a href=&quot;https://twitter.com/hashtag/PyMC3?src=hash&quot;&gt;#PyMC3&lt;/a&gt; talk you really want to hear if you want to learn how to put it to practical use. Room 251. &lt;a href=&quot;https://twitter.com/hashtag/PyCon2017?src=hash&quot;&gt;#PyCon2017&lt;/a&gt;&lt;/p&gt;&amp;mdash; Chris Fonnesbeck (@fonnesbeck) &lt;a href=&quot;https://twitter.com/fonnesbeck/status/866404594951139328&quot;&gt;May 21, 2017&lt;/a&gt;&lt;/blockquote&gt;&lt;/p&gt;
&lt;script async src=&quot;//platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;&lt;p&gt;It was also nice to have Thomas Wiecki's tweet-commendation too:&lt;/p&gt;
&lt;p&gt;&lt;blockquote class=&quot;twitter-tweet&quot; data-lang=&quot;en&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;Excellent &lt;a href=&quot;https://twitter.com/hashtag/PyMC3?src=hash&quot;&gt;#PyMC3&lt;/a&gt; talks at &lt;a href=&quot;https://twitter.com/hashtag/PyCon2017?src=hash&quot;&gt;#PyCon2017&lt;/a&gt; by &lt;a href=&quot;https://twitter.com/fonnesbeck&quot;&gt;@fonnesbeck&lt;/a&gt;: &lt;a href=&quot;https://t.co/iFbxjSz9C1&quot;&gt;https://t.co/iFbxjSz9C1&lt;/a&gt; &lt;a href=&quot;https://t.co/VhAJLpVQBR&quot;&gt;https://t.co/VhAJLpVQBR&lt;/a&gt; and &lt;a href=&quot;https://twitter.com/ericmjl&quot;&gt;@ericmjl&lt;/a&gt;: &lt;a href=&quot;https://t.co/MnkyWRivXs&quot;&gt;https://t.co/MnkyWRivXs&lt;/a&gt;&lt;/p&gt;&amp;mdash; Thomas Wiecki (@twiecki) &lt;a href=&quot;https://twitter.com/twiecki/status/866639588995063808&quot;&gt;May 22, 2017&lt;/a&gt;&lt;/blockquote&gt;&lt;/p&gt;
&lt;script async src=&quot;//platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;&lt;p&gt;Beyond that, the attendees seemed to like the talk too on the Twitterverse!&lt;/p&gt;
&lt;p&gt;&lt;blockquote class=&quot;twitter-tweet&quot; data-lang=&quot;en&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;Great introduction to Bayesian data analysis with PyMC3 by &lt;a href=&quot;https://twitter.com/ericmjl&quot;&gt;@ericmjl&lt;/a&gt; at &lt;a href=&quot;https://twitter.com/hashtag/PyCon2017?src=hash&quot;&gt;#PyCon2017&lt;/a&gt; &lt;a href=&quot;https://t.co/BjbR0tZ6AO&quot;&gt;https://t.co/BjbR0tZ6AO&lt;/a&gt;&lt;/p&gt;&amp;mdash; Matteo Visconti dOC (@MatteoVdOC) &lt;a href=&quot;https://twitter.com/MatteoVdOC/status/866413241907400704&quot;&gt;May 21, 2017&lt;/a&gt;&lt;/blockquote&gt;&lt;/p&gt;
&lt;script async src=&quot;//platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;&lt;p&gt;&lt;blockquote class=&quot;twitter-tweet&quot; data-lang=&quot;en&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;Awesome talk on Bayesian analysis by &lt;a href=&quot;https://twitter.com/ericmjl&quot;&gt;@ericmjl&lt;/a&gt;! Check out the material here: &lt;a href=&quot;https://t.co/HAPVYjKUhM&quot;&gt;https://t.co/HAPVYjKUhM&lt;/a&gt;&lt;/p&gt;&amp;mdash; Sarah Guido (@sarah_guido) &lt;a href=&quot;https://twitter.com/sarah_guido/status/866413261037555713&quot;&gt;May 21, 2017&lt;/a&gt;&lt;/blockquote&gt;&lt;/p&gt;
&lt;script async src=&quot;//platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;&lt;p&gt;&lt;blockquote class=&quot;twitter-tweet&quot; data-lang=&quot;en&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;An excellent concrete introduction to Bayesian stats. I&amp;#39;m really looking forward to working through the notebooks as well. &lt;a href=&quot;https://twitter.com/hashtag/PyCon2017?src=hash&quot;&gt;#PyCon2017&lt;/a&gt; &lt;a href=&quot;https://t.co/ec230Ui3iC&quot;&gt;https://t.co/ec230Ui3iC&lt;/a&gt;&lt;/p&gt;&amp;mdash; Leland McInnes (@leland_mcinnes) &lt;a href=&quot;https://twitter.com/leland_mcinnes/status/866437998253981696&quot;&gt;May 21, 2017&lt;/a&gt;&lt;/blockquote&gt;&lt;/p&gt;
&lt;script async src=&quot;//platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;&lt;p&gt;&lt;blockquote class=&quot;twitter-tweet&quot; data-lang=&quot;en&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;Problems, code and explanation - nice! &lt;a href=&quot;https://t.co/ulJEXfMAo0&quot;&gt;https://t.co/ulJEXfMAo0&lt;/a&gt;&lt;/p&gt;&amp;mdash; AV Speech Processing (@AV_SP) &lt;a href=&quot;https://twitter.com/AV_SP/status/866422943361794049&quot;&gt;May 21, 2017&lt;/a&gt;&lt;/blockquote&gt;&lt;/p&gt;
&lt;script async src=&quot;//platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;&lt;p&gt;&lt;blockquote class=&quot;twitter-tweet&quot; data-lang=&quot;en&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;Great summary slide &lt;a href=&quot;https://twitter.com/ericmjl&quot;&gt;@ericmjl&lt;/a&gt;! &lt;a href=&quot;https://t.co/JbFV4qrzCl&quot;&gt;pic.twitter.com/JbFV4qrzCl&lt;/a&gt;&lt;/p&gt;&amp;mdash; William Farmer (@willzfarmer) &lt;a href=&quot;https://twitter.com/willzfarmer/status/866415332264562688&quot;&gt;May 21, 2017&lt;/a&gt;&lt;/blockquote&gt;&lt;/p&gt;
&lt;script async src=&quot;//platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;&lt;p&gt;&lt;blockquote class=&quot;twitter-tweet&quot; data-lang=&quot;en&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;Last technical talk of &lt;a href=&quot;https://twitter.com/hashtag/PyCon2017?src=hash&quot;&gt;#PyCon2017&lt;/a&gt; and my brain is full. Excellent introduction to &lt;a href=&quot;https://twitter.com/hashtag/PyMc3?src=hash&quot;&gt;#PyMc3&lt;/a&gt; and Bayesian variable inference by &lt;a href=&quot;https://twitter.com/ericmjl&quot;&gt;@ericmjl&lt;/a&gt;&lt;/p&gt;&amp;mdash; Justin Gosses (@JustinGosses) &lt;a href=&quot;https://twitter.com/JustinGosses/status/866413619315056641&quot;&gt;May 21, 2017&lt;/a&gt;&lt;/blockquote&gt;&lt;/p&gt;
&lt;script async src=&quot;//platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;&lt;p&gt;&lt;blockquote class=&quot;twitter-tweet&quot; data-lang=&quot;en&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;Great talk from &lt;a href=&quot;https://twitter.com/ericmjl&quot;&gt;@ericmjl&lt;/a&gt; on PyMC3 making everything clear with concrete examples. &lt;a href=&quot;https://twitter.com/hashtag/pycon2017?src=hash&quot;&gt;#pycon2017&lt;/a&gt;&lt;/p&gt;&amp;mdash; Leland McInnes (@leland_mcinnes) &lt;a href=&quot;https://twitter.com/leland_mcinnes/status/866413119676891137&quot;&gt;May 21, 2017&lt;/a&gt;&lt;/blockquote&gt;&lt;/p&gt;
&lt;script async src=&quot;//platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;&lt;p&gt;&lt;blockquote class=&quot;twitter-tweet&quot; data-lang=&quot;en&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;.&lt;a href=&quot;https://twitter.com/ericmjl&quot;&gt;@ericmjl&lt;/a&gt; giving &lt;a href=&quot;https://twitter.com/hashtag/bayesian?src=hash&quot;&gt;#bayesian&lt;/a&gt; intuition at &lt;a href=&quot;https://twitter.com/hashtag/PyCon2017?src=hash&quot;&gt;#PyCon2017&lt;/a&gt; ; as always, a great, inspiring speaker &lt;a href=&quot;https://t.co/4qz9QeTArc&quot;&gt;https://t.co/4qz9QeTArc&lt;/a&gt;&lt;/p&gt;&amp;mdash; Hugo Bowne-Anderson (@hugobowne) &lt;a href=&quot;https://twitter.com/hugobowne/status/866407227518734337&quot;&gt;May 21, 2017&lt;/a&gt;&lt;/blockquote&gt;&lt;/p&gt;
&lt;script async src=&quot;//platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;&lt;p&gt;&lt;blockquote class=&quot;twitter-tweet&quot; data-lang=&quot;en&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;Foregoing formula, &lt;a href=&quot;https://twitter.com/ericmjl&quot;&gt;@ericmjl&lt;/a&gt; wants you to walk away w/intuition about Bayesian stats: &amp;quot;update beliefs having seen the evidence&amp;quot; &lt;a href=&quot;https://twitter.com/hashtag/pycon2017?src=hash&quot;&gt;#pycon2017&lt;/a&gt;&lt;/p&gt;&amp;mdash; Melissa @ #pycon2017 (@iff_or) &lt;a href=&quot;https://twitter.com/iff_or/status/866407192110350336&quot;&gt;May 21, 2017&lt;/a&gt;&lt;/blockquote&gt;&lt;/p&gt;
&lt;script async src=&quot;//platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;&lt;p&gt;It's very heartening to see how many people want to move into Bayes-land! The talk also happened to be the last in the session and last of the day, so I think many people were tired by that point and wanted to go to the final keynote. Thus, the only question came from my friend Hugo, with whom I also worked on a course at DataCamp, who asked about &quot;how we might communicate these ideas to, say, a manager.&quot; My thoughts on that were to report not a single number (e.g. the mean), but also the range, and communicate how the lower and upper bound of the range would affect bottomline decisions, or open up new opportunities (though I probably could have expressed this sentiment better).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;(4) Feeding Guido van Rossum.&lt;/strong&gt; Python's BDFL, &lt;a href=&quot;https://en.wikipedia.org/wiki/Guido_van_Rossum&quot;&gt;Guido van Rossum&lt;/a&gt;, wandered into the staff office asking to see whether the speaker ready room was open, because he was hungry and was looking for some snacks. We initially suggested the main conference hall, but later I ran out and called him back, because we had some English biscuits in the staff room, and we engaged in a short chat. That's when I had my star-awed moment! Was tempted to get a photo, but I figured he'd probably be fed up with people asking for photo ops, so I decided against it, hoping to be considerate for him. When he finished the biscuit, he said goodbye, and left the staff office. Amazing how everybody else just went about their own business while he was in the room; speaks to the lack of ego that PyCon celebrities have, and that sets a great example for the rest of the community!&lt;/p&gt;
&lt;p&gt;Once I'm back in Boston, I'm definitely going to catch up on the rest of PyCon. I heard that there were a lot of good talks that I missed while staffing the conference as FinAid co-chair, will have to make sure that YouTube playlist is set up!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2017/5/21/post-pycon-2017-thoughts/">
    <title type="text">Post-PyCon 2017 Thoughts</title>
    <id>urn:uuid:b9261a98-97bb-32e2-8124-8ea3cc1fefa7</id>
    <updated>2017-05-21T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2017/5/21/post-pycon-2017-thoughts/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;This year's PyCon 2017 is over! Well, for me at least, as I head back to Boston, a place I've had to call home for the past 6 years.&lt;/p&gt;
&lt;p&gt;I've noticed my Portland PyCons have felt different from my Montreal PyCons.&lt;/p&gt;
&lt;p&gt;In Montreal, I felt more like a taker, a newcomer, a beginner. In Portland, I felt more like someone who could finally give back to the community. If anything, I hope I've been able to encourage others to also give back to the community.&lt;/p&gt;
&lt;p&gt;In Montreal, with respect to the community, I felt like I had to slowly navigate a new landscape of networks with people. There, I met a bunch of people who first became my PyCon community mentors: Stuart Williams and Reuben Orduz, whose years of experience in the community and in life are way beyond mine, became long-distance friends with whom I would look forward to meeting with again at the next PyCon. Carol Willing, a fellow MIT alum whom I met at a SciPy conference, also likewise became a community mentor for me. They didn't have to do much: words of encouragement, encouraging us to contribute back while themselves leading the charge, and connecting people together.&lt;/p&gt;
&lt;p&gt;These two years in Portland, I've instead started to get involved with the internal organization of PyCon, volunteering a bit of my time on the Financial Aid committee. That's where I got to meet even more people in the community, and in person too! LVH and Ewa, a husband-and-wife team who have made many community contributions. Karan Goel, a software engineer at Google who led FinAid this year and whom I shadowed for taking on next year's FinAid chair role (I think we'll just share the duties again like this year). Kurt, PSF's treasurer who's been doing this for decades, and even at his age, still loves programming, and who loves black decaf coffee. Brandon Rhodes, who is a Python community celebrity for his eccentricity and entertaining talks, who gave me many words of encouragement as I rehearsed my PyCon talk. Ned Jackson Lovely, for whom no words other than &quot;positive energy radiating through everything he does&quot; can best describe him.&lt;/p&gt;
&lt;p&gt;I think the PyCon community has done the &quot;community building&quot; portion of coding really well, and I'm thankful to be able to be part of this community of people. At the end of the day, good code is about bringing a benefit to people. So at the end of the day, while programming is an act of making routine things efficient, it's ultimately still about people, not code in and of itself. Thank you, PyCon community, it's been really fun being a part of the community this far, and I'm looking forward to many more years too!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2017/5/15/thesis-defence-video/">
    <title type="text">Thesis Defence Video!</title>
    <id>urn:uuid:02fdb33e-142a-348e-bb0d-0ffb59e58369</id>
    <updated>2017-05-15T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2017/5/15/thesis-defence-video/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;About two weeks after being done, my thesis defence video is up on YouTube! It can be found here: &lt;a href=&quot;https://youtu.be/ePqhQusK-3Q?t=1m23s&quot;&gt;https://youtu.be/ePqhQusK-3Q?t=1m23s&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;My favourite parts are recollecting the thought of being scooped by someone else 4 years ago, saying that some people like doing sampling, and stating how the lessons from my first committee meeting have been passed on. Ahh, so many good memories!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2017/5/13/why-i-teach-coding-tutorials/">
    <title type="text">Why I Teach Coding Tutorials</title>
    <id>urn:uuid:01cd6084-2784-3ca8-9d1b-544c11804846</id>
    <updated>2017-05-13T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2017/5/13/why-i-teach-coding-tutorials/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;I'm very excited to be at PyCon! It's a bit of a personal challenge this year, as I'll be leading two tutorials, one on Network Analysis and one on Data Testing.&lt;/p&gt;
&lt;p&gt;With a bit of time on hand, I've done a bit of introspection as to why I love doing these tutorials. I think I can boil it down to a few broad themes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Reason 1: Learning.&lt;/strong&gt; When it comes to learning material, nothing beats having to teach it to someone else. This means I have to master the material in order to teach it responsibly to someone else.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Reason 2: Reputation.&lt;/strong&gt; Grounded on the foundation of having mastered the material I'm going to teach, getting out there helps me build a reputation for having both technical mastery and the ability to communicate the material out.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Reason 3: Networking.&lt;/strong&gt; By going to conferences where my tutorials are accepted, it's a great way to meet people and learn about the latest and greatest out there.&lt;/p&gt;
&lt;p&gt;My hope is wherever I end up working, I can continue this craftsmanship!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2017/5/4/pycon-2017-tutorials-and-talks-preview/">
    <title type="text">PyCon 2017: Tutorials and Talk Preview!</title>
    <id>urn:uuid:ac865ab7-55a1-3728-b8ce-da75a3e72949</id>
    <updated>2017-05-04T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2017/5/4/pycon-2017-tutorials-and-talks-preview/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;This year, I'll be at PyCon 2017 presenting two tutorials and one talk! I'm very excited to be attending!&lt;/p&gt;
&lt;p&gt;The first tutorial I will deliver is on network analysis. The GitHub repository is &lt;a href=&quot;https://github.com/ericmjl/Network-Analysis-Made-Simple&quot;&gt;online&lt;/a&gt;, and is the most mature of the three. This will be my 3rd year teaching the tutorial; I first developed the material in 2015, and have been refining it ever since. This year, I have great help from Mridul Seth, a student from India who has also been doing network analysis.&lt;/p&gt;
&lt;p&gt;The second tutorial I will be leading is on testing practices for data science. The GitHub repository is &lt;a href=&quot;https://github.com/ericmjl/data-testing-tutorial&quot;&gt;online&lt;/a&gt;, and will cover the use of automated tests for checking code and data integrity, as well as the use of visualization methods in EDA to sanity-check the data. The material is still in development right now, and I'm hoping to get good feedback from the Boston Python community when I dry-run it locally in the Boston area.&lt;/p&gt;
&lt;p&gt;My talk will be on Bayesian statistical analysis using PyMC3. As usual, the materials are available online on &lt;a href=&quot;https://github.com/ericmjl/bayesian-stats-talk&quot;&gt;GitHub&lt;/a&gt;. In it, I will cover the two most common types of statistical analysis problems - parameter estimation and comparison of treatment with controls, and demonstrate the process of reasoning through model building, implementing it in PyMC3, and interpreting the data.&lt;/p&gt;
&lt;p&gt;Really excited to be making three contributions back to the Python community. I've benefited much from the use of Python tools, and every PyCon I learn something new, so this is my little way of giving back!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2017/5/3/managing-conda-environments/">
    <title type="text">Managing conda environments</title>
    <id>urn:uuid:11bd7015-0c5f-3315-baba-568c31f1d9fa</id>
    <updated>2017-05-03T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2017/5/3/managing-conda-environments/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;I recently got around to hacking a system for managing my &lt;code&gt;conda&lt;/code&gt; environments better. Previously, my coding projects mostly relied on one master environment (with exceptions, e.g. &lt;code&gt;bokeh&lt;/code&gt; development, or my Network Analysis Made Simple tutorial), but conflicts started cropping up. Thus, I decided to separate out my environments. However, keeping track of which environments go with which projects began getting tedious.&lt;/p&gt;
&lt;p&gt;I thus decided to automate some of the steps involved in maintaining environments, and keep everything centrally managed so my brain doesn't overload. It involves a bit of GitHub and a bit of bash scripting, but altogether gives a ton of flexibility and control over keeping my environments updated.&lt;/p&gt;
&lt;p&gt;I start by keeping a central repository of &lt;code&gt;conda&lt;/code&gt; environment YAML specifications. Mine is kept &lt;a href=&quot;https://github.com/ericmjl/conda-envs&quot;&gt;here&lt;/a&gt;. Each YAML specification includes just the minimum set of packages that I need; &lt;code&gt;conda&lt;/code&gt; manages the dependencies.&lt;/p&gt;
&lt;p&gt;For example, my environment specification for Bayesian statistical analyses looks as such:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;bayesian&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# for Bayesian analysis&lt;/span&gt;
&lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;channels&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;!!python/tuple&lt;/span&gt;
&lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;conda-forge&lt;/span&gt;
&lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;defaults&lt;/span&gt;
&lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;ericmjl&lt;/span&gt;
&lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;dependencies&lt;/span&gt;&lt;span class=&quot;p p-Indicator&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;python=3.6&lt;/span&gt;
&lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;matplotlib&lt;/span&gt;
&lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;numpy&lt;/span&gt;
&lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;pandas&lt;/span&gt;
&lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;scipy&lt;/span&gt;
&lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;seaborn&lt;/span&gt;
&lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;pymc3&lt;/span&gt;
&lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;jupyter&lt;/span&gt;
&lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;jupyterlab&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, I've not pinned specific versions here, because I like to keep up with the latest stable releases. However, if version pinning is desired, it's totally possible to pin specific packages to particular versions, using the same syntax as I did for &lt;code&gt;python=3.6&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In each project repository, I have an &lt;code&gt;update_env.sh&lt;/code&gt; script, that looks something like this:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;wget https://raw.githubusercontent.com/ericmjl/conda-envs/master/lektor.yml -O environment.yml
conda env update -f environment.yml
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The key idea here is that I download only the relevant YAML file, export it as a generic &lt;code&gt;environment.yml&lt;/code&gt; file, and then run the &lt;code&gt;conda env update&lt;/code&gt; command on it to keep the environment up-to-date.&lt;/p&gt;
&lt;p&gt;Now, here's the magic. I hacked Christine Doig's &lt;code&gt;conda-auto-env&lt;/code&gt; script  to execute &lt;code&gt;update_env.sh&lt;/code&gt;, and then auto-activate the environment.&lt;/p&gt;
&lt;p&gt;If my environment needs change, I can always update the environment YAML spec file (e.g. &lt;code&gt;lektor.yml&lt;/code&gt;, or &lt;code&gt;bayesian.yml&lt;/code&gt;) in the central repository, and use that to automatically update individual project environments.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2017/4/28/defended/">
    <title type="text">Defended!</title>
    <id>urn:uuid:3e537c85-bdf2-3a78-94ee-7b0a49170d55</id>
    <updated>2017-04-28T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2017/4/28/defended/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;It is done! I have defended my doctoral thesis, supervised by Prof. Jonathan A. Runstadler, advised by Prof. Mark Bathe and Prof. Jukka-Pekka Onnela, conducted as a student in the Department of Biological Engineering at MIT.&lt;/p&gt;
&lt;p&gt;Looking back, it's been an amazingly fun journey, and I'll be updating this space with a more detailed account of the thesis defence process. Stay tuned!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2017/4/24/thesis-defence/">
    <title type="text">Thesis Defence!</title>
    <id>urn:uuid:a6f99998-8ca5-3fe4-bbc4-ec2413e0b713</id>
    <updated>2017-04-24T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2017/4/24/thesis-defence/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;My thesis defence announcement has been made to the department!&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Date:&lt;/strong&gt;  Thursday, April 27th, 2017&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Time:&lt;/strong&gt;  2:00 PM&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Location:&lt;/strong&gt;  56-614&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;&lt;b&gt;Identification of Reassortant Influenza Viruses at Scale - Algorithm and Applications&lt;/b&gt;&lt;/center&gt;&lt;center&gt;by&lt;/center&gt;&lt;center&gt;Eric Ma&lt;/center&gt;&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Reassortment is a reticulate evolutionary process that results in genome shuffling; the most prominent virus known to reassort is the influenza A virus. Methods to identify reassortant influenza viruses do not scale well beyond hundreds of isolates at a time, because they rely on phylogenetic reconstruction, a computationally expensive method. This thus hampers our ability to test systematically whether reassortment is associated with host switching events. In this thesis, I use phylogenetic heuristics to develop a new reassortment detection algorithm capable of finding reassortant viruses in tens of thousands viral isolates. Together with colleagues, we then use the algorithm to test whether reassortment events are over-represented in host switching events and whether reassortment is an alternative 'transmission strategy' for viral persistence.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Committee&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Jonathan Runstadler, Assistant Professor of Biological Engineering and Comparative Medicine, MIT (Advisor)&lt;/li&gt;
&lt;li&gt;Mark Bathe, Associate Professor of Biological Engineering, MIT (Committee Chair)&lt;/li&gt;
&lt;li&gt;Jukka-Pekka Onnela, Assistant Professor of Biostatistics, Harvard School of Public Health&lt;/li&gt;
&lt;/ul&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2017/4/21/moving-on-from-mit/">
    <title type="text">Moving on from MIT</title>
    <id>urn:uuid:d82b5e60-c156-3c44-a33d-32dcda309a66</id>
    <updated>2017-04-21T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2017/4/21/moving-on-from-mit/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;It's my final week before defending my thesis.&lt;/p&gt;
&lt;p&gt;The thesis has been submitted to my committee, and my slides, as usual, are being made until the last minute.&lt;/p&gt;
&lt;p&gt;I've had a number of conversations with a number of people, both in person and via email, and a few recurring themes start showing up.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;New Challenges&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Scientists are artists in some senses, computational scientists particularly, and I think I'm ready for a new challenge.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jobs&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;People invariably ask, &quot;What's next?&quot; I tell them that I've ruled out a &quot;traditional post-doc&quot;, and that  I'm not wedded to the academic ivory tower, I'm wedded to my wife, and so if the timing doesn't work out for independent research fellows positions, I'm jumping out. Well, as things turned out, yes, I'm jumping out, and I'm looking forward to this new journey!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Data Science&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A few months ago, I was pretty deflated with the job search. Interviews were slow to come, and I began to think that all the &quot;hype&quot; around life science DS was just that - hype - and that the demand wasn't there. A few months later, I'm proven wrong, and quite happily proven wrong too.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Insight&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I signed the Insight Health Data Science Fellows contract, to join them for 7 weeks in the summer. Many were perplexed - isn't Insight all about getting a job? Not really. For me, it's about meeting like-minded individuals and being able to network with them.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Interviews&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I've done interviews at a few places now, and the response has been very positive. No offers yet, but nonetheless these are all places where I can foresee myself being valued for what I can contribute, while having fun working with colleagues on new problems.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Feelings&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The defence, at this point, has this magical effect of inducing anxiety at times, and feeling like &quot;just another thing to do&quot; at other times. I'm not sure what to make of this.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Thesis&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;My thesis, as it turns out, is super duper short. The departmental average range is on the order of 100-400 pages. Mine stands at about 80, including references, with double-spaced text, and sometimes having one figure/table on one page. I think I've broken some departmental records here...&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Audience&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Someone asked, &quot;Who is your defence audience going to be?&quot; I've decided it'll be for my committee, who have supported my intellectual journey through infectious disease, computation, and data science. In some ways it's my &quot;final performance&quot; during grad school, a way of saluting their support.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;As the clock winds down, I'm reminded of that phrase from Philippians:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;... forgetting what lies behind and straining forward to what lies ahead... (&lt;a href=&quot;https://www.bible.com/111/PHP.3.13-14&quot;&gt;Philippians 3:13-14&lt;/a&gt;)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Laurels, yes, I have acquired indeed. MIT is no slouchy place to graduate from, and it's a privilege to have this degree. I've had the privilege and opportunity to remake my skillset while having fun along the way. But it's no time to rest on my laurels. It's time to start serving the world through the skillset I have been given. Onward!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2017/4/7/staying-nimble/">
    <title type="text">Staying Nimble</title>
    <id>urn:uuid:0effa933-9e9b-351f-9588-6fdb19783766</id>
    <updated>2017-04-07T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2017/4/7/staying-nimble/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;As I wrap up grad school, one thought recurrently comes to mind: as we grow older, do we intrinsically lose mental agility, nimbleness and ability to learn? Or is it because of external factors that cause one to become less adventurous, less curious, and hence, less able to learn new things?&lt;/p&gt;
&lt;p&gt;I find myself having flashes of fear crop up as I observe those who are older than me. They are stuck in between a hard place and a rock. Their skills may be on the wane (in terms of demand). They want to learn new things, but have to continue doing the old things to keep the ship afloat. I can imagine, it feels tough to be in that kind of position! Without the safety net to take a risk and learn something new, they may be stuck in a dying trade. Will I have to face the same fate? Is it avoidable, or even perhaps, surmountable?&lt;/p&gt;
&lt;p&gt;The even harder part is staying intellectually nimble, and not being stuck in particular ways of thought. Is that an intrinsic property of aging, or not?&lt;/p&gt;
&lt;p&gt;Just some questions I've been pondering...&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2017/3/28/thesis/">
    <title type="text">Thesis</title>
    <id>urn:uuid:4d271fbc-91f1-3867-9da4-5134e9cfc245</id>
    <updated>2017-03-28T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2017/3/28/thesis/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;I've finally turned in a polished draft of my thesis (&lt;a href=&quot;http://ericmjl.github.io/thesis&quot;&gt;HTML&lt;/a&gt; or &lt;a href=&quot;http://www.ericmjl.com/cv/thesis.pdf&quot;&gt;PDF&lt;/a&gt;) to my committee! My thesis topic is on the development of an algorithm to identify reassortant influenza viruses from large sequence databases, and its application to the study of influenza's evolution and ecology.&lt;/p&gt;
&lt;p&gt;Well, actually, it was last week when I finished it, but I've been doing the job hunt the past week that I've delayed on writing this blog post.&lt;/p&gt;
&lt;p&gt;Apart from the written summary of the work that I've been doing, I wanted to simultaneously write for PDFs and for the web, so I started assembling a software toolchain that compiles my raw markdown files, converts figures from PDF to JPG, and simultaneously builds the PDF and the HTML versions. A lot of Python packages, including &lt;a href=&quot;https://github.com/mplewis/csvtomd&quot;&gt;&lt;code&gt;csv2md&lt;/code&gt;&lt;/a&gt;, the &lt;a href=&quot;https://github.com/tomduck/pandoc-xnos&quot;&gt;pandoc-xnos&lt;/a&gt; series, and non-Python tools, including ImageMagick (&lt;a href=&quot;https://www.imagemagick.org/script/index.php&quot;&gt;https://www.imagemagick.org/script/index.php&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Yes, I know I could have done most of this with Authorea, but being me, building things and doing reverse engineering is also kind of fun! (Especially for learning purposes.)&lt;/p&gt;
&lt;p&gt;I hope you enjoy my thesis!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2017/3/11/default-bayesian-models/">
    <title type="text">&quot;Default&quot; Bayesian Models</title>
    <id>urn:uuid:ae4ec6e0-f33d-39a8-bf22-43b0eabb0bb4</id>
    <updated>2017-03-11T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2017/3/11/default-bayesian-models/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;As a positive distraction from my thesis writing, I've been thinking a bit about the statistical crisis in biomedical sciences and psychology research, and how it might be mitigated.&lt;/p&gt;
&lt;p&gt;A number of &lt;a href=&quot;https://profiles.stanford.edu/john-ioannidis&quot;&gt;opponents of p-values&lt;/a&gt; and &lt;a href=&quot;http://www.indiana.edu/~kruschke/&quot;&gt;proponents of Bayesian inference&lt;/a&gt; have influenced my thinking around this issue. As such I have come to the conclusion that Bayesian estimation and inference should be more widely used, because it essentially comes with interpretable uncertainty built into the inference philosophy.&lt;/p&gt;
&lt;p&gt;I think one thing preventing adoption of Bayesian inference methods is their flexibility (read: complexity). How does one compose an model with little grounding in statistics?&lt;/p&gt;
&lt;p&gt;To address this problem, I've started putting together &lt;a href=&quot;https://github.com/ericmjl/bayesian-analysis-recipes&quot;&gt;Jupyter notebooks&lt;/a&gt; showing common problems in the experimental sciences and a sensible default model that one can use for that kind of problem.&lt;/p&gt;
&lt;p&gt;For me, a recurrent (and very interesting) theme came up. The nature of probabilistic graphical models is such that if we are able to forward-simulate how the data may be generated, then given the data and a loss function, fitting the data is merely a matter of optimization. The core idea behind these notebooks, then, is that there are a small number of &quot;generic&quot; models of how data may be generated that can cover a large proportion of scenarios, particularly in scenarios where we don't have sufficiently good theory to forward-simulate the complex data-generating distribution underlying the data.&lt;/p&gt;
</content>
  </entry>
</feed>
