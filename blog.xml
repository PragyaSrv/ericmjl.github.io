<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title type="text">Eric Ma's Blog</title>
  <id>urn:uuid:8e5496e4-8606-3632-a35c-1d9694b4313d</id>
  <updated>2020-02-13T00:00:00Z</updated>
  <link href="http://www.ericmjl.com/blog/" />
  <link href="http://www.ericmjl.com/blog.xml" rel="self" />
  <author>
    <name></name>
  </author>
  <generator uri="https://github.com/ajdavis/lektor-atom" version="0.3">Lektor Atom Plugin</generator>
  <entry xml:base="http://www.ericmjl.com/blog/2020/2/13/one-weird-trick-to-speed-up-your-tensorflow-model-100x/">
    <title type="text">One Weird Trick to Speed Up Your TensorFlow Model 100X...</title>
    <id>urn:uuid:4f298f94-5a92-3d64-80a9-72c43ed5d026</id>
    <updated>2020-02-13T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2020/2/13/one-weird-trick-to-speed-up-your-tensorflow-model-100x/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;You’ve got a TensorFlow 1.13 model on hand, 
written by an academic group, 
and you know it’s going to be useful, 
because you’ve already done some preliminary experiments with it. 
The model takes in protein sequences, 
and returns vectors, one per protein sequence. 
The vectors turn out to be excellent descriptors 
for downstream supervised learning, 
to predict protein activity from sequence data.&lt;/p&gt;
&lt;p&gt;The only problem? 
It’s slow. 
&lt;em&gt;Excruciatingly slow&lt;/em&gt;. 
Not just on CPU. 
It’s slow on your local GPU as well. 
You can’t run this locally without waiting for hours 
to process the thousands of protein sequences that you have on hand.&lt;/p&gt;
&lt;h2 id=&quot;slow?-what-now?&quot;&gt;Slow? What now?&lt;/h2&gt;&lt;p&gt;What do we do, then? 
Install a newer GPU with faster processors and more RAM? 
“Lift and shift” onto cloud GPUs?&lt;/p&gt;
&lt;p&gt;I’d argue that all of those are evil choices, 
because premature optimization is the root of all evil (Donald Knuth). 
Moving the model onto a GPU 
without knowing the root cause of the model’s slowness 
could be construed as a form of premature optimization, 
because we are prematurely moving to more powerful hardware 
without first finding out whether it can be improved on existing ones. 
Moving the model onto a &lt;em&gt;cloud&lt;/em&gt; GPU, 
renting another person’s computer 
while not maximizing pushing the model to its extremes locally, 
is probably even worse, 
because we end up with more variables to debug. 
By the aforementioned premises and Knuth’s logical conjugation, 
those options are, therefore, evil choices.&lt;/p&gt;
&lt;p&gt;Rather provocatively, 
I’d like to suggest that one right option 
when faced with a very slow deep learning model 
is to make it fast on a &lt;em&gt;single CPU&lt;/em&gt;. 
If we make the model fast locally on a single CPU, 
then we might have a good chance 
of dissecting the source of the model’s slowness, 
and thus make it really fast on different hardware.&lt;/p&gt;
&lt;p&gt;This is exactly what we did with the UniRep model. 
Our protein engineering intern in Basel, Arkadij, 
had already done the groundwork testing the model 
on our high performance computing cluster. 
It was slow in our hands, 
and so we decided it was time to really dig into the model.&lt;/p&gt;
&lt;h2 id=&quot;reimplementation&quot;&gt;Reimplementation&lt;/h2&gt;&lt;p&gt;Because the model is an RNN model that processes sequential data, 
we need to know how the model processes one window of sequence first. 
In the deep learning literature, this is known as an “RNN cell”. 
We isolated that part of the RNN, 
and carefully translated each line of code into NumPy. 
Because it processes one “window” or &lt;strong&gt;”step”&lt;/strong&gt; at a time, 
we call it &lt;code&gt;mlstm1900_step&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Because the RNN cell is “scanned” over a single sequence, 
we then defined the function that processes a single sequence. 
To do this, we took advantage of the &lt;code&gt;jax.lax&lt;/code&gt; submodule’s &lt;code&gt;scan&lt;/code&gt; function, 
which literally “scans” the RNN cell over the entire sequence 
the way TensorFlow’s RNN cell would be scanned. 
This basically is a vectorized for-loop. 
Together, that defined the function call that processes a single sequence. 
This, incidentally, gives us the “batch-“ or “sample-wise” RNN function, 
&lt;code&gt;mlstm1900_batch&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Finally, we need a function that processes multiple sequences 
that are the same length. 
Here, we need the function to be vectorized, 
so that we do not incur a Python loop penalty 
for processing sequences one at a time. 
This forms the &lt;code&gt;mlstm1900&lt;/code&gt; layer, 
which, semantically speaking, takes in a batch of sequences 
and returns a batch of outputs.&lt;/p&gt;
&lt;h2 id=&quot;testing&quot;&gt;Testing&lt;/h2&gt;&lt;p&gt;So, does the reimplementation work in the same way as the original? 
Absolutely. We passed an &lt;em&gt;attrappe&lt;/em&gt; (Google it, it’s German) sequence through the model, 
and checked that the reps were identical to the original - which they were. 
But don’t take my word for it, 
check out the &lt;a href=&quot;https://elarkk.github.io/jax-unirep/&quot;&gt;preprint&lt;/a&gt; we wrote to document this.&lt;/p&gt;
&lt;p&gt;Besides that, 
we also added tests and elaborate docstrings 
for the functions in there, 
so anybody reading the model source code 
can know what the &lt;em&gt;semantic meaning&lt;/em&gt; of each of the tensors’ axes are. 
Having wrestled through undocumented academic code, 
knowing what input tensor dimensions were supposed to be 
were one of the hardest things to grok 
from looking at a relatively poorly undocumented model.&lt;/p&gt;
&lt;p&gt;By carefully reimplementing the deep learning model in pure JAX/NumPy, 
we were able to achieve approximately 100X speedup 
over the original implementation on a single CPU. 
Given JAX’s automatic compilation to GPU and TPU, 
the speed improvements we could obtain might be even better, 
though we have yet to try it out. 
Again, don't take my word at it - 
check out the &lt;a href=&quot;https://elarkk.github.io/jax-unirep/&quot;&gt;preprint&lt;/a&gt; 
if you're curious!&lt;/p&gt;
&lt;p&gt;I suspect that by reimplementing the model in NumPy, 
we basically eliminated the TensorFlow graph compilation overhead, 
which was the original source of slowness. 
By using some of JAX’s tricks, including &lt;code&gt;lax.scan&lt;/code&gt; and &lt;code&gt;jax.vmap&lt;/code&gt;, 
we could take advantage of vectorized looping 
that the JAX developers built into the package,
thus eliminating Python looping overhead.&lt;/p&gt;
&lt;h2 id=&quot;distribution&quot;&gt;Distribution&lt;/h2&gt;&lt;p&gt;So, now, how do we get the model distributed to end-users? 
This is where thinking about the API matters. 
I have this long-held opinion that data scientists 
need some basic proficiency in software skills, 
as we invariably end up doing tool-building whether we want to or not. 
Part of software skills involves knowing 
how to package the model for use in a variety of settings, 
and thinking about how the end-user would use it. 
Then, only, have we made a &lt;strong&gt;data product&lt;/strong&gt; that is of use to others.&lt;/p&gt;
&lt;p&gt;Two settings we thought of are the Python user setting, and a web user setting.&lt;/p&gt;
&lt;h3 id=&quot;python-user-setting&quot;&gt;Python-user setting&lt;/h3&gt;&lt;p&gt;Serving Python users means building &lt;strong&gt;&lt;code&gt;pip&lt;/code&gt;-installable packages&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;In the Python world, 
it’s idiomatic to consider a Python package 
as the unit of software that is shipped to others. 
Some packages are large (think NumPy), 
others are diverse (think SciPy), 
some are both (e.g. &lt;code&gt;scikit-learn&lt;/code&gt;), 
and yet others do one and only one thing well (think &lt;code&gt;humanize&lt;/code&gt;). 
UniRep probably falls within to the last category.&lt;/p&gt;
&lt;p&gt;To fit this idiom, 
we decided to package up the model as a Python-style package 
that is pip-installable into the environment of an end-user. 
While we develop the package, 
it is &lt;code&gt;pip&lt;/code&gt;-installable from our GitHub repository 
(where we also give full credit and reference to the original authors for the original).
We might release it to PyPI, with licenses and the likes as well.&lt;/p&gt;
&lt;p&gt;Moreover, we needed a &lt;strong&gt;nice API&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;With UniRep, there’s basically two things one would want to do:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Compute “reps” for a sequence conditioned on the pre-trained weights, and &lt;/li&gt;
&lt;li&gt;Evolutionarily-tune (&quot;evo-tune&quot;) the model weights to compute more locally-relevant reps.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As such, we designed a &lt;code&gt;get_reps(seqs)&lt;/code&gt; API 
that would correctly process single sequences, 
multiple sequences of same lengths, 
and multiple sequences of varying lengths, 
while not needing the user to think about that.&lt;/p&gt;
&lt;p&gt;For evolutionary tuning, 
we desired that the API worked regardless of whether 
the user tuned on sequences of equal or varying lengths. 
In other words, 
we wanted to wrap the complexity of handling sequences of multiple lengths 
away from the end-user. 
This is something we are working on right now, 
but at least the API sketch as it stands now 
gives the end-user a single &lt;code&gt;evotune(sequences)&lt;/code&gt; function to work with. 
Designing it this way allows us to work backwards from the outside-in, 
starting with the desired API and flexibly designing the inner parts. 
A friend of mine, Ex-Googler Jesse Johnson at Cellarity, 
&lt;a href=&quot;https://medium.com/@jejo.math/writing-software-from-the-outside-in-e5359f60fa30&quot;&gt;wrote an excellent blog post on this&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&quot;web-api&quot;&gt;Web API&lt;/h3&gt;&lt;p&gt;With a &lt;code&gt;pip&lt;/code&gt;-installable package on-hand, 
we could then depend on it to build other single-use tools. 
For example, we built a web API for the model by using &lt;a href=&quot;https://fastapi.tiangolo.com/&quot;&gt;FastAPI&lt;/a&gt;, 
and designed it to return the calculated representations for a single sequence 
based on the pre-trained weights. 
Because evo-tuning is computationally expensive, 
we don’t plan to offer that on the web API. 
The web API is also packaged up as a &lt;code&gt;pip&lt;/code&gt;-installable package, 
and can be installed inside a Docker container, 
thus forming the basis of a deep learning-powered micro-service 
that pre-calculates “standard”, non-tuned representations 
of every new protein we might encounter.&lt;/p&gt;
&lt;h2 id=&quot;lessons-learned&quot;&gt;Lessons Learned&lt;/h2&gt;&lt;h3 id=&quot;on-the-fragility-of-deep-learning-models&quot;&gt;On the fragility of deep learning models&lt;/h3&gt;&lt;p&gt;Our modern way of communicating the architecture of deep learning models 
is still a work-in-progress, with overloaded terms and ambiguity present. 
As such, there were minor details that we reimplemented incorrectly 
that incidentally also had a major impact on the correctness of the reimplementation. 
For example, the &lt;code&gt;sigmoid()&lt;/code&gt; function in TensorFlow 
was different from the other implementations that we had seen in multiple codebases 
(we detail this in our preprint), 
yet both are colloquially described as “sigmoids”.&lt;/p&gt;
&lt;p&gt;We also know from adversarial examples 
that deep learning models are notoriously fragile 
against inputs that have noise added to them. 
This has implications in the design of the API to the model. 
If we design the user-facing API to accept tensors rather than strings, 
then we leave the burden of correctly translating string inputs into tensors 
(with semantically correct dimensions) on the end-user. 
If the end-user processed the strings in a way that the model is not robust against, 
it will handle the processed tensors in a way that may be incorrect for their purposes; 
yet, there is no robust way to write a unit test against this. 
The key problem here is that the semantic meaning of the tensors 
cannot be easily and automatically tested.&lt;/p&gt;
&lt;p&gt;As such, in order for the user-facing API of the model to be “friendly” (so to speak), 
it must accept data in a format that the intended end-user will be handling. 
This is nothing new in the world of software development, to be user-centric. 
At the same time, because of its small size and the way we structure our codebase, 
all of the internals are easily accessible, 
and how they fit together is also not difficult to understand.&lt;/p&gt;
&lt;h3 id=&quot;why-tools-and-not-full-apps?&quot;&gt;Why tools, and not full apps?&lt;/h3&gt;&lt;p&gt;The environment that we're in is a research environment. 
That, by definition, means we're always doing non-standard things. 
Moreover, a model is usually but one small component 
in a large interconnected web of tools 
that come together to make a useful, consumable app. 
Rather than try to go end-to-end, 
it makes more sense to produce modular parts that fit well with one another. 
The mantra I like to keep in mind here is:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Modularity and composability give us the flexibility for creativity.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;And if creativity is not the heart of research, what then is?&lt;/p&gt;
&lt;h3 id=&quot;if-you-want-to-accelerate-a-deep-learning-model...&quot;&gt;If you want to accelerate a deep learning model...&lt;/h3&gt;&lt;p&gt;…&lt;em&gt;how about making sure it first runs well on a single CPU?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;But beyond that, 
also consider that whole frameworks 
and their associated overheads might not necessarily be what you need. 
In our case, we didn't need the entire TF framework to get the model reimplemented. 
We only really needed XLA (pre-compiled in &lt;code&gt;jaxlib&lt;/code&gt; for our systems) 
and the NumPy API to get it right. 
Also consider whether a framework is getting in the way of your modelling work, 
or if it's helping. 
(If it's the latter, keep using what you're using!)&lt;/p&gt;
&lt;h3 id=&quot;encouraging-a-co-creation-mindset&quot;&gt;Encouraging a co-creation mindset&lt;/h3&gt;&lt;p&gt;Co-creation is &lt;em&gt;the&lt;/em&gt; ethos of the open source world, 
where unrelated people come together and build software 
that is useful to others beyond themselves. 
I've experienced it many times on the &lt;code&gt;pyjanitor&lt;/code&gt; and &lt;code&gt;nxviz&lt;/code&gt; projects. 
Technically, nobody has to ask for permission to fork a project, 
make changes, and even submit a pull request back. 
Of course, we do so anyways, 
just for the polity of being courteous to the original creator - 
it's always good to let someone know something's coming their way.&lt;/p&gt;
&lt;p&gt;Pair-coding, as it turns out, 
happens to be a wonderful way to encourage co-creation. 
Much of the reimplementation work 
was done pair coding with my co-author, Arkadij Kummer, 
who lives and works in our Basel site. 
Yes, we're separated by six hours of time, 
but that doesn't stop us 
from using all of the good internet-enabled tools available to us 
to collaborate together. 
We used MS Teams at work for video chatting, 
and sometimes just did VSCode with VSLiveShare (with audio) 
to develop code together. 
All work was version-controlled and shared via Git, 
which means we could work asynchronously when needed as well. 
The time spent pair coding is time where knowledge is shared, 
trust is built, 
and mentoring relationships are forged. 
In an upcoming blog post, 
I will write about pair coding as a practice in data science, 
particularly for levelling-up junior members of the data science team 
and sharing knowledge between peers.&lt;/p&gt;
&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;&lt;p&gt;I hope you enjoyed reading about our journey reimplementing UniRep in JAX. 
Keep in mind that JAX isn't necessarily always going to speed up your TF model 100X; 
I'm clearly being facetious in writing it that way. 
JAX and TF share XLA underneath the hood, 
and XLA is really the secret sauce that makes the models execute fast. 
The only difference is TF1.x needed this compilation time, 
while JAX does it just-in-time (when ordered to do so). 
That said, because JAX gets us autodiff on the NumPy API, 
it's a super productive research and teaching tool 
with a familiar API to many Pythonistas. 
My hope is this post encourages you to try it out; happy experimenting!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2020/1/18/create-your-own-auto-publishing-slides-with-reveal-md-and-travis-ci/">
    <title type="text">Create your own auto-publishing slides with reveal-md and Travis CI</title>
    <id>urn:uuid:5530170e-4cb1-38c4-a996-92fafb9c7775</id>
    <updated>2020-01-18T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2020/1/18/create-your-own-auto-publishing-slides-with-reveal-md-and-travis-ci/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;For my PyData Ann Arbor Meetup talk, I decided to use &lt;a href=&quot;https://github.com/webpro/reveal-md&quot;&gt;&lt;code&gt;reveal-md&lt;/code&gt;&lt;/a&gt; and a Markdown file to generate my slides. Here, I'd like to write about how I used &lt;code&gt;reveal-md&lt;/code&gt; and Travis CI to continually publish my slides as I updated them, thus making them accessible to everybody on the web.&lt;/p&gt;
&lt;h2 id=&quot;what-is-reveal-md-?&quot;&gt;What is &lt;code&gt;reveal-md&lt;/code&gt;?&lt;/h2&gt;&lt;p&gt;&lt;code&gt;reveal-md&lt;/code&gt; is nothing more than a live web server for that converts Markdown files into &lt;code&gt;reveal.js&lt;/code&gt; slides that can be hosted via a web server, static site, or PDF.&lt;/p&gt;
&lt;h2 id=&quot;why-reveal-md-?&quot;&gt;Why &lt;code&gt;reveal-md&lt;/code&gt;?&lt;/h2&gt;&lt;p&gt;I much prefer to use Markdown to write my slides, as doing so comes with one big benefit: I am focused on the content that I want to deliver, and not the less important details that are easy to screw up (animations, positioning, etc.). Constraining what's accessible to me forces me to be extremely clear and succinct on what I'm trying to communicate. And if I really desired anything fancier, I could weave in some HTML with no issue.&lt;/p&gt;
&lt;h2 id=&quot;create-your-own-auto-publishing-reveal-md-slides&quot;&gt;Create your own auto-publishing &lt;code&gt;reveal-md&lt;/code&gt; slides!&lt;/h2&gt;&lt;p&gt;Let’s walk through the steps needed to make this a reality for you!&lt;/p&gt;
&lt;h3 id=&quot;create-a-new-repository&quot;&gt;Create a new repository&lt;/h3&gt;&lt;p&gt;On GitHub, create a new repository that has a nice and informative name. (For now, we’ll just refer to that repository as &lt;code&gt;my-talk&lt;/code&gt; for convenience.)&lt;/p&gt;
&lt;h3 id=&quot;get-setup-locally&quot;&gt;Get setup locally&lt;/h3&gt;&lt;p&gt;To get setup, you need to make sure that &lt;code&gt;reveal-md&lt;/code&gt; is on your &lt;code&gt;PATH&lt;/code&gt;. I choose to use &lt;code&gt;conda&lt;/code&gt; environments to manage my packages, so I have a slightly convoluted way of doing this, by using &lt;code&gt;conda&lt;/code&gt; to install &lt;code&gt;nodejs&lt;/code&gt; (which installs the &lt;code&gt;npm&lt;/code&gt; node package manager), followed by using the node package manager to install &lt;code&gt;reveal-md&lt;/code&gt;. We first start by preparing an &lt;code&gt;environment.yml&lt;/code&gt; specification file that &lt;code&gt;conda&lt;/code&gt; can use to build your environment:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;my_talk&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;conda-forge&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;dependencies&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;python=3.8&lt;/span&gt;
&lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;nodejs&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, we can execute the installation commands.&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Installation commands&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Create environment&lt;/span&gt;
conda env create -f environment.yml

&lt;span class=&quot;c1&quot;&gt;# Activate environment&lt;/span&gt;
conda activate my_talk

&lt;span class=&quot;c1&quot;&gt;# Install reveal-md&lt;/span&gt;
npm install -g reveal-md
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To learn more about &lt;code&gt;conda&lt;/code&gt; hacks that can improve your efficiency, I &lt;a href=&quot;https://ericmjl.github.io/blog/2018/12/25/conda-hacks-for-data-science-efficiency/&quot;&gt;have a blog post&lt;/a&gt; that you can reference.&lt;/p&gt;
&lt;h3 id=&quot;write-your-slides&quot;&gt;Write your slides&lt;/h3&gt;&lt;p&gt;Before we go onto the automation, it’s important that you get a feel for the workflow so you know what’s being automated.&lt;/p&gt;
&lt;p&gt;Let’s write a simple Markdown file that has two slides:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;---
title: My Fancy Talk!
---

&lt;span class=&quot;gh&quot;&gt;#&lt;/span&gt; My Fancy Talk

Speaker Name

Date

---

&lt;span class=&quot;gu&quot;&gt;##&lt;/span&gt; Hello!
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Save it as &lt;code&gt;slides.md&lt;/code&gt;. The filename isn’t special, it’s just convenient to remember.&lt;/p&gt;
&lt;p&gt;To see more of what &lt;code&gt;reveal.js&lt;/code&gt; can do, check out the &lt;a href=&quot;https://github.com/hakimel/reveal.js&quot;&gt;RevealJS GitHub repository&lt;/a&gt;!&lt;/p&gt;
&lt;h3 id=&quot;preview-your-slides&quot;&gt;Preview your slides&lt;/h3&gt;&lt;p&gt;To serve it up, run the following command at your terminal:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;reveal-md slides.md  &lt;span class=&quot;c1&quot;&gt;# replace with the filename of your slides&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Your browser should now pop open, and your slides will be there! &lt;code&gt;reveal.js&lt;/code&gt; made simple, thanks to &lt;code&gt;reveal-md&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&quot;continue-editing-your-slides&quot;&gt;Continue editing your slides&lt;/h3&gt;&lt;p&gt;Now, you can continue editing your slides, keeping in mind the following pointers.&lt;/p&gt;
&lt;p&gt;Firstly, &lt;code&gt;---&lt;/code&gt; (three of them) denotes horizontal slide transition, while &lt;code&gt;----&lt;/code&gt; (four of them) denotes vertical slide transitions. Use this to organize your content.&lt;/p&gt;
&lt;p&gt;Secondly, to progressively reveal pointers on a slide, you need to add the following HTML comment right after the element. For example, to show bullet points progressively:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;-&lt;/span&gt; Bullet Point 1
&amp;lt;!-- .element class=&amp;quot;fragment&amp;quot; --&amp;gt;
&lt;span class=&quot;k&quot;&gt;-&lt;/span&gt; Bullet Point 2
&amp;lt;!-- .element class=&amp;quot;fragment&amp;quot; --&amp;gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id=&quot;aside:-fancier-items&quot;&gt;Aside: fancier items&lt;/h3&gt;&lt;p&gt;If you need fancier things, you can weave in HTML at your own convenience. For example, I embedded an HTML table to organize &lt;a href=&quot;https://github.com/ColCarroll/imcmc&quot;&gt;&lt;code&gt;imcmc&lt;/code&gt;&lt;/a&gt; logos that I had previously compiled.&lt;/p&gt;
&lt;h3 id=&quot;get-travis-ci-setup&quot;&gt;Get Travis CI setup&lt;/h3&gt;&lt;p&gt;You’ll now want to create a &lt;code&gt;.travis.yml&lt;/code&gt;, which commands Travis to do things. It’s generally nothing more than a collection of bash commands that are executed in order. An example Travis configuration file from my &lt;a href=&quot;https://ericmjl.github.io/testing-for-data-scientists&quot;&gt;data science testing talk&lt;/a&gt; looks like this:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;language&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;python&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;python&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# We don&amp;#39;t actually use the Travis Python, but this keeps it organized.&lt;/span&gt;
    &lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;3.5&amp;quot;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;install&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# We do this conditionally because it saves us some downloading if the&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# version is the same.&lt;/span&gt;
    &lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh;&lt;/span&gt;
    &lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;bash miniconda.sh -b -p $HOME/miniconda&lt;/span&gt;
    &lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;export PATH=&amp;quot;$HOME/miniconda/bin:$PATH&amp;quot;&lt;/span&gt;
    &lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;hash -r&lt;/span&gt;
    &lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;conda config --set always_yes yes --set changeps1 no&lt;/span&gt;
    &lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;conda update -q conda&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Useful for debugging any issues with conda&lt;/span&gt;
    &lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;conda info -a&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Install Python and required packages.&lt;/span&gt;
    &lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;conda env create -f environment.yml&lt;/span&gt;
    &lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;source activate testing-for-data-scientists&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Install reveal-md&lt;/span&gt;
    &lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;npm install -g reveal-md&lt;/span&gt;


&lt;span class=&quot;nt&quot;&gt;script&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Create the docs directory. This is where we will be publishing from&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# (see the &amp;quot;deploy&amp;quot; section below).&lt;/span&gt;
    &lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;mkdir -p docs/&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Use reveal-md to generate static docs.&lt;/span&gt;
    &lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;reveal-md slides.md --static docs --disable-auto-open --theme white&lt;/span&gt;
    &lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;cp -r assets docs/.&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Use reveal-md to generate PDF.&lt;/span&gt;
    &lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;reveal-md slides.md --disable-auto-open --theme white --print slides.pdf&lt;/span&gt;
    &lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;cp slides.pdf docs/.&lt;/span&gt;


&lt;span class=&quot;c1&quot;&gt;# This is an example to deploy to a branch through Travis.&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;deploy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;provider&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;pages&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;skip-cleanup&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;true&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;github-token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;$GITHUB_TOKEN&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# Set in the settings page of your repository, as a secure variable&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;keep-history&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;true&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# We read the master branch&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;on&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;nt&quot;&gt;branch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;master&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Take the docs/ directory&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;local-dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;docs&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Publish to the gh-pages branch&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;target-branch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;gh-pages&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;true&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You’ll notice that the commands we want Travis to execute are basically the same as those we executed manually. The only difference now is that we command &lt;code&gt;reveal-md&lt;/code&gt; to build a static site under the directory &lt;code&gt;site/&lt;/code&gt;, which we then command Travis to push to GitHub pages.&lt;/p&gt;
&lt;p&gt;In the &lt;code&gt;deploy&lt;/code&gt; section, we are specifying to Travis that we want all of the content under the directory &lt;code&gt;site/&lt;/code&gt; to be pushed to the &lt;code&gt;gh-pages&lt;/code&gt; branch of our repository. (We have not yet connected Travis to our repo; that will happen next, so sit tight!)&lt;/p&gt;
&lt;p&gt;Notice also the &lt;code&gt;$GITHUB_TOKEN&lt;/code&gt; environment variable: we need to declare that as well. The &lt;code&gt;GITHUB_TOKEN&lt;/code&gt; is an authentication token that GitHub will recognize when Travis CI pushes the &lt;code&gt;site/&lt;/code&gt; directory to &lt;code&gt;gh-pages&lt;/code&gt;. Because underneath the hood we are using &lt;code&gt;bash&lt;/code&gt; syntax in the YAML file, when we declare the &lt;code&gt;GITHUB_TOKEN&lt;/code&gt;, we do it without the &lt;code&gt;$&lt;/code&gt; symbol, but when we need to grab it from the environment, we include the &lt;code&gt;$&lt;/code&gt; symbol, just as in regular plain old &lt;code&gt;bash&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&quot;get-a-github-deploy-token&quot;&gt;Get a GitHub deploy token&lt;/h3&gt;&lt;p&gt;Under your repository settings, generate a deploy/“personal access” token. Exact docs are &lt;a href=&quot;https://help.github.com/en/github/authenticating-to-github/creating-a-personal-access-token-for-the-command-line&quot;&gt;here&lt;/a&gt;, so in the spirit of “don’t repeat yourself” and “learn to read the docs”, I will encourage you to read them.&lt;/p&gt;
&lt;p&gt;Once you have generated a deploy key, copy them somewhere - you will need it later. (Tip: also make sure you don’t accidentally save it to disk!)&lt;/p&gt;
&lt;h3 id=&quot;connect-travis-ci-to-your-repository&quot;&gt;Connect Travis CI to your repository&lt;/h3&gt;&lt;p&gt;On Travis CI, connect your Travis CI account to GitHub, and then enable Travis to look for changes on the &lt;code&gt;my-talk&lt;/code&gt; repository. Generally, this is done by going to your user settings, and searching for “Legacy Services Integration”, then toggling the checkbox to enable it on your &lt;code&gt;my-talk&lt;/code&gt; repository.&lt;/p&gt;
&lt;p&gt;Once that is done, go into the Travis CI settings for the repository. Navigate to the “Environment Variables” section, and declare the &lt;code&gt;GITHUB_TOKEN&lt;/code&gt; there. Be sure to keep it hidden from the output!&lt;/p&gt;
&lt;h3 id=&quot;turn-on-github-pages&quot;&gt;Turn on GitHub pages&lt;/h3&gt;&lt;p&gt;To turn on GitHub pages, we are going to stick to a pretty sane and widely-used set of practices when interacting with GitHub repositories and static sites.&lt;/p&gt;
&lt;p&gt;Firstly, on the GitHub repository for &lt;code&gt;my-talk&lt;/code&gt;, create a new branch (using the web interface or through the CLI) called &lt;code&gt;gh-pages&lt;/code&gt;. This time round, the name is definitely special, as GitHub recognizes this branch as a legitimate GitHub pages branch to serve content from.&lt;/p&gt;
&lt;p&gt;Secondly, go to the &lt;strong&gt;repository&lt;/strong&gt; settings (not your user settings), and ensure that GitHub pages is enabled for the repository. Usually, adding the &lt;code&gt;gh-pages&lt;/code&gt; branch will result in this option being  automagically turned on.&lt;/p&gt;
&lt;p&gt;Now, run:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;git add slides.md environment.yml .travis.yml&lt;/span&gt;
&lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;git commit -m &amp;quot;first commit&amp;quot;&lt;/span&gt;
&lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;git push&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Your slides, environment config, and Travis CI config files are now pushed to GitHub.&lt;/p&gt;
&lt;h3 id=&quot;check-on-travis&quot;&gt;Check on Travis&lt;/h3&gt;&lt;p&gt;Travis is now going to be building your slides and pushing them to the &lt;code&gt;gh-pages&lt;/code&gt; branch. If all goes well, you will see the slides show up at the URL: &lt;code&gt;https://your_username.github.io/my-talk&lt;/code&gt;. (Naturally, replace &lt;code&gt;your_username&lt;/code&gt; with your GitHub username, and &lt;code&gt;my-talk&lt;/code&gt; with your repository name.&lt;/p&gt;
&lt;h2 id=&quot;debugging-builds&quot;&gt;Debugging builds&lt;/h2&gt;&lt;p&gt;In case the build fails, you can inspect the output. Any errors are basically your standard bash &lt;code&gt;stderr&lt;/code&gt;, so if you know how to debug error messages, you should be able to debug issues with the build.&lt;/p&gt;
&lt;h2 id=&quot;beyond-this&quot;&gt;Beyond this&lt;/h2&gt;&lt;p&gt;Going beyond serving up &lt;code&gt;reveal.js&lt;/code&gt; slides, Travis hooked up to GitHub pages can help you build static sites very easily.&lt;/p&gt;
&lt;p&gt;If you use a static site generator (such as &lt;a href=&quot;https://www.getlektor.com/&quot;&gt;Lektor&lt;/a&gt;, &lt;a href=&quot;https://gohugo.io/&quot;&gt;Hugo&lt;/a&gt;, &lt;a href=&quot;https://blog.getpelican.com/&quot;&gt;Pelican&lt;/a&gt;, &lt;a href=&quot;https://www.gatsbyjs.org/&quot;&gt;Gatsby&lt;/a&gt; or &lt;a href=&quot;https://getnikola.com/&quot;&gt;Nikola&lt;/a&gt;), then you can create websites whose sources are fully under your control, 100% customizable, and fast to load. I do not blog on Medium because I desire full control over the display of my blog content, and I want to be able to take it anywhere I desire, without relying on a platform that might lock my content in. My personal website uses Lektor + Travis to push to GitHub pages; please feel free to look at &lt;a href=&quot;https://github.com/ericmjl/website&quot;&gt;the source&lt;/a&gt; and raid the repo for anything you’d like!&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Addendum: I learned today from a fellow friend Nathan Matias that Netflix’s blog posts on Medium have been paywalled. Another reason for us to take back hosting of slides and blog content into our own hands!&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;blockquote class=&quot;twitter-tweet&quot; data-lang=&quot;en&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;Today I learned the &lt;a href=&quot;https://twitter.com/netflix?ref_src=twsrc%5Etfw&quot;&gt;@netflix&lt;/a&gt; tech blog posts are paywalled by Medium and I may have to take their post off my syllabus.&lt;br&gt;&lt;br&gt;Now I wonder if they paywalled posts which I intend to be freely available. Have any of you found that Medium was unexpectedly charging people for your work?&lt;/p&gt;&amp;mdash; J. Nathan Matias (@natematias) &lt;a href=&quot;https://twitter.com/natematias/status/1218244174299963392?ref_src=twsrc%5Etfw&quot;&gt;January 17, 2020&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2020/1/16/pydata-ann-arbor-meetup-testing-for-data-science/">
    <title type="text">PyData Ann Arbor Meetup: Testing for Data Science</title>
    <id>urn:uuid:8adc6ded-60f5-3b49-b48d-fbb6a86ce30b</id>
    <updated>2020-01-16T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2020/1/16/pydata-ann-arbor-meetup-testing-for-data-science/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;I had the privilege of being invited to deliver a talk at the PyData Ann Arbor meetup this January, held at TD Ameritrade. My hosts, &lt;a href=&quot;https://seanlaw.github.io/&quot;&gt;Sean Law&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/roseputler/&quot;&gt;Rose Putler&lt;/a&gt; and &lt;a href=&quot;https://twitter.com/quasiben&quot;&gt;Ben Zaitlen&lt;/a&gt; were very welcoming and inviting, and I enjoyed my time in Ann Arbor (or A2, as the locals seem to call it).&lt;/p&gt;
&lt;h2 id=&quot;the-talk&quot;&gt;The Talk&lt;/h2&gt;&lt;p&gt;The talk I delivered was on testing for data scientists. (&lt;a href=&quot;https://ericmjl.github.io/testing-for-data-scientists/&quot;&gt;Slides are available here&lt;/a&gt;), and the &lt;a href=&quot;https://www.youtube.com/watch?time_continue=632&amp;amp;v=5RKuHvZERLY&amp;amp;feature=emb_logo&quot;&gt;YouTube video is up too&lt;/a&gt;. The topic stemmed from a long-standing problem that I had seen: untested code that I depended on slowing later analyses down because I did not have the confidence that it would behave correctly outside of the original situations I used it in.&lt;/p&gt;
&lt;p&gt;To communicate this point, I used two examples from work I had done before: one being the use of an automagic testing system, Hypothesis, to ferret out bugs in my code for me, and the other being the use of tests on our data schema to make the creation and caching of views robust and dependable.&lt;/p&gt;
&lt;h2 id=&quot;the-community&quot;&gt;The Community&lt;/h2&gt;&lt;p&gt;The PyData Ann Arbor community has some very dedicated members. There was someone who drove a whole 1.5 hours across the US-Canada border from Windsor, ON to listen in on the talk; another came by from a town 53 minutes away. Sean plays the role that &lt;a href=&quot;https://nedbatchelder.com/&quot;&gt;Ned Batchelder&lt;/a&gt; has done for the Boston Python User Group, and has really fostered a wonderful community here. I was really honored by the dedication that they possessed.&lt;/p&gt;
&lt;p&gt;There were also a bunch of people I had never met in person,  with whom I had interacted with online, with whom I finally got a chance to interact with in-person. It was great to meet some of them, including &lt;a href=&quot;https://bradleydice.com/&quot;&gt;Bradley Dice&lt;/a&gt; (U of M PhD student) and &lt;a href=&quot;https://www.linkedin.com/in/kyleweaton/&quot;&gt;Kyle Eaton&lt;/a&gt; (a UX engineer at Superconductive Health who is closely affiliated with the Great Expectations project).&lt;/p&gt;
&lt;h2 id=&quot;the-food&quot;&gt;The Food&lt;/h2&gt;&lt;p&gt;Ann Arbor, according to Sean, has quite the foodie scene. It is quite true, even as a land-locked midwestern university town.&lt;/p&gt;
&lt;p&gt;On the first afternoon here, I went to this restaurant called &lt;a href=&quot;https://holaseoul.com/&quot;&gt;Hola Seoul&lt;/a&gt;, which served Korean-Mexican fusion food. On my second day here, I had Panera for breakfast (not knowing anything better), skipped lunch, but went out with the PyData organizers to &lt;a href=&quot;http://slurpingturtle.com/annarbor/&quot;&gt;Slurping Turtle&lt;/a&gt; for some  very delicious sushi, fried chicken, and spicy miso ramen. And on my third day here, I decided to enjoy: smoked salmon and avocado omelette at &lt;a href=&quot;http://www.cafezola.com/content/&quot;&gt;Cafe Zola&lt;/a&gt; (it was expensive… but &lt;em&gt;worth  it&lt;/em&gt;), and &lt;a href=&quot;https://savasannarbor.com/&quot;&gt;Sava’s&lt;/a&gt; for a late lunch with Ben (NVIDIA) and Logan (TD Ameritrade).&lt;/p&gt;
&lt;h2 id=&quot;career-chat-with-sean&quot;&gt;Career Chat with Sean&lt;/h2&gt;&lt;p&gt;A few hours before the talk, I had a free-ranging chat with Sean about the problems we tackle in our respective roles. Here’s a smattering of thinking and talking points from our conversation.&lt;/p&gt;
&lt;p&gt;We share an “internal consulting” role at work, where both our teams’ missions are to spearhead new initiatives. His team is explicitly tasked with feeling out what’s upcoming in the next 5 years, POC-ing it, and seeing how it best fits in TD’s systems. It’s a very similar type of role that I’m in.&lt;/p&gt;
&lt;p&gt;I shared with him the frustrations I had battling what I saw was  unnecessary vendor tech. We both seemed to agree that front-liners and decision-makers not operating in the same circles was  an important reason for this phenomena.&lt;/p&gt;
&lt;p&gt;I also learned a few lessons from Sean that I think I need to emphasize more at work:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A relentless focus on generating wins for other teams.&lt;/li&gt;
&lt;li&gt;Strategically cycling between quick wins for credibility, and parlaying that into longer-term (but riskier) wins for the organization.&lt;/li&gt;
&lt;li&gt;Coffee talk tours throughout the company to spread good ideas.&lt;/li&gt;
&lt;li&gt;Minimizing surprises on colleagues (especially nasty ones; pleasant surprises are ok though).&lt;/li&gt;
&lt;li&gt;Strategically holding back on “just doing everything”, and instead letting colleagues co-create, to generate a sense of ownership over the final product.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;the-miscellaneous&quot;&gt;The Miscellaneous&lt;/h2&gt;&lt;p&gt;I was picked up by a limo to and from the hotel. While extremely comfortable, I was definitely pleasantly surprised, to the point of being a little bit not used to it.&lt;/p&gt;
&lt;p&gt;Delta Airlines was half-empty on the way over, but really full on the way back. Either way, my impressions are that without the WiFi access for free, they can’t beat JetBlue for me. That said, PyData Ann Arbor paid for the flight and lodging, so I won’t complain, it was still a comfortable flight nonetheless.&lt;/p&gt;
&lt;p&gt;This was my first time ever being in Michigan!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2020/1/12/honk-on-you-fools-insecure-in-your-miniscule-private-parts/">
    <title type="text">“Honk on, you fools insecure in your miniscule private parts.”</title>
    <id>urn:uuid:6d56bddc-44f4-30f7-a5e8-b167806684db</id>
    <updated>2020-01-12T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2020/1/12/honk-on-you-fools-insecure-in-your-miniscule-private-parts/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;A pattern I have noticed: the drivers who honk the most at cyclists are the ones who have the largest vehicles. Based on my anecdotal counting (highly inaccurate, probably exaggerated, but captures the effect), 1 in 15 pickup truck drivers will honk at me for cycling on the road in Quincy, and will shout out their window that I should ride on the sidewalk, followed by a gas pedal run right after. Supremely insecure ignoramus is the name I would give them.&lt;/p&gt;
&lt;p&gt;Usually, out of pure frustration, I just give them the metaphorical finger back by ignoring them. Based on my memory from my driving exam, I vaguely remembered a few pointers about the rules of the road pertaining to cyclist-motorist interactions. I decided to check them out.&lt;/p&gt;
&lt;p&gt;From the Massachusetts government website, &lt;a href=&quot;https://www.mass.gov/doc/laws-for-bicyclists-and-motorists-in-the-presence-of-bicyclists/download&quot;&gt;the rules of the road document Chapter 4&lt;/a&gt; (page 108), a selection of rules pertaining to the cyclist-driver interactions and cyclist road usage.&lt;/p&gt;
&lt;p&gt;As a cyclist:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You can use the &lt;strong&gt;full lane&lt;/strong&gt; (emphasis in original) anywhere, anytime, and on any street (except limited access or express state highways where signs specifically prohibiting bicycles have been posted), &lt;em&gt;even if there is a bike lane&lt;/em&gt; (emphasis mine).&lt;/li&gt;
&lt;li&gt;You can keep to the right when passing a motor vehicle moving in the travel lane and you can move to the front of an intersection at stop lights.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As a motorist:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Do Not Squeeze Bicycles in a Narrow Lane:&lt;/strong&gt; If a lane is too narrow to pass a bicycle at a safe distance, be &lt;strong&gt;PATIENT&lt;/strong&gt; (emphasis original) until you can safely use an adjacent lane or &lt;strong&gt;WAIT&lt;/strong&gt; (emphasis original) until it is safe to pass in the lane you share. (Chap. 89, Sec. 2) &lt;em&gt;You should stay at least three feet away when passing.&lt;/em&gt; (emphasis mine)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Be aware that bicyclists Do Not Always Have to Signal Turns!&lt;/strong&gt; Bicyclists must signal their intent by either hand to stop or turn. However, the signal does not have to be continuous or be made at all if both hands are needed for the bicycle’s safe operation. (Chap. 85, Sec. 11B).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Turns out, I was not wrong about riding on the road!&lt;/p&gt;
&lt;p&gt;So there, according to Massachusetts’ government-endorsed rules of the road, as a cyclist, I am entitled to use the entire lane whenever I see fit, &lt;em&gt;even in the presence of a bike lane&lt;/em&gt;, and am under zero obligation to use the sidewalk simply for a motorist’s convenience. I choose to ride on the right side just to make things easier for drivers (and as an occasional driver myself, I much appreciate it when cyclists do so; the Golden Rule is pretty good here). But where the rules are quite clear, I definitely do not appreciate being honked and shouted at for riding on a road that I am rightfully allowed to use.&lt;/p&gt;
&lt;h2 id=&quot;addendum&quot;&gt;Addendum&lt;/h2&gt;&lt;p&gt;To be clear, I’m already thankful it’s only about 1 in 15 pickup-truck drivers and about 1 in 50 regular drivers who honk at me.&lt;/p&gt;
&lt;p&gt;At the same time, I’m looking for ideas to make the ignoramuses aware of the rules of the road. Would be happy to chat on &lt;a href=&quot;https://octodon.social/web/accounts/7711&quot;&gt;Mastodon&lt;/a&gt; or &lt;a href=&quot;../../../../../blog/2020/1/12/honk-on-you-fools-insecure-in-your-miniscule-private-parts/twitter.com/ericmjl&quot;&gt;Twitter&lt;/a&gt; if you have ideas.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2020/1/7/fastapi-flask-like-generator-of-web-apis/">
    <title type="text">FastAPI: Flask-like generator of web APIs</title>
    <id>urn:uuid:b3367081-dc60-3b82-8a63-eb54cfe84e6d</id>
    <updated>2020-01-07T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2020/1/7/fastapi-flask-like-generator-of-web-apis/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;I test-drove &lt;a href=&quot;https://fastapi.tiangolo.com/&quot;&gt;FastAPI&lt;/a&gt; yesterday, and true to it's name, it's fast! And when the developers say fast, they mean it in at least two ways:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;It's fast to execute.&lt;/li&gt;
&lt;li&gt;It's fast to develop an API.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I'd like to write about the second point.&lt;/p&gt;
&lt;p&gt;Prior to FastAPI, my development framework of choice to build a web API would have been Flask (though I will admit, I have only developed web apps, not web APIs before). My familiarity with Flask turned out to be extremely helpful for learning FastAPI: its own API tracks very closely with Flask! Many of the same idioms, including instantiating a &lt;code&gt;FastAPI&lt;/code&gt; object and using &lt;code&gt;@app.route&lt;/code&gt;  decorators with string interpolation in the URIs, carry over. This is the first way that FastAPI makes API development fast: &lt;strong&gt;by using existing idioms&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Prior to FastAPI, I did have reservations about developing a web API using Flask because I wasn't sure how to provide sufficient documentation to API users/testers on how to use it. (Have you seen Swagger APIs &lt;em&gt;without&lt;/em&gt; example usage documentation? Frustrating!) Turns out, FastAPI &lt;strong&gt;parses your docstrings to provide API docs&lt;/strong&gt;. This means you can provide an example usage inside the routing function, and on the &lt;code&gt;/docs&lt;/code&gt; endpoint, all they will render on the API docs!&lt;/p&gt;
&lt;p&gt;So in summary, FastAPI is a very productive tool to use for developing APIs. I highly recommend you check it out once you hit the appropriate opportunity!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2020/1/4/build-your-digital-profile-as-a-data-scientist/">
    <title type="text">Build your digital profile as a data scientist</title>
    <id>urn:uuid:f294b030-879e-3e9c-a070-eeaa5a3ec421</id>
    <updated>2020-01-04T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2020/1/4/build-your-digital-profile-as-a-data-scientist/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;I have received questions from others on how to build a digital profile for career development. Everybody’s going to have a unique path, and what I think I can offer are observations on what I think have been helpful for myself and others who have enjoyed similar successes in getting started. My hope is that these pointers, which come from self-reflection over the past two years of work, help you.&lt;/p&gt;
&lt;h2 id=&quot;the-maybe-timeless-and-cross-industry-principles&quot;&gt;The maybe-timeless and cross-industry principles&lt;/h2&gt;&lt;p&gt;First off, here are some things that I think are timeless principles in the job search, and are probably true for a large swathe of professional roles.&lt;/p&gt;
&lt;p&gt;Firstly, nobody owes me their time or a job; &lt;strong&gt;I have a lot of leeway to make it as easy as possible for others to say, “Yes, I would like to have you on board”&lt;/strong&gt;. Naturally, this statement ignores structural privileges and inequalities that exist in society, so I would be realistic in what I can accomplish. That said, it's usually a good prior to assume that everybody is busy, and that you need to properly advertise what you can offer.&lt;/p&gt;
&lt;p&gt;In line with the same idea, I start with the presumption that nobody has the spare time to think deeply about a candidate when there are many other things in one's mind. The corollary is that &lt;strong&gt;it is on me to frame how I want others to view me&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&quot;data-science-specific-things&quot;&gt;Data science-specific things&lt;/h2&gt;&lt;p&gt;Now, for a data science role, being able to demonstrate the following can only help, not hurt, your application and candidacy:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Ability to use your skills to solve real problems of value.&lt;/li&gt;
&lt;li&gt;Good coding practices.&lt;/li&gt;
&lt;li&gt;Good storytelling and communication ability.&lt;/li&gt;
&lt;li&gt;Proficiency with building &quot;products&quot; that aid in decision-making.&lt;/li&gt;
&lt;li&gt;Ability to work collaboratively with others.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In data science, “value” usually means saving money. Saving time = saving money, in case that was not clear. Hence, automation &lt;em&gt;is&lt;/em&gt; valuable.&lt;/p&gt;
&lt;p&gt;Good coding practices are important: I have an &lt;a href=&quot;https://ericmjl.github.io/essays-on-data-science/software-skills/&quot;&gt;essay collection&lt;/a&gt; on them, if you need a resource to get started.&lt;/p&gt;
&lt;p&gt;Communication skills are universally important in professional roles, and data science is no different.&lt;/p&gt;
&lt;p&gt;What does enhance communication is the ability to build interactive tools that guide a busy decision-maker towards ethical and profitable choices (hopefully in that order). Good value judgment is needed here!&lt;/p&gt;
&lt;h2 id=&quot;where-are-prospective-hiring-teams-going-to-look?&quot;&gt;Where are prospective hiring teams going to look?&lt;/h2&gt;&lt;p&gt;This comes from &lt;code&gt;n=7&lt;/code&gt; candidates for (I think) 3 candidate searches at work that I was involved in. The order in which I was looking was:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Resume&lt;/li&gt;
&lt;li&gt;LinkedIn&lt;/li&gt;
&lt;li&gt;GitHub&lt;/li&gt;
&lt;li&gt;Personal website&lt;/li&gt;
&lt;li&gt;Google Scholar&lt;/li&gt;
&lt;li&gt;Old/current research group website&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The resume is what you turn in. Make sure it's clean, readable, and that it concisely captures exactly what you're looking to communicate to the hiring team: how you're going to be a good fit for the role.&lt;/p&gt;
&lt;p&gt;When I looked at LinkedIn, I was, quite interestingly, looking at their social network. Who might they plausibly know? At least for the team I'm on, I know credentials and certifications matter less than evidence of projects done, which brings me to the next place...&lt;/p&gt;
&lt;p&gt;GitHub. I was looking for evidence of candidates' ability to code. A well fleshed-out GitHub profile with publicly browsable repositories and a contribution record that is mostly your own makes it so much easier to see your coding style. I also looked for evidence of familiarity with packages, continuous integration tooling, good version control, and collaborations with other package developers.&lt;/p&gt;
&lt;p&gt;Your projects that demonstrate the data science skills above should be prominently featured on your profile page. Project types that, in contemporary times, communicate these skills well include:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Data products that you've built&lt;/li&gt;
&lt;li&gt;Teaching material that you've made&lt;/li&gt;
&lt;li&gt;Contributions you've made to other repositories, in particular pull requests and issues politely raised.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I looked at Google Scholar as well to get a flavour for a candidate's prior research work. It's an indication of one's domain expertise, and &lt;em&gt;possibly&lt;/em&gt; an indicator of the kinds of problems one will gravitate towards. (This last point has been at least true for myself; however, for one jumping from, say, biological data science to flight data science, this will be much less relevant.)&lt;/p&gt;
&lt;p&gt;The diversity of one's collaborators also helps paint a picture: did you specialize in work with one other person all of your academic career, or did you work in large teams, or did you work mostly solo? (Don't put a value judgment so quickly: each has their own strengths.)&lt;/p&gt;
&lt;p&gt;A candidate's old research group is something I would check only out of curiosity, just to know more.&lt;/p&gt;
&lt;h2 id=&quot;the-kitchen-sink-of-tips&quot;&gt;The kitchen sink of tips&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Tip #1:&lt;/strong&gt; if you put your source code on GitHub, always include in the README why the repository exists, and a guide to how to use the repository. It is a marker of &quot;sociable working style&quot;: in other words, you're able to think of how others are going to interact with things that you've created. (Using others' tools happens all the time at work!)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tip #2:&lt;/strong&gt; If you put a notebook up in your repository, be sure to make the repo &lt;a href=&quot;https://mybinder.org/&quot;&gt;Binder-friendly&lt;/a&gt;. It doesn't take much: environment spec file is all thats needed!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2020/1/2/on-automating-principled-statistical-analyses/">
    <title type="text">On automating principled statistical analyses</title>
    <id>urn:uuid:ac86f8f4-829c-3409-8608-a393d4c21d76</id>
    <updated>2020-01-02T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2020/1/2/on-automating-principled-statistical-analyses/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;I’ve been known to rant against the t-test, because I see it as a canned statistical test that most scientists “just” reach for. From a statistical viewpoint, reaching for the t-test by default is unprincipled because our data may not necessarily fulfill the Gaussian-distributed assumptions of the t-test.&lt;/p&gt;
&lt;p&gt;That isn’t to say, though, that I’m against automated statistical analyses.&lt;/p&gt;
&lt;p&gt;If there’s a data generating process that will need continual analysis, and we are aware that these processes can be broadly standardized enough that we can use a single statistical model across multiple groups and/or samples, then we might be able to automate the analysis method used.&lt;/p&gt;
&lt;p&gt;An example from my line of work is standardized high throughput (and/or large-scale) measurements with the same randomized experimental structure. If the high throughput measurement assay stays the same from project to project, and is a standardized assay measurement, then we should be able to use a single statistical model across all samples in the assay.&lt;/p&gt;
&lt;p&gt;I have done this with large-scale electrophysiology measurements, where we quantified electrophysiological curve decay constants as a function of molecule concentrations, and wrote a custom hierarchical Bayesian model for the data. In another project, my colleagues and I built a hierarchical Bayesian model for enzyme catalysis efficiency. In both cases, because we had confidence that the data generating process was constant over time, we could write a program through which we fed in standardized data and from which we obtained robust, regularized estimates of our quantities of interest.&lt;/p&gt;
&lt;p&gt;Counterfactually, if we had &lt;em&gt;just&lt;/em&gt; picked some quantity and gone with the t-test (or worse, used t-test assumptions with multiple hypothesis correction), we would have likely made a number of errors in our automated analyses that would compound in our later decision-making steps. More pedestrian would have been the fact that I would not have been able to properly defend what we were doing in front of a properly-trained statistician who knows how to use likelihoods in appropriate situations. (Our data didn’t necessarily have t-distributed likelihoods!)&lt;/p&gt;
&lt;p&gt;There’s always this “smell test” that we can do. &lt;strong&gt;The “likelihood smell test” is a good one.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In conclusion, automating a principled statistical analysis is fine, as long as the data generating process is more or less constant. Reaching for a canned test by default is not.&lt;/p&gt;
&lt;p&gt;And friends, if you write an automated pipeline, don’t forget to &lt;a href=&quot;https://ericmjl.github.io/essays-on-data-science/software-skills/testing/&quot;&gt;write tests&lt;/a&gt;!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/12/26/serving-multiple-panel-apps-together/">
    <title type="text">Serving multiple Panel apps together</title>
    <id>urn:uuid:a4a07dc4-3a93-330c-8cb1-79e1cbec98d2</id>
    <updated>2019-12-26T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/12/26/serving-multiple-panel-apps-together/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;I learned a new thing today! If I have a bunch of small dashboard-like utilities, &lt;code&gt;panel&lt;/code&gt;, which uses the &lt;code&gt;bokeh&lt;/code&gt; server behind the scenes, can serve up multiple files together from the same server.&lt;/p&gt;
&lt;p&gt;Here's an example. Assume I have the following directory structure:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;|- /
  |- app1.py
  |- app2.py
  |- app3.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If I start a Panel server here using:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;panel serve *.py
&lt;span class=&quot;c1&quot;&gt;# or, to be selective&lt;/span&gt;
panel serve src1.py src2.py src3.py
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If you have a bunch of Jupyter notebooks, the analogous command is:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;panel serve *.ipynb
&lt;span class=&quot;c1&quot;&gt;# or, to be selective:&lt;/span&gt;
panel serve nb1.ipynb nb2.ipynb nb3.ipynb
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then all of the apps will be served up, with a default Bokeh landing page provided to link to each of the apps.&lt;/p&gt;
&lt;p&gt;Doing so lets us build multiple little utilities that can help ourselves and our colleagues be more productive!&lt;/p&gt;
&lt;p&gt;For an example of this, check out the &lt;a href=&quot;https://minimal-panel-app.herokuapp.com/home&quot;&gt;minimal panel app&lt;/a&gt; I built to record these ideas. (Source code is available &lt;a href=&quot;https://github.com/ericmjl/minimal-panel-app&quot;&gt;here&lt;/a&gt;.)&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/12/19/simplifying-uncertainty-responsibly/">
    <title type="text">Simplifying Uncertainty Responsibly</title>
    <id>urn:uuid:e86b3753-7b6d-3738-8eda-d66aeea78b40</id>
    <updated>2019-12-19T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/12/19/simplifying-uncertainty-responsibly/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;I read an article from Brandon Rohrer titled “&lt;a href=&quot;https://brohrer.github.io/oversimplify.html&quot;&gt;Oversimplify&lt;/a&gt;”. The article provoked some thoughts.&lt;/p&gt;
&lt;p&gt;As someone who strives to model uncertainty in a day-to-day setting, it’s easy to misread Brandon’s article as saying, “discard your modelling of uncertainty”. It took me a few reads and a bit of thinking to realize that my misunderstanding would have been wrong. The gist of Brandon’s article is to communicate the fact that &lt;em&gt;most people don’t like uncertainty, so simplify &lt;strong&gt;the communication&lt;/strong&gt; of uncertainty&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;In a Bayesian setting, I think this corresponds mostly to “minimize regret” when making decisions.&lt;/p&gt;
&lt;p&gt;Here’s a classic example: weather forecast says “30% probability of precipitation” (assume this is rain). How does this fit into Brandon’s conception of simplifying the communication of uncertainty?&lt;/p&gt;
&lt;p&gt;Simply telling someone the probability of precipitation doesn’t help. It’s like stating an unhelpful fact - unhelpful if the audience doesn’t have a thought framework for acting on that uncertainty.&lt;/p&gt;
&lt;p&gt;Any good card-carrying Bayesian who also knows how to minimize regret in-line with pretty universal human values would say, “there’s a 30% probability of precipitation. Since getting wet is more miserable than carrying an umbrella on a cloudy day, take that umbrella, and throw in your waterproof jacket and boots while you’re at it.”&lt;/p&gt;
&lt;p&gt;By contrast, someone other consultant might take the same probabilities, and instead recommend, “Oh, there’s basically greater odds of not raining than raining, so wear your cotton jacket since it’s cold, but don’t bother about an umbrella.” This consultant, we would say, is basically deaf to human psychology and ethics. (I will not go deeper on the question of “what makes ethics” - too much for this blog post.)&lt;/p&gt;
&lt;p&gt;The key question most people are seeking an answer to is not “what do the data say”, but rather “how &lt;em&gt;ought&lt;/em&gt; I act?” Though the latter question is normative, it &lt;em&gt;can&lt;/em&gt; be informed by quantitative reasoning; we just have to make sure it is not &lt;em&gt;only&lt;/em&gt; informed by quantitative reasoning.&lt;/p&gt;
&lt;p&gt;So to summarize: &lt;em&gt;most people don’t like uncertainty, so simplify &lt;strong&gt;the communication&lt;/strong&gt; of uncertainty by providing an actionable recommendation in-line with universal of values (as much as possible) and culturally-specific behaviours of your audience&lt;/em&gt;. In that way, our handling of uncertainty can be responsible and ethical.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/12/15/a-review-of-the-python-data-science-dashboarding-landscape-in-2019/">
    <title type="text">A Review of the Python Data Science Dashboarding Landscape in 2019</title>
    <id>urn:uuid:de09337d-5023-3bca-960c-70e369a20733</id>
    <updated>2019-12-15T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/12/15/a-review-of-the-python-data-science-dashboarding-landscape-in-2019/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;&lt;em&gt;This blog post is also available on &lt;a href=&quot;https://ericmjl.github.io/essays-on-data-science/&quot;&gt;my collection of essays&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;&lt;p&gt;As Pythonista data scientists,
we are spoiled for choice when it comes to developing front-ends
for our data apps.
We used to have to fiddle with HTML in Flask (or Plotly's Dash),
but now, there are tools in which
&quot;someone wrote the HTML/JS so I didn't have to&quot;.&lt;/p&gt;
&lt;p&gt;Let me give a quick tour of the landscape of tools
as I've experienced it in 2019.&lt;/p&gt;
&lt;h3 id=&quot;beginnings:-voila&quot;&gt;Beginnings: Voila&lt;/h3&gt;&lt;p&gt;Previously, I had test-driven
&lt;a href=&quot;https://voila.readthedocs.io/en/latest/&quot;&gt;Voila&lt;/a&gt;.
The key advantage I saw back then was that in my workflow,
once I had the makings of a UI present in the Jupyter notebook,
and just needed a way to serve it up
independent of having my end-users run a Jupyter server,
then Voila helped solve that use case.
By taking advantage of existing the &lt;code&gt;ipywidgets&lt;/code&gt; ecosystem
and adding on a way to run and serve the HTML output of a notebook,
Voila solved that part of the dashboarding story quite nicely.
In many respects,
I regard Voila as the first proper dashboarding tool for Pythonistas.&lt;/p&gt;
&lt;p&gt;That said, development in a Jupyter notebook
didn't necessarily foster best practices
(such as refactoring and testing code).
When my first project at work ended,
and I didn't have a need for further dashboarding,
I didn't touch Voila for a long time.&lt;/p&gt;
&lt;h3 id=&quot;another-player:-panel&quot;&gt;Another player: Panel&lt;/h3&gt;&lt;p&gt;Later, &lt;a href=&quot;http://panel.pyviz.org/&quot;&gt;Panel&lt;/a&gt; showed up.
Panel's development model allowed a more modular app setup,
including importing of plotting functions defined inside &lt;code&gt;.py&lt;/code&gt; files
that returned individual plots.
Panel also allowed me to prototype in a notebook and see the output live
before moving the dashboard code into a source &lt;code&gt;.py&lt;/code&gt; file.&lt;/p&gt;
&lt;p&gt;At work, we based a one-stop shop dashboard for a project on Panel,
and in my personal life,
I also built a
&lt;a href=&quot;https://github.com/ericmjl/minimal-panel-app&quot;&gt;minimal panel app&lt;/a&gt;
that I also
&lt;a href=&quot;https://minimal-panel-app.herokuapp.com/&quot;&gt;deployed to Heroku&lt;/a&gt;.
Panel was definitely developed
targeting notebook and source file use cases in mind,
and this shows through in its source development model.&lt;/p&gt;
&lt;p&gt;That said, panel apps could be slow to load,
and without having a &quot;spinner&quot; solution in place
(i.e. something to show the user
that the app is &quot;doing something&quot; in the background),
it sometimes made apps &lt;em&gt;feel&lt;/em&gt; slow
even though the slowness was not Panel's fault really.
(My colleagues and I pulled out all the tricks in our bag to speed things up.)&lt;/p&gt;
&lt;p&gt;Additionally, any errors that show up don't get surfaced to the app's UI,
where developer eyeballs are on -
instead, they get buried in the browser's JavaScript console
or in the Python terminal where the app is being served.
When deployed, this makes it difficult to see where errors show up
and debug errors.&lt;/p&gt;
&lt;h3 id=&quot;enter-streamlit&quot;&gt;Enter Streamlit&lt;/h3&gt;&lt;p&gt;Now, Streamlit comes along, and some of its initial demos are pretty rad.
In order to test-drive it,
I put together this &lt;a href=&quot;https://minimal-streamlit.herokuapp.com/&quot;&gt;little tutorial&lt;/a&gt;
on the Beta probability distribution for my colleagues.&lt;/p&gt;
&lt;p&gt;Streamlit definitely solves some of the pain points
that I've observed with Panel and Voila.&lt;/p&gt;
&lt;p&gt;The most important one that I see is that errors are captured by Streamlit
and bubbled up to the UI,
where our eyeballs are going to be when developing the app.
For me, this is a very sensible decision to make, for two reasons:&lt;/p&gt;
&lt;p&gt;Firstly, it makes debugging interactions that much easier.
Instead of needing to have two interfaces open,
the error message shows up right where the interaction fails,
in the same browser window as the UI elements.&lt;/p&gt;
&lt;p&gt;Secondly, it makes it possible for us
to use the error messages as a UI &quot;hack&quot; to inform users
where their inputs (e.g. free text) might be invalid,
thereby giving them &lt;em&gt;informative error messages&lt;/em&gt;.
(Try it out in the Beta distribution app:
it'll give you an error message right below
if you try to type something that cant be converted into a float!)&lt;/p&gt;
&lt;p&gt;The other key thing that Streamlit provides as a UI nice-ity
is the ability to signal to end-users that a computation is happening.
Streamlit does this in three ways, two of which always come for free.
&lt;strong&gt;Firstly&lt;/strong&gt;, if something is &quot;running&quot;,
then in the top-right hand corner of the page,
the &quot;Running&quot; spinner will animate.
&lt;strong&gt;Secondly&lt;/strong&gt;, anything that is re-rendering will automatically be greyed out.
&lt;strong&gt;Finally&lt;/strong&gt;, we can use a special context manager
to provide a custom message on the front-end:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;streamlit&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;st&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;st&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;spinner&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Message goes here...&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# stuff happens&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;So all-in-all, Streamlit seems to have a solution of some kind
for the friction points that I have observed with Panel and Voila.&lt;/p&gt;
&lt;p&gt;Besides that, Streamlit, I think, uses a procedural paradigm,
rather than a callback paradigm, for app construction.
We just have to think of the app as a linear sequence of actions
that happen from top to bottom.
State is never really an issue, because every code change
and interaction re-runs the source file from top to bottom, from scratch.
When building quick apps,
this paradigm really simplifies things compared to a callback-based paradigm.&lt;/p&gt;
&lt;p&gt;Finally, Streamlit also provides a convenient way to add text to the UI
by automatically parsing as Markdown any raw strings unassigned to a variable
in a &lt;code&gt;.py&lt;/code&gt; file and rendering them as HTML.
This opens the door to treating a &lt;code&gt;.py&lt;/code&gt; file as a
&lt;a href=&quot;https://en.wikipedia.org/wiki/Literate_programming&quot;&gt;literate programming document&lt;/a&gt;,
hosted by a Python-based server in the backend.
It'd be useful especially in teaching scenarios.
(With &lt;code&gt;pyiodide&lt;/code&gt; bringing the PyData stack to the browser,
I can't wait to see standalone &lt;code&gt;.py&lt;/code&gt; files rendered to the DOM!)&lt;/p&gt;
&lt;p&gt;Now, this isn't to say that Streamlit is problem-free.
There are still rough edges,
the most glaring (as of today) in the current release
is the inability to upload a file and operate on it.
This has been fixed in &lt;a href=&quot;https://github.com/streamlit/streamlit/pull/488&quot;&gt;a recent pull request&lt;/a&gt;,
so I'm expecting this should show up in a new release any time soon.&lt;/p&gt;
&lt;p&gt;The other not-so-big-problem that I see with Streamlit at the moment
is the procedural paradigm -
by always re-running code from top-to-bottom afresh on every single change,
apps that rely on long compute may need a bit more thought to construct,
including the use of Streamlit's caching mechanism.
Being procedural does make things easier for development though,
and on balance, I would not discount Streamlit's simplicity here.&lt;/p&gt;
&lt;h2 id=&quot;where-does-streamlit-fit?&quot;&gt;Where does Streamlit fit?&lt;/h2&gt;&lt;p&gt;As I see it, Streamlit's devs are laser-focused on enabling devs
to &lt;em&gt;very quickly&lt;/em&gt; get to a somewhat good-looking app prototype.
In my experience, the development time for the Beta distribution app
took about 3 hours, 2.5 of which were spent on composing prose.
So effectively, I only used half an hour doing code writing,
with a live and auto-reloading preview
greatly simplifying the development process.
(I conservatively estimate that this is about 1.5 times
as fast as I would be using Panel.)&lt;/p&gt;
&lt;p&gt;Given Streamlit, I would use it to develop two classes of apps:
(1) very tightly-focused utility apps that do one lightweight thing well, and
(2) bespoke, single-document literate programming education material.&lt;/p&gt;
&lt;p&gt;I would be quite hesitant to build more complex things;
then again, for me, that statement would be true more generally anyways
with whatever tool.
In any case, I think bringing UNIX-like thinking to the web
is probably a good idea:
we make little utilities/functional tools
that can pipe standard data formats from to another.&lt;/p&gt;
&lt;h2 id=&quot;common-pain-points-across-all-three-dashboarding-tools&quot;&gt;Common pain points across all three dashboarding tools&lt;/h2&gt;&lt;p&gt;A design pattern I have desired is to be able to serve up a fleet of small,
individual utilities served up from the same codebase,
served up by individual server processes,
but all packaged within the same container.
The only way I can think of at the moment
is to build a custom Flask-based gateway
to redirect properly to each utility's process.
That said, I think this is probably out of scope
for the individual dashboarding projects.&lt;/p&gt;
&lt;h2 id=&quot;how-do-we-go-forward?&quot;&gt;How do we go forward?&lt;/h2&gt;&lt;p&gt;The ecosystem is ever-evolving, and,
rather than being left confused by the multitude of options available to us,
I find myself actually being very encouraged
at the development that has been happening.
There's competing ideas with friendly competition between the developers,
but they are also simultaneously listening to each other and their users
and converging on similar things in the end.&lt;/p&gt;
&lt;p&gt;That said, I think it would be premature to go &quot;all-in&quot; on a single solution
at this moment.
For the individual data scientist,
I would advise to be able to build something
using each of the dashboarding frameworks.
My personal recommendations are to know how to use:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Voila + &lt;code&gt;ipywidgets&lt;/code&gt; in a Jupyter notebook&lt;/li&gt;
&lt;li&gt;Panel in Jupyter notebooks and standalone &lt;code&gt;.py&lt;/code&gt; files&lt;/li&gt;
&lt;li&gt;Streamlit in &lt;code&gt;.py&lt;/code&gt; files.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These recommendations stem mainly from
the ability to style and layout content without needing much knowledge of HTML.
In terms of roughly when to use what,
my prior experience has been that
Voila and Streamlit are pretty good for quicker prototypes,
while Panel has been good for more complex ones,
though in all cases, we have to worry about speed impacting user experience.&lt;/p&gt;
&lt;p&gt;From my experience at work,
being able to quickly hash out key visual elements in a front-end prototype
gives us the ability to better communicate with UI/UX designers and developers
on what we're trying to accomplish.
Knowing how to build front-ends ourselves
lowers the communication and engineering barrier
when taking a project to production.
It's a worthwhile skill to have;
be sure to have it in your toolbox!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/11/9/principled-git-based-workflow-in-collaborative-data-science-projects/">
    <title type="text">Principled Git-based Workflow in Collaborative Data Science Projects</title>
    <id>urn:uuid:54f99df0-bb6a-32f8-ab64-13d6a77605ee</id>
    <updated>2019-11-09T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/11/9/principled-git-based-workflow-in-collaborative-data-science-projects/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;Having worked with GitFlow on a data science project and coming to a few epiphanies with it, I decided to share some of my thoughts &lt;a href=&quot;https://ericmjl.github.io/essays-on-data-science/workflow/gitflow/&quot;&gt;in an essay&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;One of my thoughts here is that most data scientists aren't resistant to using GitFlow (and more generally, just being more intentional about what gets worked on) because it's a bad idea, but because there's a lack of incentives to do so. In there, I try to address this concern.&lt;/p&gt;
&lt;p&gt;And because GitFlow does require knowledge of Git, it can trigger an, &quot;Oh no, one more thing to learn!&quot; response. These things do take time to learn, yes, but I see it also as an investment of time with a future payoff.&lt;/p&gt;
&lt;p&gt;Apart from that, I hope you enjoy the essay; writing it was also a great opportunity for me to pick up more advanced features of &lt;code&gt;pymdownx&lt;/code&gt;, a package that extends Markdown syntax with other really cool features.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/10/31/reimplementing-and-testing-deep-learning-models/">
    <title type="text">Reimplementing and Testing Deep Learning Models</title>
    <id>urn:uuid:eda83e45-1c19-3613-9a34-21831ed14a99</id>
    <updated>2019-10-31T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/10/31/reimplementing-and-testing-deep-learning-models/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;&lt;em&gt;Note: this blog post is cross-posted on my personal &lt;a href=&quot;https://ericmjl.github.io/essays-on-data-science/machine-learning/reimplementing-models/&quot;&gt;essay collection on the practice of data science&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;At work, most deep learners I have encountered
have a tendency to take deep learning models
and treat them as black boxes that we should be able to wrangle.
While I see this as a pragmatic first step
to testing and proving out the value of a newly-developed deep learning model,
I think that stopping there
and not investing the time into understanding the nitty-gritty of the model
leaves us in a poor position
to know that model's
(1) applicability domain (i.e. where the model should be used),
(2) computational and statistical performance limitations, and
(3) possible engineering barriers to getting the model performant
in a &quot;production&quot; setting.&lt;/p&gt;
&lt;p&gt;As such, with deep learning models,
I'm actually a fan of investing the time to re-implement the model
in a tensor framework that we all know and love,
NumPy (and by extension, JAX).&lt;/p&gt;
&lt;h2 id=&quot;benefits-of-re-implementing-deep-learning-models&quot;&gt;Benefits of re-implementing deep learning models&lt;/h2&gt;&lt;p&gt;Doing a model re-implementation from a deep learning framework
into NumPy code actually has some benefits for the time being invested.&lt;/p&gt;
&lt;h3 id=&quot;developing-familiarity-with-deep-learning-frameworks&quot;&gt;Developing familiarity with deep learning frameworks&lt;/h3&gt;&lt;p&gt;Firstly, doing so forces us to know the translation/mapping
from deep learning tensor libraries into NumPy.
One of the issues I have had with deep learning libraries
(PyTorch and Tensorflow being the main culprits here)
is that their API copies something like 90% of NumPy API
without making easily accessible
the design considerations discussed when deciding to deviate.
(By contrast, CuPy has an explicit API policy
that is well-documented and front-and-center on the docs,
while JAX strives to replicate the NumPy API.)&lt;/p&gt;
&lt;p&gt;My gripes with tensor library APIs aside, though,
translating a model by hand from one API to another
forces growth in familiarity with both APIs,
much as translating between two languages
forces growth in familiarity with both languages.&lt;/p&gt;
&lt;h3 id=&quot;developing-a-mechanistic-understanding-of-the-model&quot;&gt;Developing a mechanistic understanding of the model&lt;/h3&gt;&lt;p&gt;It is one thing to describe a deep neural network
as being &quot;like the brain cell connections&quot;.
It is another thing to know that the math operations underneath the hood
are nothing more than dot products (or tensor operations, more generally).
Re-implementing a deep learning model
requires combing over every line of code,
which forces us to identify each math operation used.
No longer can we hide behind an unhelpfully vague abstraction.&lt;/p&gt;
&lt;h3 id=&quot;developing-an-ability-to-test-and-sanity-check-the-model&quot;&gt;Developing an ability to test and sanity-check the model&lt;/h3&gt;&lt;p&gt;If we follow the workflow (that I will describe below)
for reimplementing the model,
(or as the reader should now see, translating the model between APIs)
we will develop confidence in the correctness of the model.
This is because the workflow I am going to propose
involves proper basic software engineering workflow:
writing documentation for the model,
testing it,
and modularizing it into its logical components.
Doing each of these requires a mechanistic understanding
of how the model works,
and hence forms a useful way of building intuition behind the model
as well as correctness of the model.&lt;/p&gt;
&lt;h3 id=&quot;reimplementing-models-is-_not_-a-waste-of-time&quot;&gt;Reimplementing models is &lt;em&gt;not&lt;/em&gt; a waste of time&lt;/h3&gt;&lt;p&gt;By contrast, it is a highly beneficial practice
for gaining a deeper understanding into the inner workings
of a deep neural network.
The only price we pay is in person-hours,
yet under the assumption that the model is of strong commercial interest,
that price can only be considered an investment, and not a waste.&lt;/p&gt;
&lt;h2 id=&quot;a-proposed-workflow-for-reimplementing-deep-learning-models&quot;&gt;A proposed workflow for reimplementing deep learning models&lt;/h2&gt;&lt;p&gt;I will now propose a workflow for re-implementing deep learning models.&lt;/p&gt;
&lt;h3 id=&quot;identify-a-coding-partner&quot;&gt;Identify a coding partner&lt;/h3&gt;&lt;p&gt;Pair programming is a productive way of teaching and learning.
Hence, I would start by identifying a coding partner
who has the requisite skillset and shared incentive
to go deep on the model.&lt;/p&gt;
&lt;p&gt;Doing so helps a few ways.&lt;/p&gt;
&lt;p&gt;Firstly, we have real-time peer review on our code,
making it easier for us to catch mistakes that show up.&lt;/p&gt;
&lt;p&gt;Secondly, working together at the same time means that
both myself and my colleague will learn something about the neural network
that we are re-implementing.&lt;/p&gt;
&lt;h3 id=&quot;pick-out-the-forward-step-of-the-model&quot;&gt;Pick out the &quot;forward&quot; step of the model&lt;/h3&gt;&lt;p&gt;The &quot;forward&quot; pass of the model is where the structure of the model is defined:
basically the mathematical operations
that transform the input data into the output observations.&lt;/p&gt;
&lt;p&gt;A few keywords to look out for
are the &lt;code&gt;forward()&lt;/code&gt; and  &lt;code&gt;__call__()&lt;/code&gt; class methods.&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;MyModel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# ...&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# Implementation of model happens here.&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;something&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;something&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For models that involve an autoencoder,
somewhat more seasoned programmers
might create a class method called &lt;code&gt;encoder()&lt;/code&gt; and &lt;code&gt;decoder()&lt;/code&gt;,
which themselves reference another model
that would have a &lt;code&gt;forward()&lt;/code&gt; or &lt;code&gt;__call__()&lt;/code&gt; defined.&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;AutoEncoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# ...&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;something&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;something&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Re-implementing the &lt;code&gt;forward()&lt;/code&gt; part of the model
is usually a good way of building a map
of the equations that are being used
to transform the input data into the output data.&lt;/p&gt;
&lt;h3 id=&quot;inspect-the-shapes-of-the-weights&quot;&gt;Inspect the shapes of the weights&lt;/h3&gt;&lt;p&gt;While the equations give the model &lt;em&gt;structure&lt;/em&gt;,
the weights and biases, or the &lt;em&gt;parameters&lt;/em&gt;,
are the part of the model that are optimized.
(In Bayesian statistics, we would usually presume a model structure,
i.e. the set of equations used alongside the priors,
and fit the model parameters.)&lt;/p&gt;
&lt;p&gt;Because much of deep learning hinges on linear algebra,
and because most of the transformations that happen
involve transforming the &lt;em&gt;input space&lt;/em&gt; into the &lt;em&gt;output space&lt;/em&gt;,
getting the shapes of the parameters is very important.&lt;/p&gt;
&lt;p&gt;In a re-implementation exercise with my intern,
where we re-implemented
a specially designed recurrent neural network layer in JAX,
we did a manual sanity check through our implementation
to identify what the shapes would need to be
for the inputs and outputs.&lt;/p&gt;
&lt;h3 id=&quot;write-tests-for-the-neural-network-components&quot;&gt;Write tests for the neural network components&lt;/h3&gt;&lt;p&gt;Once we have the neural network model and its components implemented,
writing tests for those components is a wonderful way of making sure
that
(1) the implementation is correct, to the best of our knowledge, and that
(2) we can catch when the implementation might have been broken inadvertently.&lt;/p&gt;
&lt;p&gt;The shape test (as described above) is one way of doing this.&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;test_layer_shapes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;assert&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_dims&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If there are special elementwise transforms performed on the data,
such as a ReLU or exponential transform,
we can test that the numerical properties of the output are correct:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;test_layer_shapes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nonlinearity&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;relu&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;assert&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id=&quot;write-tests-for-the-entire-training-loop&quot;&gt;Write tests for the entire training loop&lt;/h3&gt;&lt;p&gt;Once the model has been re-implemented in its entirety,
prepare a small set of training data,
and pass it through the model,
and attempt to train it for a few epochs.&lt;/p&gt;
&lt;p&gt;If the model, as implemented, is doing what we think it should be,
then after a dozen epochs or so,
the training loss should go down.
We can then test that the training loss at the end
is less than the training loss at the beginning.
If the loss does go down, it's necessary but not sufficient for knowing
that the model is implemented correctly.
However, if the loss &lt;em&gt;does not&lt;/em&gt; go down, then we will definitely know
that a problem exists somewhere in the code, and can begin to debug.&lt;/p&gt;
&lt;p&gt;An example with pseudocode below might look like the following:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dummy_graph_data&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gnn_model&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;make_gnn_params&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;losses&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mse_loss&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;jax&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;jax.experimental.optimizers&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;adam&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;test_gnn_training&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Prepare training data&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dummy_graph_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kwargs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;make_gnn_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kwargs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;dloss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mse_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;update&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;adam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.005&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;start_loss&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mse_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dloss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;update&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;end_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mse_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;assert&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;end_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start_loss&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;A side benefit of this is that
if you commit to only judiciously changing the tests,
you will end up with a stable
and copy/paste-able
training loop that you know you can trust
on new learning tasks,
and hence only need to worry about swapping out the data.&lt;/p&gt;
&lt;h3 id=&quot;build-little-tools-for-yourself-that-automate-repetitive-boring-things&quot;&gt;Build little tools for yourself that automate repetitive (boring) things&lt;/h3&gt;&lt;p&gt;You may notice in the above integration test,
we wrote a lot of other functions
that make testing much easier,
such as dummy data generators,
and parameter initializers.&lt;/p&gt;
&lt;p&gt;These are tools that make composing parts of the entire training process
modular and easy to compose.
I strongly recommend writing these things,
and also backing them with more tests
(since we will end up relying on them anyways).&lt;/p&gt;
&lt;h3 id=&quot;now-run-your-deep-learning-experiments&quot;&gt;Now run your deep learning experiments&lt;/h3&gt;&lt;p&gt;Once we have the model re-implemented and tested,
the groundwork is present for us to conduct extensive experiments
with the confidence that we know
how to catch bugs in the model
in a fairly automated fashion.&lt;/p&gt;
&lt;h2 id=&quot;concluding-words&quot;&gt;Concluding words&lt;/h2&gt;&lt;p&gt;Re-implementing deep learning models can be a very fun and rewarding exercise,
because it serves as an excellent tool
to check our understanding of the models that we work with.&lt;/p&gt;
&lt;p&gt;Without the right safeguards in place, though,
it can also very quickly metamorphose into a nightmare rabbithole of debugging.
Placing basic safeguards in place when re-implementing models
helps us avoid as many of these rabbitholes as possible.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/10/30/code-review-in-data-science/">
    <title type="text">Code review in data science</title>
    <id>urn:uuid:56184e25-58be-3132-aed3-fed449074129</id>
    <updated>2019-10-30T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/10/30/code-review-in-data-science/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;&lt;em&gt;This blog post is cross-posted in my &lt;a href=&quot;https://ericmjl.github.io/essays-on-data-science/workflow/code-review/&quot;&gt;essays collection&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The practice of code review is extremely beneficial to the practice of software engineering.
I believe it has its place in data science as well.&lt;/p&gt;
&lt;h2 id=&quot;what-code-review-is&quot;&gt;What code review is&lt;/h2&gt;&lt;p&gt;Code review is the process by which a contributor's newly committed code
is reviewed by one or more teammate(s).
During the review process, the teammate(s) are tasked with ensuring that they&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;understand the code and are able to follow the logic,&lt;/li&gt;
&lt;li&gt;find potential flaws in the newly contributed code,&lt;/li&gt;
&lt;li&gt;identify poorly documented code and confusing use of variable names,&lt;/li&gt;
&lt;li&gt;raise constructive questions and provide constructive feedback&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;on the codebase.&lt;/p&gt;
&lt;p&gt;If you've done the practice of scientific research before,
it is essentially identical to peer review,
except with code being the thing being reviewed instead.&lt;/p&gt;
&lt;h2 id=&quot;what-code-review-_isn-t_&quot;&gt;What code review &lt;em&gt;isn't&lt;/em&gt;&lt;/h2&gt;&lt;p&gt;Code review is not the time
for a senior person to slam the contributions of a junior person,
nor vice versa.&lt;/p&gt;
&lt;h2 id=&quot;why-data-scientists-should-do-code-review&quot;&gt;Why data scientists should do code review&lt;/h2&gt;&lt;p&gt;The first reason is to ensure that project knowledge
is shared amongst teammates.
By doing this, we ensure that
in case the original code creator needs to be offline for whatever reason,
others on the team cover for that person and pick up the analysis.
When N people review the code, N+1 people know what went on.
(It does not necessarily have to be N == number of people on the team.)&lt;/p&gt;
&lt;p&gt;In the context of notebooks, this is even more important.
An analysis is complex,
and involves multiple modelling decisions and assumptions.
Raising these questions,
and pointing out where those assumptions should be documented
(particularly in the notebook)
is a good way of ensuring
that N+1 people know those implicit assumptions that go into the model.&lt;/p&gt;
&lt;p&gt;The second reason is that
even so-called &quot;senior&quot; data scientists are humans,
and will make mistakes.
With my interns and less-experienced colleagues,
I will invite them to constructively raise queries about my code
where it looks confusing to them.
Sometimes, their lack of experience gives me an opportunity to explain
and share design considerations during the code review process,
but at other times, they are correct, and I have made a mistake in my code
that should be rectified.&lt;/p&gt;
&lt;h2 id=&quot;what-code-review-can-be&quot;&gt;What code review can be&lt;/h2&gt;&lt;p&gt;Code review can become a very productive time of learning for all parties.
What it takes is the willingness to listen to the critique provided,
and the willingness to raise issues on the codebase in a constructive fashion.&lt;/p&gt;
&lt;h2 id=&quot;how-code-review-happens&quot;&gt;How code review happens&lt;/h2&gt;&lt;p&gt;Code review happens usually in the context of a pull request
to merge contributed code into the master branch.
The major version control system hosting platforms (GitHub, BitBucket, GitLab)
all provide an interface to show the &quot;diff&quot;
(i.e. newly contributed or deleted code)
and comment directly on the code, in context.&lt;/p&gt;
&lt;p&gt;As such, code review can happen entirely asynchronously, across time zones,
and without needing much in-person interaction.&lt;/p&gt;
&lt;p&gt;Of course, being able to sync up either via a video call,
or by meeting up in person,
has numerous advantages by allowing non-verbal communication to take place.
This helps with building trust between teammates,
and hence doing even &quot;virtual&quot; in-person reviews
can be a way of being inclusive towards remote colleagues.&lt;/p&gt;
&lt;h2 id=&quot;parting-words&quot;&gt;Parting words&lt;/h2&gt;&lt;p&gt;If your firm is set up to use a version control system,
then you probably have the facilities to do code review available.
I hope this essay encourages you to give it a try.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/10/29/ai-will-not-solve-medicine/">
    <title type="text">“AI will not solve medicine”</title>
    <id>urn:uuid:2f60d040-fd91-3f65-986f-e24b5d432f6d</id>
    <updated>2019-10-29T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/10/29/ai-will-not-solve-medicine/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;Those who think “AI will solve medicine” are delusional.
I say this as a practitioner of machine learning in drug discovery and development.&lt;/p&gt;
&lt;p&gt;First things first, “AI” is an overused term.
We should stop using it, especially in medicinal research.&lt;/p&gt;
&lt;p&gt;Now, my thoughts are more sanguine.
The real value proposition of machine learning models in drug development
is to navigate chemical, sequence, pathway, and knowledge space
faster and smarter than we might otherwise do so
without machine learning methods.
It’s a modeling tool, and nothing more than that.
It’s a tool for helping the human collective make better decisions than without it,
but it’s also a double-edged sword.
We can use the tool and then constrain our thinking because we have that tool,
because we want to continue using that tool.
Or we can use the tool to our advantage
and liberate our mind to think of other things.&lt;/p&gt;
&lt;p&gt;This thought was sparked off by an email that I was on at work.
A molecule was approved for &lt;em&gt;continued investigation&lt;/em&gt; (not even “go for safety trials”!),
and 63 people were on that email.
Imagine the number of people
who are involved in getting a molecule past all research-phase checkpoints
&lt;em&gt;and&lt;/em&gt; all 3 clinical trial checkpoints.
&lt;em&gt;Hint: Many people are involved.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;As I combed through the names on that email,
the number of machine learners was vastly outnumbered by the number of colleagues
who toiled daily at the bench,
wrangling with even more uncertainty than that we have at our computers.
We machine learners work in service of them,
delivering insights and prioritized directions,
just as they toil to generate the data that our data-hungry models need.
It’s a symbiotic relationship.&lt;/p&gt;
&lt;p&gt;What do all of those 63 people work on?&lt;/p&gt;
&lt;p&gt;Some make the molecules.
Others design the assays to test the molecules in.
Yet others design the assays to find the target to then develop the assay for.
It’s many layers of human creativity in the loop.
I can’t automate the entirety of their work with my software tools, but I can augment them.
I mean, yeah, I can find a new potential target,
but ultimately it's a molecular biologist
who develops the assay, especially if that assay has never existed before.&lt;/p&gt;
&lt;p&gt;There are others who professionally manage the progress of the project.
There’s sufficient complexity at the bench
and in the silicon chips
that we can’t each keep track of the big picture.
Someone has to do that, and keep everybody focused.&lt;/p&gt;
&lt;p&gt;And then there’s the handful of us who deal with numbers and mainly just numbers.
Yes, it’s a handful.
I counted them on my fingers.
We do have an outsized impact compared to our numbers,
but that’s because we can get computers to do our repetitive work for us.
At the bench, robots are harder to work with.
Having been at the bench before and failing badly at it,
I can very much empathize with how tedious the work is.
It’s expensive to collect that data,
so the onus is on us computation types to get help navigate “data space” more smartly.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/10/18/caching-long-running-function-results/">
    <title type="text">Caching Long-Running Function Results</title>
    <id>urn:uuid:7c950ab3-a3cb-3eaa-b470-3e746ed8ca30</id>
    <updated>2019-10-18T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/10/18/caching-long-running-function-results/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;I found this nifty tool for caching the results of long-running functions: &lt;a href=&quot;https://pypi.org/project/cachier/&quot;&gt;&lt;code&gt;cachier&lt;/code&gt;&lt;/a&gt;. This is useful when we’re building, say, Python applications for which quick interactions are necessary, or for caching the results of a long database query.&lt;/p&gt;
&lt;p&gt;How do we use it? Basically it’s nothing more than a decorator!&lt;/p&gt;
&lt;p&gt;Let’s imagine I have a long-running function as below.&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;long_running_function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arg1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arg2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# stuff happens&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Turns out, if you have a need to cache the result in a lightweight fashion, you can simply add &lt;code&gt;cachier&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;cachier&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cachier&lt;/span&gt;

&lt;span class=&quot;nd&quot;&gt;@cachier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;long_running_function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arg1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arg2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# stuff happens&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The result is stored in your home directory, so the cache is accessible to you.&lt;/p&gt;
&lt;p&gt;One nice thing &lt;code&gt;cachier&lt;/code&gt; also offers is the ability to set a time duration after which the cache goes stale. This can be useful in situations where you know that you need to refresh the cache, such as a database query that may go stale because of new data added into it. This is done by specifying the &lt;code&gt;stale_after&lt;/code&gt; keyword argument:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;cachier&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cachier&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;datetime&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;timedelta&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Re-cache result after 1 week.&lt;/span&gt;
&lt;span class=&quot;nd&quot;&gt;@cachier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stale_after&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;timedelta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weeks&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;long_running_function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arg1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arg2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# stuff happens&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If you need to reset the cache manually, you can always do:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;long_running_function&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clear_cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;There are other advanced features that &lt;code&gt;cachier&lt;/code&gt; provides, and so I’d encourage you to go and take a look at it!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/10/5/multiple-coin-flips-vs-one-coin-flip-generalized/">
    <title type="text">Multiple Coin-Flips vs. One Coin Flip Generalized?</title>
    <id>urn:uuid:4f62bcf6-339a-3ffa-b585-fc7108ab6371</id>
    <updated>2019-10-05T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/10/5/multiple-coin-flips-vs-one-coin-flip-generalized/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;Do people learn better by:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Generalizing from one example explained well, or by&lt;/li&gt;
&lt;li&gt;Having multiple case studies that highlight the same point?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I think both are needed, but I am also torn sometimes by whether it’s more effective to communicate using the former or the latter.&lt;/p&gt;
&lt;p&gt;Case in point: In teaching Bayesian statistics, the coin flip is a particular case of the Beta-Binomial model. However, the Beta-Binomial model can be taken from its most elementary form (estimation on one group) through to its most sophisticated form (hierarchically estimating &lt;code&gt;p&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;I guess if the goal is to show how broadly applicable a given model class (i.e. the beta-binomial model) is, a teacher would elect to jump between multiple examples that are apparently distinct. However, if the goal is build depth (i.e. going from single group to multiple group estimation), sticking with one example (e.g. of baseball players, classically) would be the better strategy.&lt;/p&gt;
&lt;p&gt;Both are needed, just at different times, I think. Thinking through this example, I think, gives me a first-principles way of deciding which approach to go for.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/10/5/jupyter-server-with-https-on-personal-server/">
    <title type="text">Jupyter Server with HTTPS on Personal Server</title>
    <id>urn:uuid:76cd7411-e7c6-320a-8edd-b0551d6aaac3</id>
    <updated>2019-10-05T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/10/5/jupyter-server-with-https-on-personal-server/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;Recording this for myself, since I did it once and probably don't have the brain bandwidth to remember this through repetition.&lt;/p&gt;
&lt;p&gt;I have known how to run a &quot;public&quot; Jupyter server (password-protected, naturally), but one thing I've struggled with was getting HTTPS working.&lt;/p&gt;
&lt;p&gt;Turns out, the &lt;code&gt;letsencrypt&lt;/code&gt; instructions aren't that bad on Jupyter's docs. I just was ignorant in the past, and didn't know enough about Linux to get this working right.&lt;/p&gt;
&lt;p&gt;The key here is creating a &lt;code&gt;letsencrypt&lt;/code&gt; certificate, and making sure file permissions are set correctly.&lt;/p&gt;
&lt;p&gt;First off, go to the &lt;a href=&quot;https://certbot.eff.org&quot;&gt;Certbot page&lt;/a&gt;. Select the type of website you're running and operating system. For Jupyter, I chose &quot;None of the Above&quot; and &quot;Ubuntu 18.04 LTS (bionic)&quot; (even though I'm technically on Ubuntu 19). (Here's a &lt;a href=&quot;https://certbot.eff.org/lets-encrypt/ubuntubionic-other&quot;&gt;shortcut link&lt;/a&gt; to the instructions if you're in the same situation.)&lt;/p&gt;
&lt;p&gt;On my system (Ubuntu-based), I used the following commands to install &lt;code&gt;certbot&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Add repository&lt;/span&gt;
sudo apt-get update
sudo apt-get install software-properties-common
sudo add-apt-repository universe
sudo add-apt-repository ppa:certbot/certbot
sudo apt-get update

&lt;span class=&quot;c1&quot;&gt;# Install certbot&lt;/span&gt;
sudo apt-get install certbot

&lt;span class=&quot;c1&quot;&gt;# Run certbot&lt;/span&gt;
sudo certbot certonly --standalone
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Follow the instructions. &lt;code&gt;certbot&lt;/code&gt; will install into a protected directory. In my case, it was &lt;code&gt;/etc/letsencrypt/live/&amp;lt;mywebsite&amp;gt;/&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Here, a problem will show up. That directory above is not accessible by a Jupyter server run under a user other than &lt;code&gt;root&lt;/code&gt;. But a desired property of running Jupyter servers is that we don't have to use &lt;code&gt;sudo&lt;/code&gt; to run it. How can we solve this? Basically, by making sure that the certificate is readable by a non-&lt;code&gt;root&lt;/code&gt; user.&lt;/p&gt;
&lt;p&gt;What I did, then, was to copy the files that were created by &lt;code&gt;certbot&lt;/code&gt; into a location under my home directory. For security by obscurity, I'm naturally not revealing its identity. Then, I changed ownership of those files to my username:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;pwd&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# you should be in the directory where the certbot-created files are located&lt;/span&gt;
su -
chown &amp;lt;myusername&amp;gt; *.pem  &lt;span class=&quot;c1&quot;&gt;# changes ownership of those files&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Finally, I went into my Jupyter config (&lt;code&gt;~/.jupyter/jupyter_notebook_config.py&lt;/code&gt;, this is well-known), and edited the two lines that specified the &quot;certfile&quot; and the &quot;keyfile&quot;:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NotebookApp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;certfile&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;sa&quot;&gt;u&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;/absolute/path/to/your/certificate/mycert.pem&amp;#39;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NotebookApp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keyfile&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;sa&quot;&gt;u&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;/absolute/path/to/your/certificate/mykey.key&amp;#39;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If this helps you, leave me a note in the comments below. :)&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/9/7/dokku-building-an-internal-heroku-at-work/">
    <title type="text">Dokku: Building an internal Heroku at work</title>
    <id>urn:uuid:e193b1a1-5835-3857-b7ba-5e27a6f3bdc3</id>
    <updated>2019-09-07T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/9/7/dokku-building-an-internal-heroku-at-work/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;At work, we don’t have a service that has the simplicity of Heroku. Part of it is that we’re still behind what’s available for free in my FOSS life (both commercial and FOSS offerings), and cybersecurity tends to be a gatekeeper against adoption of new things, which is a reality I have to face at work.&lt;/p&gt;
&lt;p&gt;BUT! I am unwilling to simply bow down to this secnario. “There’s got to be a better way.”&lt;/p&gt;
&lt;p&gt;What does that mean? It means if we want a Heroku-like thing internally, we have to hack together workarounds.&lt;/p&gt;
&lt;p&gt;Enter &lt;a href=&quot;http://dokku.viewdocs.io/dokku/&quot;&gt;Dokku&lt;/a&gt;!&lt;/p&gt;
&lt;p&gt;What is it? It’s a FOSS implementation of the functionality that Heroku provides. It’s only slightly more involved than Heroku, and gives us a really nice taste of what’s possible with Heroku.&lt;/p&gt;
&lt;p&gt;Dokku claims to be the “smallest PaaS implementation you’ve ever seen”, and I fully believe it. The maintainers have done a wonderful thing, making the installation process as simple and clean as possible. I’ve successfully installed it on a bare DigitalOcean droplet and on my home Linux tower. I’ve also successfully installed it in EC2 instances at work, albeit needing a few minor modifications to the script they provide.&lt;/p&gt;
&lt;h2 id=&quot;why-would-i-want-to-use-dokku?&quot;&gt;Why would I want to use Dokku?&lt;/h2&gt;&lt;p&gt;Taking Dokku on my DigitalOcean droplet as an example, what it effectively provides is a self-hosted Heroku.&lt;/p&gt;
&lt;p&gt;This means you can get 95% of the convenience that Heroku offers, except done in-house. This can be handy if you’ve got cybersecurity standing in the way of awesome convenience, or if finance isn’t willing to shell out the moolah.&lt;/p&gt;
&lt;h2 id=&quot;what-can-we-do-with-dokku?&quot;&gt;What can we do with Dokku?&lt;/h2&gt;&lt;p&gt;Here’s a few neat things that we can do.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We can provision a database to run on the same compute node as the app, and then link them together. If your compute node is “beefy” enough (RAM/CPU/storage-wise) to handle both the database and the app (and I mean, I’m confident that most disposable apps aren’t going to be at a large scale), then it can be pretty handy, because it means we save on latency.&lt;/li&gt;
&lt;li&gt;We can deploy apps using either Heroku buildpacks (which look for Procfiles) or using Dockerfiles. Docker containers can be easier to maintain if we have a large and/or complex &lt;code&gt;conda&lt;/code&gt; environment, in my opinion, as we can reuse the existing environment spec, but Procfiles are much nicer for smaller projects. This fits with the paradigm of “declaring what you need”, rather than “programming what you need”. &lt;/li&gt;
&lt;li&gt;Because Dokku is managing everything through isolated Docker containers, we can actually enter into a Docker container and muck around to debug, without worrying about breaking the broader system. I realize now how neat it is to have containerization, but without a unified front-end interface to manage the containers, networking interfaces, and environment variables, it’s tough to keep everything straight. Dokku provides that front-end interface.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;what-are-you-deploying-right-now?&quot;&gt;What are you deploying right now?&lt;/h2&gt;&lt;p&gt;On my DigitalOcean box, which I use for personal projects, I have deployed both the “Getting Started” Ruby app that Heroku provides as well as a &lt;a href=&quot;../../../../../blog/2019/9/7/dokku-building-an-internal-heroku-at-work/minimal-panel.ericmjl.com&quot;&gt;minimal app&lt;/a&gt; showcasing a minimal dashboard using &lt;a href=&quot;../../../../../blog/2019/9/7/dokku-building-an-internal-heroku-at-work/panel.pyviz.org&quot;&gt;Panel&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The easy part was getting Dokku up and running. The hard part, though, was getting URLs and DNSs right. It took some debugging to get that work correctly.&lt;/p&gt;
&lt;p&gt;In particular, Dokku uses a concept called virtual hosts (&lt;code&gt;VHOSTS&lt;/code&gt;) to route from the Dokku host to individual containers. For example, to get &lt;code&gt;minimal-panel.ericmjl.com&lt;/code&gt; up and running correctly, I had to ensure that &lt;code&gt;*.ericmjl.com&lt;/code&gt; was routed to my DigitalOcean box.&lt;/p&gt;
&lt;h2 id=&quot;how-have-we-used-this-at-work?&quot;&gt;How have we used this at work?&lt;/h2&gt;&lt;p&gt;At work, I just finished prototyping the use of Dokku on EC2. In particular, I was able to deploy both Dockerfile-based and Procfile-based projects. Once again, getting a domain was the most troublesome part of this project; spinning up an EC2 instance and configuring it became easy using a simple Bash script which we executed on each test spin-up machine.&lt;/p&gt;
&lt;h2 id=&quot;what-changes-between-heroku-and-dokku?&quot;&gt;What changes between Heroku and Dokku?&lt;/h2&gt;&lt;p&gt;The biggest thing I found is that I need to at least have SSH-access to the compute box that is running Dokku. This is because what we would usually configure on Heroku’s web interface (e.g. environment variables), we would instead configure using &lt;code&gt;dokku&lt;/code&gt;’s command-line interface via SSH. Hence, not being afraid of the CLI is important.&lt;/p&gt;
&lt;h2 id=&quot;whats-your-verdict?&quot;&gt;What’s your verdict?&lt;/h2&gt;&lt;p&gt;If you know Heroku, Dokku gets you 95% of the convenience you’re used to, plus quite a bit more flexibility to customize it to your own compute environment.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/7/29/how-to-be-a-great-code-sprinter/">
    <title type="text">How to be a great code sprinter</title>
    <id>urn:uuid:871d5f77-e092-3bbc-a4ae-5584c93b64e2</id>
    <updated>2019-07-29T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/7/29/how-to-be-a-great-code-sprinter/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;This blog post is the second in a series of two on participating in code sprints. The first one is &lt;a href=&quot;https://ericmjl.github.io/blog/2019/7/21/how-to-lead-a-great-code-sprint/&quot;&gt;here&lt;/a&gt;. In this post, I will write about how a sprinter themselves can also help contribute to a positive sprint experience all-ways.&lt;/p&gt;
&lt;h2 id=&quot;read-the-docs-to-understand-the-scope-of-the-project&quot;&gt;Read the docs to understand the scope of the project&lt;/h2&gt;&lt;p&gt;As a sprinter, we may often have preconceived notions about what a project is about. It helps to have an accurate view on what a project is and isn’t about. This is oftentimes best accomplished by reading the documentation of that project, assuming the docs are well-written. Doing so can help you better align what you think should be done on the project with what the package maintainer sees as priorities for the project.&lt;/p&gt;
&lt;h2 id=&quot;be-ready-to-make-documentation-contributions-of-any-scale&quot;&gt;Be ready to make documentation contributions of any scale&lt;/h2&gt;&lt;p&gt;Documentation is oftentimes the hardest thing for a package maintainer to write, because it often entails slowing down to a beginner’s speed (an unnatural speed at this point), while knowing one’s own blind spots on where a beginner would stumble (also challenging to do).&lt;/p&gt;
&lt;p&gt;If you are newcomer sprinter, by focusing on the sections of the docs that pertain to “processes” (e.g. getting development environment setup) and slowly working through them and documenting what’s missing, that can go a long way to helping other newcomers get set up as well. Anything that the maintainer leaves out may need to be made explicitly clear - and you can help make it clear!&lt;/p&gt;
&lt;p&gt;Package maintainers, and prior contributors, are human. That means that there inadvertently may be errors in language that may have been inserted into the package. Any small patch that fixes the docs, including even small typographical errors, can be very helpful to improving documentation quality.&lt;/p&gt;
&lt;h2 id=&quot;dont-be-afraid-to-ask-questions...&quot;&gt;Don’t be afraid to ask questions...&lt;/h2&gt;&lt;p&gt;You will find that asking questions can really accelerate your own progress on the project. This is important for getting unstuck, wherever you might be stuck.&lt;/p&gt;
&lt;h2 id=&quot;...but-also-try-keep-your-questions-to-the-non-obvious-things.&quot;&gt;...but also try keep your questions to the non-obvious things.&lt;/h2&gt;&lt;p&gt;That said, asking the too-simple questions that can be answered by a Google query is likely going to steal time and attention away from other sprinters who might have more substantial questions on hand.&lt;/p&gt;
&lt;p&gt;A pet peeve of mine is asking questions that can be answered in the docs. Asking these questions of the maintainer doesn’t reflect positively on you, the sprinter. Whether or not you intended, what often gets received/communicated to the maintainer is carelessness and a lack of attention to detail, the opposite of both being generally good qualities to possess and project.&lt;/p&gt;
&lt;p&gt;There’s a pretty broad balance point between the two, so don’t feel inhibited by fear of not hitting a precise balance between looking for docs and asking questions.&lt;/p&gt;
&lt;h2 id=&quot;for-any-feature-requests-try-to-be-ready-with-a-proposed-implementation&quot;&gt;For any feature requests, try to be ready with a proposed implementation&lt;/h2&gt;&lt;p&gt;This one I find very important. Having a proposed implementation on hand for a thing that you think should be in the library goes a long way to helping the package maintainer (or other contributors) see what exactly you’re trying to accomplish with that feature. Having a sketch on-hand makes it much easier for the package maintainer to say “yes” to the new feature, and having written the documentation and a proposed suite of tests for that new feature makes it &lt;em&gt;even easier&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;If you aren’t able to propose an implementation, then raising an inquiry rather than a request makes a world of difference in how a package maintainer perceives the communication of the issue at hand.&lt;/p&gt;
&lt;p&gt;As an example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Request&lt;/strong&gt;: “Package X should be able to do Y.”&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Inquiry&lt;/strong&gt;: “Is it within scope for package X to be able to do Y?” or “Has this feature Y been considered before?”&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The latter are more thoughtful, and communicates much less a sense of entitlement on the part of the sprinter’s request.&lt;/p&gt;
&lt;h2 id=&quot;were-all-building-mental-maps-of-each-others-knowledge&quot;&gt;We’re all building mental maps of each others’ knowledge&lt;/h2&gt;&lt;p&gt;When two colleagues meet for the first time, we have to build a mental model of each others’ strengths. At a sprint, the package maintainer has to multiply this by however many people are sprinting.&lt;/p&gt;
&lt;p&gt;If they are making an effort to map your skills against theirs, they may be very verbose, asking lots of questions to clarify what you do and don’t know. It pays to be patient here.&lt;/p&gt;
&lt;p&gt;If they don’t have the bandwidth to do so (and this is a charitable description for some maintainers), then they may be glossing over detail. Rather than being stuck, it pays to interrupt them gently and clarify. (Taking notes is a very good way of communicating that you’re treating this process seriously too!)&lt;/p&gt;
&lt;h2 id=&quot;give-your-sprint-leader-sufficient-context&quot;&gt;Give your sprint leader sufficient context&lt;/h2&gt;&lt;p&gt;As mentioned above, the sprint leader will oftentimes be context switching from person to person. It’s mentally exhausting, so spoon-feeding a bit more context (such as the thing you’re working on), and condensing your question to the essentials and asking it very precisely can go a long way to helping your sprint leader help you better.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/7/26/pyviz-panel-apps/">
    <title type="text">PyViz Panel Apps</title>
    <id>urn:uuid:45aaf34d-e0d0-3611-99df-f9c8e2650355</id>
    <updated>2019-07-26T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/7/26/pyviz-panel-apps/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;I finally learned how to build and serve apps with Panel!&lt;/p&gt;
&lt;p&gt;Here are the key ideas:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Prototype the app inside a Jupyter notebook. That gives the real-time feedback on whether your apps/widgets are working or not. &lt;/li&gt;
&lt;li&gt;The most important thing is that the final thing you package together is now a &lt;code&gt;.servable()&lt;/code&gt; object.&lt;/li&gt;
&lt;li&gt;Use Panel’s &lt;code&gt;serve&lt;/code&gt; command to test the app locally. It’s actually quite magical - the serve command can actually parse a Jupyter notebook and serve it up on a local web server.&lt;/li&gt;
&lt;li&gt;When you’ve confirmed that everything is working properly locally, Heroku is a great deployment option. Using the default Python buildpack and a &lt;code&gt;requirements.txt&lt;/code&gt; file, one can easily specify the exact Python environment for deployment.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As a pedagogical implementation, I put up a &lt;a href=&quot;https://github.com/ericmjl/minimal-panel-app&quot;&gt;minimal panel app on GitHub&lt;/a&gt;, and also &lt;a href=&quot;https://minimal-panel-app.herokuapp.com&quot;&gt;served it up on Heroku&lt;/a&gt;. Come check it out! I hope it’s useful for you.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/7/23/t-distributed-likelihoods-are-kind-of-neat/">
    <title type="text">T-distributed likelihoods are kind of neat</title>
    <id>urn:uuid:73bf8792-c5e2-3d6b-8dd6-0d3bf2e6aa6d</id>
    <updated>2019-07-23T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/7/23/t-distributed-likelihoods-are-kind-of-neat/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;The Student’s T distribution is the generalization of the Gaussian and Cauchy distributions. How so? Basically by use of its “degrees of freedom” ($df$) parameter.&lt;/p&gt;
&lt;p&gt;If we plot the probability density functions of the T distribution with varying degrees of freedom, and compare them to the Cauchy and Gaussian distributions, we get the following:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;../../../../../blog/2019/7/23/t-distributed-likelihoods-are-kind-of-neat/t-distributions.png&quot;&gt;&lt;img src=&quot;../../../../../blog/2019/7/23/t-distributed-likelihoods-are-kind-of-neat/t-distributions.png&quot; alt=&quot;Student T distributions with varying degrees of freedom.&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Notice that when $df=1$, the T distribution is identical to the Cauchy distribution, and that as $df$ increases, it gradually becomes more and more like the Normal distribution. At $df=30$, we can consider it to be approximately enough Gaussian.&lt;/p&gt;
&lt;p&gt;On its own, this is already quite useful; when placed in the context of a hierarchical Bayesian model, that’s when it gets even more interesting! In a hierarchical Bayesian model, we are using samples to estimate group-level parameters, but constraining group parameters to vary mostly like each other, unless evidence in the data suggests otherwise. If we allow the $df$ parameter to vary, then if some groups look more Cauchy while other groups look more Gaussian, this can be flexibly captured in the model.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/7/21/how-to-lead-a-great-code-sprint/">
    <title type="text">How to lead a great code sprint</title>
    <id>urn:uuid:d31597fa-6b63-37e8-adbf-b3bf799955a4</id>
    <updated>2019-07-21T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/7/21/how-to-lead-a-great-code-sprint/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;This blog post is the first in a series of two blog posts on participating in code sprints, and is the culmination of two other blog posts I’ve written on leading a sprint. In this post, I’ll be writing it from the perspective of what a sprint participant might appreciate from a sprint leader.&lt;/p&gt;
&lt;h2 id=&quot;write-good-docs&quot;&gt;Write good docs&lt;/h2&gt;&lt;p&gt;Documentation scales you, the package maintainer. Good docs let others get going without needing your intervention, while bad docs create more confusion. Write good docs ahead-of-time on:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The purpose and scope of the project.&lt;/li&gt;
&lt;li&gt;How to get setup for contributing&lt;/li&gt;
&lt;li&gt;What contributors should look out for when contributing, including:&lt;ul&gt;
&lt;li&gt;Code style&lt;/li&gt;
&lt;li&gt;Function scope&lt;/li&gt;
&lt;li&gt;Documentation requirements&lt;/li&gt;
&lt;li&gt;Testing requirements&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;How contributors can contribute without necessarily providing code&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;require-documentation-as-a-first-contribution&quot;&gt;Require documentation as a first contribution&lt;/h2&gt;&lt;p&gt;This may not necessarily apply to all projects, but for small-ish enough projects, this might be highly relevant. Requiring documentation contributions as the first contribution has a few nice side effects for newcomer contributors:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;This enforces familiarity with the project before making contributions.&lt;/li&gt;
&lt;li&gt;It’s a very egalitarian way to kick-off the sprints, reducing the probability of sprinter anxiety from falling behind.&lt;/li&gt;
&lt;li&gt;This reduces the burden of new contributions for first-time sprinters: docs do not break code!&lt;/li&gt;
&lt;li&gt;Apart from being non-intimidating, it can sometimes give rise to repetitive tasks that newcomer sprinters with which newcomers can practice git workflow.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;make-clear-who-should-sprint-with-your-project&quot;&gt;Make clear who should sprint with your project&lt;/h2&gt;&lt;p&gt;Mismatched expectations breed frustration; hence, making clear what pre-requisite knowledge participants should have can go a long way to reducing frustrations later on.&lt;/p&gt;
&lt;p&gt;Drawing on my &lt;code&gt;pyjanitor&lt;/code&gt; experience, I would want participants to at the minimum be me of the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;pandas&lt;/code&gt; users who have frustrations with the library, and would like to make contributions, or&lt;/li&gt;
&lt;li&gt;Individuals who wish to make a documentation contribution and don’t mind doing a fine-toothed pass over the docs to figure out what is unclear in the docs.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Defining your so-called “sprint audience” ahead-of-time can go a long way to making the sprint productive and friendly.&lt;/p&gt;
&lt;h2 id=&quot;communicate-priorities-of-the-project&quot;&gt;Communicate priorities of the project&lt;/h2&gt;&lt;p&gt;Sprinters want to know that their contributions are going to be valued. Though it is easy to say, “come talk with me before you embark on a task”, the reality is that you, the sprint lead, are likely going to be extremely overbooked. One way to get around this, I think, is to have a high-level list of priorities for the sprint, which can help sprinters better strategize which tasks to tackle. Communicate this on a whiteboard, large sticky notes, or online Wiki page that you can direct people to.&lt;/p&gt;
&lt;h2 id=&quot;have-a-publicly-viewable-file-lock&quot;&gt;Have a publicly-viewable “file lock”&lt;/h2&gt;&lt;p&gt;Merge conflicts will inadvertently show up if multiple people are contributing to the same file simultaneously. It helps to have a publicly-viewable “file lock” on, say, a whiteboard or large sticky notes, so that we know who is working on what file. This helps prevent you, the sprint lead, from accidentally getting two people to work on the same file, and then having to resolve merge conflicts later.&lt;/p&gt;
&lt;p&gt;In the &lt;code&gt;pyjanitor&lt;/code&gt; sprints, I frequently approved two people working on the same notebook; resolving merge conflicts in the notebook JSON later proved to be a big pain! This lesson was one learned hard.&lt;/p&gt;
&lt;h2 id=&quot;encourage-contribution-of-life-like-examples&quot;&gt;Encourage contribution of life-like examples&lt;/h2&gt;&lt;p&gt;If there are new package users in the crowd who want to get familiar with the package, then encouraging them to contribute life-like examples is a great way to have them make a contribution! This has some nice side effects:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;In creating the example, they may find limitations in the package that could form the substrate of future contributions.&lt;/li&gt;
&lt;li&gt;By using the library in the creation of an example, they become users of the project themselves.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;celebrate-every-contribution&quot;&gt;Celebrate every contribution&lt;/h2&gt;&lt;p&gt;This one is particularly important for first-time contributors. Oftentimes, they have never done standard Gitflow, and that is intimidating enough. So it doesn’t matter if the contribution is nothing more than deleting an unnecessary plural &lt;code&gt;s&lt;/code&gt; or correcting a broken URL. We should celebrate that contribution, because they have now learned how to make a contribution (regardless of type), and can repeat the unfamiliar Gitflow pattern until they have it muscle memorized.&lt;/p&gt;
&lt;p&gt;At the SciPy sprints, for the &lt;code&gt;pyjanitor&lt;/code&gt; project, once a contributor’s PR finished building and passed all checks, I brought my iPad over to their table to let them hit the Big Green Button on GitHub. This is one touch I am quite confident our sprinters loved!&lt;/p&gt;
&lt;h2 id=&quot;recognize-and-assign-non-code-tasks&quot;&gt;Recognize and assign non-code tasks&lt;/h2&gt;&lt;p&gt;While code contributions are useful, I think a great way to encourage them to help out would be to have them help with non-code contributions. A few examples include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Debugging others’ setups&lt;/li&gt;
&lt;li&gt;Triaging/tagging issues on your issue tracker&lt;/li&gt;
&lt;li&gt;Talking with sprinters to help them prioritize&lt;/li&gt;
&lt;li&gt;Social media sprinting for the project&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is because you, the sprinter, might have your hands full helping beginners, and so having as much help as possible is extremely helpful to you. Be sure, of course, the acknowledge them in some public way that expresses your appreciation of their effort, because they might not necessarily get into the commit record (which, by the way, is not the only way to keep track of contributions).&lt;/p&gt;
&lt;h2 id=&quot;stay-humble-and-calm&quot;&gt;Stay humble and calm&lt;/h2&gt;&lt;p&gt;In open source software development, it is hard to find contributors who are willing to sustain an effort, and so any contributions are generally welcome (barring those that are clearly out of scope). Hence, as far as it is humanly possible, I would be inclined to express appreciation for contributors’ contributions.&lt;/p&gt;
&lt;p&gt;One of the PyMC maintainers, &lt;a href=&quot;https://colindcarroll.com&quot;&gt;Colin Carroll&lt;/a&gt;, said something of a contribution that I wanted to make that stuck with me. The gist of it was as follows:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;It’s a contribution from someone who is willing, and I’d take that any day.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So yes, even though we may see our project as providing an opportunity for newcomers to contribute, the fact that they are willing to contribute is even more so an important thing to recognize! Gratitude makes more sense than entitlement here.&lt;/p&gt;
&lt;p&gt;Staying calm is also important. It’s easy to get irritated because of all of the context switching that happens. Leverage the help you can get from your sprint co-leads to help shoulder the load. If you take good care of your mental state, you can help make the sprints fun and productive for others.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/7/15/scipy-2019-post-conference/">
    <title type="text">SciPy 2019 Post-Conference</title>
    <id>urn:uuid:c2d83688-0df1-3f2d-9e58-23b29a5f3730</id>
    <updated>2019-07-15T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/7/15/scipy-2019-post-conference/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;It’s my last day in Austin, TX, having finished a long week of conferencing at &lt;a href=&quot;https://www.scipy2019.scipy.org/&quot;&gt;SciPy 2019&lt;/a&gt;. This trip was very fruitful and productive! At the same time, I’m ready for a quieter change - meeting and talking with people does take a drain on my brain, and I have a mildly strong preference for quiet time over interaction time.&lt;/p&gt;
&lt;h2 id=&quot;tutorials&quot;&gt;Tutorials&lt;/h2&gt;&lt;p&gt;I participated in the tutorials as an instructor for three tutorials, which I think have become my “data science toolkit”: &lt;a href=&quot;https://www.youtube.com/watch?v=2wvt6GPZl1U&quot;&gt;Bayesian statistical modeling&lt;/a&gt;, network analysis, and &lt;a href=&quot;https://www.youtube.com/watch?v=JPBz7-UCqRo&quot;&gt;deep learning&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Of the three, the one I had the most fun teaching was the deep learning one. The goal of that tutorial was to peel back a layer behind the frameworks and see what’s going on. To reinforce this and make it all concrete, we live coded a deep learning framework prototype, and it worked! (I didn’t plan for it, and so I was quite nervous while doing it, but we pulled it off as a class, and I think it reinforced the point about revealing what goes on underneath a framework.&lt;/p&gt;
&lt;p&gt;I also had a lot of fun teaching the Bayesian statistical modeling tutorial, which I had co-created with Hugo Bowne-Anderson, and as always, my personal “evergreen” tutorial on Network Analysis always brings me joy, especially when we reach the end and talk about graphs and matrices. I think the material connecting linear algebra to graph concepts is one that the crowd enjoys, and I might emphasize it more going forth at the SciPy tutorials.&lt;/p&gt;
&lt;h2 id=&quot;talks&quot;&gt;Talks&lt;/h2&gt;&lt;p&gt;This year, &lt;a href=&quot;https://www.youtube.com/watch?v=sSIT0rJh2OM&quot;&gt;I delivered a talk on &lt;code&gt;pyjanitor&lt;/code&gt;&lt;/a&gt;. Excluding lightning talks, this is probably the first time I’ve started my slides one day before having to deliver it (yikes!). Granted, I’ve had the outline in my head for a long time now, I guess having to do the talk was good impetus to actually get it done.&lt;/p&gt;
&lt;p&gt;Apart from that, there’s a rich selection of talks at SciPy from which I think we can screen at work over lunches (Data Science YouTube). I particularly like &lt;a href=&quot;https://www.youtube.com/watch?v=J_aymk4YXhg&quot;&gt;the talk&lt;/a&gt; on &lt;a href=&quot;https://optuna.org/&quot;&gt;Optuna&lt;/a&gt;, a framework for hyperparameter optimization, and I think I’ll be using this tool going forwards.&lt;/p&gt;
&lt;h2 id=&quot;sprints&quot;&gt;Sprints&lt;/h2&gt;&lt;p&gt;I did a sprint on &lt;code&gt;pyjanitor&lt;/code&gt; with my colleague Zach Barry. This sprint, we had about 20+ sprinters join us, the vast majority of them being first-time sprinters.&lt;/p&gt;
&lt;p&gt;One thing that stuck for me, this time round, is how even first-timers have different degrees of experience. Some know &lt;code&gt;git&lt;/code&gt; while most others don’t; most don’t have any prior experience with &lt;a href=&quot;https://www.atlassian.com/git/tutorials/comparing-workflows/gitflow-workflow&quot;&gt;Gitflow&lt;/a&gt;. I had an interaction that led me to realize it’s very important to state meaningfully what “beginner” means in concrete terms. For example, a “beginner” &lt;code&gt;pyjanitor&lt;/code&gt; contributor is probably a &lt;code&gt;pandas&lt;/code&gt; user, may or may not have used &lt;code&gt;git&lt;/code&gt; before, probably doesn’t know GitFlow. A common prerequisite quality amongst contributors would probably be that they would have the patience to&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Read the documentation,&lt;/li&gt;
&lt;li&gt;Attempt at least one pass digesting the documentation, and &lt;/li&gt;
&lt;li&gt;Ask questions regarding the intent behind something before asking for a change.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In terms of the things accomplished at this sprint, contributions mainly revolved around:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Improving language in the docs,&lt;/li&gt;
&lt;li&gt;New functions, and&lt;/li&gt;
&lt;li&gt;New example notebooks.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In addition to &lt;code&gt;pyjanitor&lt;/code&gt; sprinting, special thanks goes to &lt;a href=&quot;https://twitter.com/ocefpaf&quot;&gt;Felipe Fernandes&lt;/a&gt;, who helped me &lt;a href=&quot;https://anaconda.org/conda-forge/jax&quot;&gt;get &lt;code&gt;jax&lt;/code&gt; up onto conda-forge&lt;/a&gt;! SciPy is really the place where we can get to meet people &lt;em&gt;and&lt;/em&gt; get things done.&lt;/p&gt;
&lt;h2 id=&quot;career-advice-learned&quot;&gt;Career advice learned&lt;/h2&gt;&lt;p&gt;While at SciPy, I had a chance to talk with Eric Jones, CEO of Enthought. Having described my current role at work, he mentioned how having a team like the one I’m on parked inside IT gives us a very unique position to connect data science work across the organization to the consumers of our data products. When I raised to him my frustrations regarding our infatuation with vendors &lt;em&gt;when FOSS alternatives clearly exist&lt;/em&gt;, his advice in return was essentially this:&lt;/p&gt;
&lt;blockquote class=&quot;blockquote&quot;&gt;
&lt;p&gt;Focus on leveling-up your colleagues skills and knowledge, keep pushing the education piece at work, and don’t worry about the money that gets spent on tooling.&lt;/p&gt;
&lt;/blockquote&gt;&lt;p&gt;Having thought about this, I agree. Over time, we should let the results speak. At the same time, I want to help create the environment that I would like to work in: where my colleagues use the same tooling stack, are hacker-types, aren’t afraid to dig deep into the “computer stuff” and into the biology/chemistry, and have the necessary skill + desire to design machine learning systems to systematically accelerate discovery science.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/7/7/order-of-magnitude-is-more-than-accurate-enough/">
    <title type="text">Order of magnitude is more than accurate enough</title>
    <id>urn:uuid:c54c9f1a-17cd-3db2-83f7-740b261e796b</id>
    <updated>2019-07-07T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/7/7/order-of-magnitude-is-more-than-accurate-enough/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;When I was in &lt;a href=&quot;https://scienceone.ubc.ca/&quot;&gt;Science One&lt;/a&gt; at UBC in 2006, our Physics professor, &lt;a href=&quot;https://www.phas.ubc.ca/users/mark-halpern&quot;&gt;Mark Halpern&lt;/a&gt;, said a quotable statement that has stuck for many years.&lt;/p&gt;
&lt;blockquote class=&quot;blockquote&quot;&gt;
&lt;p&gt; Order of magnitude is more than accurate enough.&lt;/p&gt;
&lt;p class=&quot;blockquote-footer&quot;&gt;Mark Halpern&lt;/p&gt;
&lt;/blockquote&gt;&lt;p&gt;At the time, that statement rocked the class, myself included. We were classically taught that significant digits are significant, and that we have to keep track of them. But Mark’s quote seemed to throw all of that caution and precision in Physics into the wind. Did what we learn in Physics lab class not matter?&lt;/p&gt;
&lt;p&gt;Turns out, there was one highly instructive activity that still hasn’t left my mind. We were asked, during a recitation, to estimate how many days the city of Vancouver could be powered for if we took a piece of chalk and converted its entire mass into energy. This clearly required estimation of chalk mass and Vancouver daily energy consumption, both of which we had no way of accurately knowing.&lt;/p&gt;
&lt;p&gt;Regardless, I took it upon myself to carry significant digits in our calculation, while my recitation partner, Charles Au, was fully convinced that this wasn’t necessary, and so did all calculations order-of-magnitude. We debated and agreed upon what assumptions we needed to arrive at a solution, and then proceeded to do the same calculations, one with significant digits, the other without.&lt;/p&gt;
&lt;p&gt;We reached the same conclusion.&lt;/p&gt;
&lt;p&gt;More precisely, I remember obtaining a result along the lines of $6.2 \cdot 10^3$ days, while Charles obtained $10^4$ days. On an order of magnitude, more or less equivalent.&lt;/p&gt;
&lt;p&gt;In retrospect, I shouldn’t have been so surprised. Mark is an astrophysicist, and at that scale, 1 or 2 significant digits might not carry the most importance; rather, getting into the right ballpark might be more important. At the same time, the recitation activity was a powerful first-hand experience of that last point: getting into the right ballpark first.&lt;/p&gt;
&lt;p&gt;At the same time, I was also missing a second perspective, which then explains my surprise at Mark’s quote. Now that I’ve gone the route of more statistics-oriented work, I see a similar theme showing up. John Tukey said something along these lines:&lt;/p&gt;
&lt;blockquote class=&quot;blockquote&quot;&gt;
&lt;p&gt;Far better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question, which can always be made precise.&lt;/p&gt;
&lt;p class=&quot;blockquote-footer&quot;&gt;John Tukey&lt;/p&gt;
&lt;/blockquote&gt;&lt;p&gt;The connection to order of magnitude estimates should be quite clear here. If we’re on an order of magnitude correct on the right questions, we can always refine the answer further. If we’re precisely answering the wrong question, God help us.&lt;/p&gt;
&lt;p&gt;What does this mean for a data scientist? For one, it means that means approximate methods are usually good enough practically to get ourselves into the right ballpark; we can use pragmatic considerations to decide whether we need a more complicated model or not. It also means that when we’re building data pipelines, minimum viable products, which help us test whether we’re answering the right question, matter more than the fanciest deep learning model.&lt;/p&gt;
&lt;p&gt;So yes, to mash those two quotes together:&lt;/p&gt;
&lt;blockquote class=&quot;blockquote&quot;&gt;
&lt;p&gt;Order of magnitude estimates on the right question are more useful than precise quantifications on the wrong question. &lt;/p&gt;
&lt;p class=&quot;blockquote-footer&quot;&gt;Mashup&lt;/p&gt;
&lt;/blockquote&gt;</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/7/7/scipy-2019-pre-conference/">
    <title type="text">SciPy 2019 Pre-Conference</title>
    <id>urn:uuid:148bd441-7938-36c3-ab2c-b643de40f34c</id>
    <updated>2019-07-07T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/7/7/scipy-2019-pre-conference/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;For the 6th year running, I’m out at UT Austin for &lt;a href=&quot;https://www.scipy2019.scipy.org/&quot;&gt;SciPy 2019&lt;/a&gt;! It’s one of my favorite conferences to attend, because the latest in data science tooling is well featured in the conference program, and I get to meet in-person a lot of the GitHub usernames that I interact with online.&lt;/p&gt;
&lt;p&gt;I will be involved in three tutorials this year, which I think have become my data science toolkit: Bayesian stats, network science, and deep learning. Really excited to share my knowledge; my hope is that at least a few more people find the practical experience I’ve gained over the years useful, and that they can put it to good use in their own work too. This year is also the first year I’ve submitted a talk on &lt;code&gt;pyjanitor&lt;/code&gt;, which is a package that I have developed with others for cleaning data, also excited to share this with the broader SciPy community!&lt;/p&gt;
&lt;p&gt;I’m also looking forward to meeting the conference scholarship recipients. Together with Scott Collis and Celia Cintas, we’ve been managing the FinAid process for the past three years, and each year it heartens me to see the scholarship recipients in person.&lt;/p&gt;
&lt;p&gt;Finally, this year’s SciPy is quite unique for me, as it is the first year that I’ll be here with colleagues at work! (In prior years, I came alone, and did networking on my own.) I hope they all have as much of a fun time as I have at SciPy!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/6/30/bone-marrow-donations/">
    <title type="text">Bone Marrow Donations</title>
    <id>urn:uuid:bc446949-9164-3bef-8203-3cbff6478270</id>
    <updated>2019-06-30T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/6/30/bone-marrow-donations/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;A friend of mine just reached out to me, saying that he’s been diagnosed with leukemia. Thankfully, he’s not subject to the abysmal state of US healthcare (as he lives in a place where healthcare coverage is great), and so he’s on treatment, progressing, and hopefully has a great shot at beating this cancer.&lt;/p&gt;
&lt;p&gt;He definitely knows how to speak to a data scientist: using data. The odds of a match for a patient who needs a bone marrow transplant are 500:1. That means on average, only about 1 donor in 500 will be a match. On the other hand, under certain assumptions, every 500 donors who registers will mean one life, on average, can be saved. I did some digging myself: According to the &lt;a href=&quot;https://bloodcell.transplant.hrsa.gov/research/registry_donor_data/index.html&quot;&gt;US Health Resources and Services Administration&lt;/a&gt;, nearly every single minority ethnic group is underrepresented in donor registry databases.&lt;/p&gt;
&lt;p&gt;As things turn out, signing up to be a donor is quite lightweight. Genetic information - specifically, only Human Leukocyte Antigen (HLA) type - is needed, and that can be obtained in a non-invasive fashion. If a match is found, the donor still has the option to withdraw if they have any objections. As such, the process is completely voluntary for the donor. There are two types of donations possible: peripheral blood stem cells (PBSC) and bone marrow, with PBSC donations being lightweight and bone marrow donations being more involved. Digging a bit deeper, it seems like the only sacrifice a donor has to make is that of time and some discomfort.&lt;/p&gt;
&lt;p&gt;I’m putting this blog post up as a reminder to myself to register, and to encourage others to do so as well. If you’re in the United States, &lt;a href=&quot;https://bethematch.org/&quot;&gt;Be The Match&lt;/a&gt; is the organization to get in touch with; if you’re from my home country of Canada, the &lt;a href=&quot;https://blood.ca/en&quot;&gt;Canadian Blood Services&lt;/a&gt; manages the process.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/6/15/graphs-and-matrices/">
    <title type="text">Graphs and Matrices</title>
    <id>urn:uuid:e41b8fff-0a7a-350c-acb8-e445f4b2ad7c</id>
    <updated>2019-06-15T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/6/15/graphs-and-matrices/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;Once again, I’m reminded through my research how neat and useful it is to be able to think of matrices as graphs and vice-versa.&lt;/p&gt;
&lt;p&gt;I was constructing a symmetric square matrix of values, in which multiple cells of the matrix were empty (i.e. no values present). (Thankfully, the diagonal is guaranteed dense.) From this matrix, I wanted the largest set of rows/columns that formed a symmetric, densely populated square matrix of values, subject to a second constraint that the set of rows/columns also maximally intersected with another set of items.&lt;/p&gt;
&lt;p&gt;Having thought about the requirements of the problem, my prior experience with graphs reminded me that every graph has a corresponding adjacency matrix, and that finding the densest symmetric subset of entries in the matrix was equivalent to finding cliques in a graph! My intern and I proceeded to convert the matrix into its graph representation, and a few API calls in &lt;code&gt;networkx&lt;/code&gt; later, we found the matrix we needed.&lt;/p&gt;
&lt;p&gt;The key takeaway from this experience? Finding the right representation for a problem, we can computationally solve them quickly by using the appropriate APIs!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/6/14/mobile-working-on-the-ipad/">
    <title type="text">Mobile Working on the iPad</title>
    <id>urn:uuid:5fa1e5d1-cef0-3174-8a67-dfd804b2e17e</id>
    <updated>2019-06-14T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/6/14/mobile-working-on-the-ipad/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;A few years ago, I test-drove mobile work using my thesis as a case study, basically challenging myself with the question: how much of my main thesis paper could I write on iOS (specifically, an iPad Mini)? Back then, iOS turned out to be a superb tool for the writing phase (getting ideas into a text editor), and a horrible one for final formatting before submitting a paper to a journal (inflexible). Now, don’t get me wrong, though - I would still use it as part of my workflow if I were to do it again!&lt;/p&gt;
&lt;p&gt;Fast-forward a few years, I now do more programming in a data science context than I do formal writing, and the tooling for software development and data analysis on iOS has improved greatly. I thought I’d challenge myself with an experiment: how much of development and analytics could I do on an iPad, especially the Mini?&lt;/p&gt;
&lt;p&gt;This time round, armed with an iPad Pro (11”), I decided to test again how much one can do on iOS, once again.&lt;/p&gt;
&lt;h2 id=&quot;software-development&quot;&gt;Software Development&lt;/h2&gt;&lt;p&gt;I develop &lt;a href=&quot;https://pyjanitor.readthedocs.io&quot;&gt;&lt;code&gt;pyjanitor&lt;/code&gt;&lt;/a&gt; as a tool that I use in my daily work, and as part of my open source software portfolio. When I’m on my MacBook or Pro, or on my Linux desktop at home, I usually work with VSCode for it’s integrated terminal, superb syntax highlighting, git integration, code completion with Kite, and more.&lt;/p&gt;
&lt;p&gt;Moving to iOS, VSCode is not available, and that immediately means to rely on Terminal-based tools to get my work done. I ponied up for &lt;a href=&quot;https://www.blink.sh/&quot;&gt;Blink Shell&lt;/a&gt;, and found it to pay off immediately. Having enabled remote access on my Linux tower at home, I was thrilled to learn that Blink supports &lt;a href=&quot;https://mosh.org/&quot;&gt;&lt;code&gt;mosh&lt;/code&gt;&lt;/a&gt;, and when paired with &lt;a href=&quot;https://github.com/tmux/tmux&quot;&gt;&lt;code&gt;tmux&lt;/code&gt;&lt;/a&gt;, it is a superb solution for maintaining persistent shells across spotty internet conditions.&lt;/p&gt;
&lt;p&gt;A while ago, I also configured &lt;code&gt;nano&lt;/code&gt; with syntax highlighting. As things turned out, syntax highlighting has the biggest effect on my productivity compared to other text editor enhancements (e.g. code completion, git integration, etc.). After I mastered most of &lt;code&gt;nano&lt;/code&gt;'s shortcut keys, I found I could be productive at coding in just &lt;code&gt;nano&lt;/code&gt; itself. Even though missing out on the usual assistive tools meant I was coding somewhat slower, the pace was still acceptable; moreover, relying less on those tools helped me develop a muscle memory for certain API calls. I also found myself becoming more effective because the single window idioms of iOS meant I was focusing on the programming task at hand, rather than getting distracted while looking at docs in a web browser (a surprisingly common happening for me!).&lt;/p&gt;
&lt;h2 id=&quot;data-analysis&quot;&gt;Data Analysis&lt;/h2&gt;&lt;p&gt;For data analysis, Jupyter notebooks are the tool of my choice, for their interactive nature, and the ability to weave a narrative throughout the computation. Jupyter Lab is awesome for this task, but it’s poorly supported on mobile Safari. To use Jupyter notebooks in iOS, the best offering at the moment is &lt;a href=&quot;https://juno.sh&quot;&gt;Juno&lt;/a&gt;, with its ability to connect to a Jupyter server accessible through an IP address or URL. This does require payment, though, and I gladly ponied up for that as well.&lt;/p&gt;
&lt;p&gt;I run a Jupyter server on my Linux tower at home. Because it has a GPU installed on it, when I am accessing the machine through Juno, my iPad suddenly has access to a full-fledged, fully-configured GPU as part of the compute environment! Coupled with the responsiveness of Juno, this makes for a fairly compelling setup to do Python programming on an iPad.&lt;/p&gt;
&lt;h2 id=&quot;pros-and-cons-of-ipad-based-development&quot;&gt;Pros and Cons of iPad-based Development&lt;/h2&gt;&lt;h3 id=&quot;cons&quot;&gt;Cons&lt;/h3&gt;&lt;p&gt;Overall, the experience has been positive, but there have been some challenges, which I would like to detail here.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Remote server required:&lt;/strong&gt; Firstly, because we are essentially using the iPad as a thin client to a remote server, one must either pay for a remote development server in the cloud, or go through the hassle of setting up a development machine that one can SSH into. This may turn off individuals who either are loathe to rent a computer, or don’t have the necessary experience to setup a remote server on their own.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;iOS Multi-Windowing:&lt;/strong&gt; It’s sometimes non-trivial to check up source code or function signatures (API docs, really) on the sidebar browser window in iOS. Unlike macOS, in which I have a number of shortcut keys that will let me launch and/or switch between apps, the lack of this capability on iOS means I find myself slowed down because I have to use a bunch of swiping gestures to get to where I need to be. (&lt;code&gt;Cmd-tab&lt;/code&gt; seems to be the only exception, for it activates the app switcher, but the number of apps in the app switcher remembers is limited.)&lt;/p&gt;
&lt;h3 id=&quot;pros&quot;&gt;Pros&lt;/h3&gt;&lt;p&gt;Even with the issues detailed above, there’s still much to love about doing mobile development work on an iOS device.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;iOS Speed:&lt;/strong&gt; On the latest hardware, iOS is speedy. Well, even that is a bit of an understatement. I rarely get lags while typing in Blink and Juno, and even when I do, I can usually pin it down to network latency more than RAM issues.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Focus:&lt;/strong&gt; This is the biggest win. Because iOS makes it difficult to switch contexts, this is actually an upside for work that involves creating things. Whether it’s someone who is drawing, producing videos, editing photos, writing blog posts, or creating code, the ability to focus on the context at hand is tremendous for producing high quality work. Here, iOS is actually a winner for focused productivity.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Mobility:&lt;/strong&gt; The second upside would be that of battery life, and hence mobility, by extension. My 12” MacBook is super mobile, yes, but macOS appears to have issues restraining battery drainage when the lid is closed. By contrast, iOS seems to have fewer issues with this. The battery life concerns mean I’m carrying my mouse, charger and dongle with me all the time, and I’ll get the equivalent of range anxiety when I take only my laptop.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Keyboard Experience:&lt;/strong&gt; The keyboard experience on the Smart Keyboard Folio is surprisingly good! It’s tactile, and is fully covered, so we won’t have issues arise due to dust getting underneath the keys, like my little MacBook had.&lt;/p&gt;
&lt;h2 id=&quot;concluding-thoughts&quot;&gt;Concluding Thoughts&lt;/h2&gt;&lt;p&gt;This test has been quite instructive. As usual, tooling is superbly important for productivity; investing in the right tools makes it worthwhile. Granted, none of this comes cheap or for free, naturally. Given the future directions of iOS, I think it’s shaping up to be a real contender for productivity!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/5/29/reasoning-about-shapes-and-probability-distributions/">
    <title type="text">Reasoning about Shapes and Probability Distributions</title>
    <id>urn:uuid:7d251bbe-af45-3bb0-8b08-346e8011341e</id>
    <updated>2019-05-29T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/5/29/reasoning-about-shapes-and-probability-distributions/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;I’m here with the PyMC4 dev team and Tensorflow Probability developers Rif, Brian and Chris in Google Montreal, and have found the time thus far to be an amazing learning opportunity.&lt;/p&gt;
&lt;p&gt;Prior to this summit, it never dawned on me how interfacing tensors with probability distributions could be such a minefield of overloaded ideas and terminology. Yet, communicating clearly about tensors is important, because if problems can be cast into a tensor-space operation, vectorization can help speed up many operations that we wish to handle. I wanted to share a bit about something new about tensors that I learned here: the different types of shapes involved in a probabilistic programming language.&lt;/p&gt;
&lt;p&gt;Let’s start by thinking about a few questions involving the most venerable distribution of them all: the Gaussian, also known as the Normal distribution.&lt;/p&gt;
&lt;p&gt;Let’s start by thinking about a single draw from a standard Gaussian. Drawing one number from the standard Gaussian yields a scalar. In tensor space, a scalar is a rank 0 tensor, and this colloquially means that there’s no dimensions involved. If we drew out the distribution, and drew out the process of drawing numbers from the distribution, it might look like the following:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;../../../../../blog/2019/5/29/reasoning-about-shapes-and-probability-distributions/one-draw-one-normal.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The distribution we are drawing from is on the left, and a draw is represented by a line (on the same numerical axis as the probability distribution), and the &lt;/em&gt;event shape&lt;em&gt;, &lt;/em&gt;batch shape&lt;em&gt; and &lt;/em&gt;sample shape&lt;em&gt; shown to their right, followed by a &quot;plain English&quot; description. Over the course of this blog post, the shape concepts will be disambiguated; sit tight and enjoy the ride!&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;What if we were to draw two numbers from this one Gaussian? We could use a vector with two slots to represent those draws. This might look like the following:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;../../../../../blog/2019/5/29/reasoning-about-shapes-and-probability-distributions/two-draws-one-normal.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;However, the elementary &lt;em&gt;event&lt;/em&gt; of drawing a single number did not fundamentally change when we drew two numbers, as we merely repeated the same &lt;em&gt;event&lt;/em&gt; to draw two. With my hands waving in the air, I will claim that this holds true even with K &lt;em&gt;samples&lt;/em&gt; drawn from the distribution.&lt;/p&gt;
&lt;p&gt;Now, what if I had a second Gaussian, say, with a different mean and/or variance? If I were to draw one number from the first Gaussian alongside one number from the second Gaussian, and then concatenate them into a vector, we can represent this as us drawing numbers from independent Gaussians. The illustration below should help clarify how this is different from the first.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;../../../../../blog/2019/5/29/reasoning-about-shapes-and-probability-distributions/one-draw-two-normals.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;In this case, we may argue that per distribution, the elementary shape of the &lt;em&gt;event&lt;/em&gt; did not change. However, since we have a &lt;em&gt;batch&lt;/em&gt; of two distributions, this contributes to the final shape of the tensor. Again, with much waving of my hands in the air, this should extend to more than two distributions.&lt;/p&gt;
&lt;p&gt;Now, what if we had a multivariate Gaussian, with two variates? This makes for a very interesting case! The elementary &lt;em&gt;event&lt;/em&gt; drawn from this multivariate Gaussian is a two-element vector, not a scalar, which means that its shape is apparently identical to the case where we have a single pair of numbers drawn from a batch of two independent Gaussians! This looks like the following:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;../../../../../blog/2019/5/29/reasoning-about-shapes-and-probability-distributions/one-draw-bivariate-normal.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;This is interesting, because a single draw from a bivariate Gaussian has the same overall shape as two draws from one Gaussian, which also has the same shape as one draw from a batch of two Gaussians. Yet, these apparently same-shaped draws are shaped differently semantically! In particular, the two independent Gaussians individually have elementary &lt;em&gt;event&lt;/em&gt; shapes that are scalar, but when drawn as a &lt;em&gt;batch&lt;/em&gt; of two, that is when their shape of &lt;code&gt;(2,)&lt;/code&gt; forms. On the other hand, the multivariate Gaussian cannot have its two numbers drawn independent of one another (unless this is the special case of diagonal-only covariance - in which case, this is equivalent to independent Gaussians). Hence, the elementary &lt;em&gt;event&lt;/em&gt; shape is not scalar, but vector (or more generally, same rank tensor as the mean tensor), but the &lt;em&gt;batch&lt;/em&gt; has only a single distribution, hence it has a scalar batch shape.&lt;/p&gt;
&lt;p&gt;To summarize, here are the various kinds of shapes, defined:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Event shape:&lt;/strong&gt; The atomic shape of a single event/observation from the distribution (or batch of distributions of the same family).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Batch shape:&lt;/strong&gt; The atomic shape of a single sample of observations from one or more distributions &lt;em&gt;of the same family&lt;/em&gt;. As an example, we can’t have a batch of a Gaussian and a Gamma distribution together, but we can have a batch of more than one Gaussians.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sample shape:&lt;/strong&gt; The shape of a bunch of samples drawn from the distributions.&lt;/p&gt;
&lt;p&gt;And finally, here’s the full spread of possibilities, using one or two draws, uni- or bi-variate Gaussians, and one or two batches of distributions as an illustration.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;../../../../../blog/2019/5/29/reasoning-about-shapes-and-probability-distributions/shapes-blog-post-md.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;Special thanks goes to fellow PyMC devs, Ravin Kumar, Brandon Willard, Colin Carroll, and Peadar Coyle, who provided feedback on the figure over a late-night tea/dinner/bar session at the end of Day 2.&lt;/p&gt;
&lt;h2 id=&quot;why-shapes-matter:-broadcasting&quot;&gt;Why Shapes Matter: Broadcasting&lt;/h2&gt;&lt;p&gt;Why do these different shapes matter? Well, it matters most when we are thinking about broadcasting in a semantically-consistent fashion, particularly when considering batches and events. When it comes to implementing a tensor library with probability distributions as first-class citizens, reasoning about these shapes properly can really help with implementing an API that end-users can grok in a reasonable fashion.&lt;/p&gt;
&lt;p&gt;Let’s return to the simple case where we have two different types of shape &lt;code&gt;(2,)&lt;/code&gt; Gaussians declared: a batch of two Gaussians, and a bivariate Gaussian. One useful thought experiment is to think about the computation of the log probability of a vector of two numbers, &lt;code&gt;X = (x1, x2)&lt;/code&gt;, where &lt;code&gt;x1&lt;/code&gt; and &lt;code&gt;x2&lt;/code&gt; are not necessarily the same numbers.&lt;/p&gt;
&lt;p&gt;In the case of the bivariate Gaussians, how many log probabilities should we return? In this case, it makes semantic sense to return only one number, because in a bivariate Gaussian, the two numbers could not have been drawn independent of each other, and hence the log probability has to be computed with consideration to the full joint distribution.&lt;/p&gt;
&lt;p&gt;In the case of the batch of two Gaussians, how many log probabilities should we return? Is it one number, or is it two? Semantically, it makes sense to return two numbers, because we are evaluating &lt;code&gt;x1&lt;/code&gt; against the first Gaussian, and &lt;code&gt;x2&lt;/code&gt; against the second Gaussian in the batch of Gaussians. Most crucially, this differs from the bivariate case, because by structuring our Gaussians in a batch, we are essentially declaring our intent to evaluate their log probabilities independent of each other.&lt;/p&gt;
&lt;p&gt;Borrowing from the above master figure, here's the a figure that shows how likelihood computations happen:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;../../../../../blog/2019/5/29/reasoning-about-shapes-and-probability-distributions/likelihood-computation.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;To compute the likelihood over the data, we multiply the likelihoods of each of the individual data points (or since we're doing computation on a computer, we sum the log probabilities). You will noticed that essentially, in each case, the dimension we intend to collapse is the &lt;code&gt;sample&lt;/code&gt; dimension - and that means keeping track of the sample dimension is extremely important! Also important to note is that we do &lt;/em&gt;not&lt;em&gt; intend to collapse the &lt;code&gt;batch&lt;/code&gt; dimension, as it does not carry the same statistical meaning as a &lt;code&gt;sample&lt;/code&gt; from a distribution, but is a tensor computation construct.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&quot;other-scenarios&quot;&gt;Other Scenarios&lt;/h2&gt;&lt;p&gt;There are more scenarios where reasoning about shapes in a &lt;em&gt;semantic&lt;/em&gt; manner becomes super important! Here’s a sampling of them, posed as questions and then maybe some suggested answers or further questions.&lt;/p&gt;
&lt;p&gt;If I now asked to evaluate the log probability of &lt;code&gt;x1&lt;/code&gt; only, how should broadcasting of &lt;code&gt;x1&lt;/code&gt; happen on the bivariate Gaussian, and on the batch of two Gaussians? Perhaps in this trivial case, it would be tempting to automatically broadcast the same scalar number... but wait! In the case of the bivariate Gaussian, how do we know that the end-user has not forgotten to supply the second number?&lt;/p&gt;
&lt;p&gt;If I have a batch of two bivariate Gaussians, hence effectively creating a (batch shape = 2, event shape = 2) vector of bivariate distributions, and I ask to evaluate the log probability of a matrix of values &lt;code&gt;((x1, x2), (x3, x4))&lt;/code&gt;, in which way do we orient the values? Do we assume that &lt;code&gt;(x1, x2)&lt;/code&gt; are to be evaluated against the first Gaussian, or &lt;code&gt;(x1, x3)&lt;/code&gt; are to be evaluated against the first Gaussian? (We don’t have to worry about &lt;code&gt;(x1, x4)&lt;/code&gt;, because to the average user, it is unreasonable whichever way we look.)&lt;/p&gt;
&lt;p&gt;Both these examples illustrate an inherent difficulty to thinking about tensor shapes without reference to what each of the dimensions mean.&lt;/p&gt;
&lt;h2 id=&quot;improving-shape-semantics&quot;&gt;Improving Shape Semantics&lt;/h2&gt;&lt;p&gt;What could we do, then, to improve the semantic understandability of tensor shapes?&lt;/p&gt;
&lt;p&gt;One solution would be to name tensor axes by what they mean. The &lt;code&gt;xarray&lt;/code&gt; project does exactly that! However, it can’t be used in differential computing, because (to the best of my knowledge), there is no automatic differentiation system that works with it.&lt;/p&gt;
&lt;p&gt;In addition, there’s &lt;code&gt;namedtensor&lt;/code&gt; from Harvard NLP that aims to provide an analogous solution to the problem, though I find it a pity that they chose to implement it against &lt;code&gt;pytorch&lt;/code&gt; rather than to create an extension to the idiomatic &lt;code&gt;numpy&lt;/code&gt; instead.&lt;/p&gt;
&lt;p&gt;The TensorFlow Probability team also has a solution, in which they separate the three types of shapes explicitly, though no naming happens on a per-axis basis.&lt;/p&gt;
&lt;p&gt;I think there are great ideas in all three, and when I take a birds-eye view of the scientific computing ecosystem in Python as both a developer and end-user, I’d love to see the NumPy API, which is idiomatic and widely used and built on top of, become aware of each of these types of designs, something akin to NEP-18, the array function dispatching protocol that allows NumPy APIs to be called on other tensor libraries.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/5/11/pycon-2019-sprints/">
    <title type="text">PyCon 2019 Sprints</title>
    <id>urn:uuid:a42b3f69-a28c-3ea5-945b-64486982c78e</id>
    <updated>2019-05-11T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/5/11/pycon-2019-sprints/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;This year was the first year that I decided to lead a sprint! The sprint I led was for &lt;code&gt;pyjanitor&lt;/code&gt;, a package that I developed with my colleague, Zach Barry, and a remote collaborator in NYC, Sam Zuckerman (whom I've never met in person!). This being the first sprint I've ever led, I think I was lucky to stumble upon a few ideas that made for a productive, collaborative, and most importantly, fun sprint.&lt;/p&gt;
&lt;h2 id=&quot;pyjanitor?&quot;&gt;pyjanitor?&lt;/h2&gt;&lt;p&gt;I'm going to deliver a talk on &lt;code&gt;pyjanitor&lt;/code&gt; later in the year, so I'll save the details for that talk. The short description of &lt;code&gt;pyjanitor&lt;/code&gt; is that if you have &lt;code&gt;pandas&lt;/code&gt; one-liners that are difficult to remember, they should become a function in &lt;code&gt;pyjanitor&lt;/code&gt;; if you have a 10-liner that you always copy/paste from another source, they should become a function in &lt;code&gt;pyjanitor&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&quot;sprints?&quot;&gt;Sprints?&lt;/h2&gt;&lt;p&gt;Code sprints are a part of PyCon, and it's basically one to four days of intense and focused software development on a single project.&lt;/p&gt;
&lt;p&gt;Project sprint leads first pitch their projects at the &quot;&lt;em&gt;Sprintros&lt;/em&gt;&quot;, where they indicate what days they will be sprinting, and at what times. The next day, we indicate on which rooms our projects will be sprinting. Volunteers from the conference, who would like to make an open source project contribution, then identify projects they would like to come sprint with.&lt;/p&gt;
&lt;p&gt;In some senses, there's no way for a sprint leader to know how popular their sprint will be &lt;em&gt;a priori&lt;/em&gt;. We have to be prepared to handle a range of scenarios from sprinting with just one other person to sprinting with a crowd.&lt;/p&gt;
&lt;h2 id=&quot;structure&quot;&gt;Structure&lt;/h2&gt;&lt;h3 id=&quot;preparation&quot;&gt;Preparation&lt;/h3&gt;&lt;p&gt;In preparation for the sprint, I absorbed many lessons learned over the years of sprinting on others' projects.&lt;/p&gt;
&lt;p&gt;The most obvious one was to ensure that every sprinter had something to do right from the get-go. Having a task from the get-go keeps sprinters, especially newcomers, engaged right from the beginning. This motivated the requirement to make a doc fix before making a code fix. (You can read more below on how we made this happen.) I wrote out this requirement in a number of places, and by the time the sprint day rolled by, this rolled off pretty naturally.&lt;/p&gt;
&lt;p&gt;The second thing that I did to prep was to triage existing issues and label them as being beginner- or intermediate-friendly, and whether they were doc, infrastructure, or code contributions.&lt;/p&gt;
&lt;p&gt;Those two things were the my highest priority preparation for the sprint, and I think that helped a ton.&lt;/p&gt;
&lt;h3 id=&quot;doc-fixes&quot;&gt;Doc Fixes&lt;/h3&gt;&lt;p&gt;This sprint, I gave the structure some thought, and settled on the following: Before making a code contribution, I required a docs contribution.&lt;/p&gt;
&lt;p&gt;Docs contributions could be of any scale:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A typographical, grammatical, or spelling error.&lt;/li&gt;
&lt;li&gt;A docstring that was unclear.&lt;/li&gt;
&lt;li&gt;Installation/setup instructions that are unclear.&lt;/li&gt;
&lt;li&gt;A sentence/phrase/word choice that didn't make sense.&lt;/li&gt;
&lt;li&gt;New example/tutorial notebooks using the library.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I think this worked well for the following reasons:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;New contributors must read the docs before developing on the project, and hence become familiar with the project.&lt;/li&gt;
&lt;li&gt;There's always something that can be done better in the docs, and hence, there is something that can be immediately acted on.&lt;/li&gt;
&lt;li&gt;The task is a pain point personally uncovered by the contributor, and hence the contributor has the full context of the problem. &lt;/li&gt;
&lt;li&gt;The docs don't break the code/tests, and hence doc contributions are a great way make a contribution without wrestling with more complex testing.&lt;/li&gt;
&lt;li&gt;Starting &lt;em&gt;everybody&lt;/em&gt; who has never worked on &lt;code&gt;pyjanitor&lt;/code&gt; on docs is an egalitarian way of on-boarding every newcomer, beginner and experienced individuals alike. Nobody gets special treatment.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For each individual's contribution, I asked them to first raise an issue on the GitHub issue tracker describing the contribution that they would like to make, and then clearly indicate in the comments that they would like to work on it. Then, they would go through the process of doing the documentation fix, from forking the repository, cloning it locally, creating a new branch, making edits, committing, pushing, and PR-ing.&lt;/p&gt;
&lt;p&gt;If two people accidentally ended up working on the same docs issue, I would assess the effort of the later one, and if it was substantial enough, I would allow them to consider it done, and move onto a different issue.&lt;/p&gt;
&lt;p&gt;Going forth, as the group of contributors expands, I will enforce this &quot;docs-first&quot; requirement only for newcomer sprinters, and request experienced ones to help manage the process.&lt;/p&gt;
&lt;h3 id=&quot;code-contributions&quot;&gt;Code Contributions&lt;/h3&gt;&lt;p&gt;Once the docs contributions were done, sprinters were free to either continue with more docs contributions, or provide a code contribution.&lt;/p&gt;
&lt;p&gt;Code contributions could be of one of the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;New function contributions.&lt;/li&gt;
&lt;li&gt;Cleaner implementations of existing functions.&lt;/li&gt;
&lt;li&gt;Restructuring of existing functions.&lt;/li&gt;
&lt;li&gt;Identification of functions to deprecate (very important!)&lt;/li&gt;
&lt;li&gt;Infrastructural changes to docs, build system, and more.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The process for doing this was identical to docs: raise an issue, claim it, and then make a new branch with edits, and finally PR it.&lt;/p&gt;
&lt;h2 id=&quot;my-role&quot;&gt;My Role&lt;/h2&gt;&lt;p&gt;For both days, we had more than 10 people sprint on &lt;code&gt;pyjanitor&lt;/code&gt;. Many who were present on the 1st day (and didn't have a flight to catch) came back on the 2nd day. As such, I actually didn't get much coding done. Instead, I took on the following roles:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Q&amp;amp;A person: Answering questions about what would be acceptable contributions, technical (read: git) issues, and more.&lt;/li&gt;
&lt;li&gt;Issue labeller and triage-r: I spent the bulk of my downtime (and pre-/post-sprint time) tagging issues on the GitHub issue tracker and marking them as being available or unavailable for hacking on, and tagging them with whether they were docs-related, infrastructure-related, or code enhancements. &lt;/li&gt;
&lt;li&gt;Code reviewer: As PRs came in, I would conduct code reviews on each of them, and would discuss with them where to adjust the code to adhere to code style standards. &lt;/li&gt;
&lt;li&gt;Continuous integration pipeline babysitter: Because I had just switched us off from Travis CI to Azure Pipelines, I was babysitting the pipelines to make sure nothing went wrong. (Spoiler: something did!) &lt;/li&gt;
&lt;li&gt;Green button pusher: Once all tests passed, I would hit the big green button to merge PRs!&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If I get to sprint with other experienced contributors at the sprints, I would definitely like to have some help with the above.&lt;/p&gt;
&lt;h2 id=&quot;thoughts&quot;&gt;Thoughts&lt;/h2&gt;&lt;h3 id=&quot;making-sprints-human-friendly&quot;&gt;Making sprints human-friendly&lt;/h3&gt;&lt;p&gt;I tried out a few ideas, which I hope made the sprints just that little bit more human-friendly.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Used large Post-It easel pads to write out commonly-used commands at the terminal.&lt;/li&gt;
&lt;li&gt;Displayed claimed seating arrangements at the morning, and more importantly, get to know every sprinter's name.&lt;/li&gt;
&lt;li&gt;Announcing every PR to the group that was merged and what the content was, followed by a round of applause.&lt;/li&gt;
&lt;li&gt;Setting a timer for 5 minutes before lunch so that they could all get ahead in the line.&lt;/li&gt;
&lt;li&gt;I used staff privileges to move the box of leftover bananas into our sprint room. :)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I think the applause is the most encouraging part of the process. Having struggled through a PR, however big or small, and having group recognition for that effort, is super encouraging, especially for first-time contributors. I think we need to encourage this more at sprints.&lt;/p&gt;
&lt;h3 id=&quot;relinquishing-control&quot;&gt;Relinquishing control&lt;/h3&gt;&lt;p&gt;The only things about the pyjanitor project that I'm unwilling to give up on are: good documentation of what a function does, that a function should do one thing well, and that it be method-chainable. Everything else, including functionality, is an open discussion that we can have!&lt;/p&gt;
&lt;p&gt;One PR I particularly enjoyed was that from Lucas, who PR'd in a logo for the project on the docs page. He had the idea to take the hacky broomstick I drew on the big sticky note (as a makeshift logo), redraw it as a vector graphic in Inkscape, and PR it in as the (current) official logo on the docs.&lt;/p&gt;
&lt;p&gt;More broadly, I deferred to the sprinters' opinions on docs, because I recognized that I'd have old eyes on the docs, and wouldn't be able to easily identify places where the docs could be written more clearly. Eventually, a small, self-organizing squad of 3-5 sprinters ended up becoming the unofficial docs squad, rearranging the structure of the docs, building automation around it, and better organizing and summarizing the information on the docs.&lt;/p&gt;
&lt;p&gt;In more than a few places, if there were a well-justified choice for the API (which really meant naming the functions and keyword arguments), I'd be more than happy to see the PR happen. Even if it is evolved away later, the present codebase and PRs that led to it provided the substrate for better evolution of the API!&lt;/p&gt;
&lt;h3 id=&quot;a-new-microsoft&quot;&gt;A new Microsoft&lt;/h3&gt;&lt;p&gt;This year, I switched from Travis CI to Azure Pipelines. In particular, I was attracted to the ability to build on all three major operating systems, Windows, macOS, and Linux, on the free tier.&lt;/p&gt;
&lt;p&gt;Microsoft had a booth at PyCon, in which Steve Dowell led an initiative to get us set up with Azure-related tools. Indeed, as a major sponsor of the conference, this was one of the best swag given to us. Super practical, relationship- and goodwill-building. Definitely not lesser than the Adafruit lunchboxes with electronics as swag!&lt;/p&gt;
&lt;h3 id=&quot;hiccups&quot;&gt;Hiccups&lt;/h3&gt;&lt;p&gt;Naturally, not everything was smooth sailing throughout. I did find myself a tad expressing myself in an irate fashion at times with the amount of context switching that I was doing, especially switching between talking to different sprinters one after another. (I am very used to long stretches hacking on one problem.) One thing future sprinters could help with, which I will document, is to give me enough ramp-up context around their problem, so that I can quickly pinpoint what other information I might need.&lt;/p&gt;
&lt;p&gt;The other not-so-smooth-sailing thing was finding out that Azure sometimes did not catch errors in a script block! My unproven hypothesis at this point is that if I have four commands executed in a script block, and if any of the first three fail but the last one passes, the entire script block will behave as if it passes. This probably stems from the build system looking at only the last exit code to determine exit status. Eventually, after splitting each check into individual steps, linting and testing errors started getting caught automatically! (Automating this is much preferred to me running the &lt;code&gt;black&lt;/code&gt; code formatter in my head.)&lt;/p&gt;
&lt;p&gt;Though the above issue is fixed, I think I am still having issues getting &lt;code&gt;pycodestyle&lt;/code&gt; and &lt;code&gt;black&lt;/code&gt; to work on the Windows builds. Definitely looking forward to hearing from Azure devs what could be done here!&lt;/p&gt;
&lt;h2 id=&quot;suggestions&quot;&gt;Suggestions&lt;/h2&gt;&lt;p&gt;I'm sure there's ways I could have made the sprint a bit better.  I'd love to hear them if there's something I've missed! Please feel free to comment below.&lt;/p&gt;
&lt;h2 id=&quot;sprinter-acknowledgement&quot;&gt;Sprinter Acknowledgement&lt;/h2&gt;&lt;p&gt;I would like to thank all the sprinters who joined in this sprint. Their GitHub handles are below:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;@HectorM14 (who was remote!)&lt;/li&gt;
&lt;li&gt;@jekwatt&lt;/li&gt;
&lt;li&gt;@kurtispinkney&lt;/li&gt;
&lt;li&gt;@lphk92&lt;/li&gt;
&lt;li&gt;@jonnybazookatone&lt;/li&gt;
&lt;li&gt;@SorenFrohlich&lt;/li&gt;
&lt;li&gt;@dave-frazzetto&lt;/li&gt;
&lt;li&gt;@dsouzadaniel&lt;/li&gt;
&lt;li&gt;@Eidhagen&lt;/li&gt;
&lt;li&gt;@mdini&lt;/li&gt;
&lt;li&gt;@kimt33&lt;/li&gt;
&lt;li&gt;@jack-kessler-88&lt;/li&gt;
&lt;li&gt;@NapsterInBlue&lt;/li&gt;
&lt;li&gt;@jk3587&lt;/li&gt;
&lt;li&gt;@ricky-lim&lt;/li&gt;
&lt;li&gt;@catherinedevlin&lt;/li&gt;
&lt;li&gt;@StephenSchroed&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And as always, big thanks to my collaborators on the repository:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;@zbarry&lt;/li&gt;
&lt;li&gt;@szuckerman&lt;/li&gt;
&lt;/ul&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/5/10/pycon-2019-tutorial-and-conference-days/">
    <title type="text">PyCon 2019 Tutorial and Conference Days</title>
    <id>urn:uuid:e540031b-e5df-3893-bf8f-bd692ef9b95b</id>
    <updated>2019-05-10T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/5/10/pycon-2019-tutorial-and-conference-days/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;It's just been days since I got back from PyCon, and I'm already looking forward to 2020! But I thought it'd be nice to continue the recap.&lt;/p&gt;
&lt;p&gt;This year at PyCon, I co-led two tutorials, one with Hugo Bowne-Anderson on &lt;a href=&quot;https://us.pycon.org/2019/schedule/presentation/77/&quot;&gt;Bayesian Data Science by Simulation&lt;/a&gt;, and the other with Mridul Seth on &lt;a href=&quot;https://us.pycon.org/2019/schedule/presentation/70/&quot;&gt;Network Analysis Made Simple&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I always enjoy teaching with &lt;a href=&quot;../../../../../blog/2019/5/10/pycon-2019-tutorial-and-conference-days/#&quot;&gt;Hugo&lt;/a&gt;. He brought his giant sense of humour, both figuratively and literally, to this tutorial, and melded it with his deep grasp of the math behind Bayesian statistics, delivering a workshop that, by many points of feedback, is excellent. Having reviewed the tutorial feedback, we've got many ideas for our showcase of Part II at SciPy 2019!&lt;/p&gt;
&lt;p&gt;This year was the first year that Mridul and I swapped roles. In previous years, he was my TA, helping tutorial participants while I did the main lecturing. This year, the apprentice became the master, and a really good one indeed! Looking forward to seeing him shine more in subsequent tutorial iterations.&lt;/p&gt;
&lt;p&gt;During the conference days, I spent most of my time either helping out with Financial Aid, or at the Microsoft booth. As I have alluded to in multiple tweets, Microsoft's swag this year was the best of them all. Microelectronics kits in a blue lunch box from Adafruit, and getting set up with &lt;a href=&quot;https://azure.microsoft.com/en-us/&quot;&gt;Azure&lt;/a&gt;. In fact, this website is now re-built with each push on Azure pipelines! Indeed, &lt;a href=&quot;https://twitter.com/zooba&quot;&gt;Steve Dowell&lt;/a&gt; from Microsoft told me that this year's best swag was probably getting setup with Azure, and I'm 100% onboard with that! (Fun fact, Steve told me that he's never been called by his Twitter handle (&lt;code&gt;zooba&lt;/code&gt;) in real-life... until we met.)&lt;/p&gt;
&lt;p&gt;I also delivered a talk this year, which essentially amounted to a &lt;a href=&quot;https://us.pycon.org/2019/schedule/presentation/174/&quot;&gt;formal rant against canned statistical procedures&lt;/a&gt;. I had a ton of fun delivering this talk. The usual nerves still get to me, and I had to do a lot of talking-to-myself-style rehearsals to work off those nerves. For the first time, I also did office hours post-talk at an Open Space, where for one hour, we talked about all things Bayes. Happy to have met everybody who came by; I genuinely enjoyed the chat!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/5/10/context-switching/">
    <title type="text">Context Switching</title>
    <id>urn:uuid:b323afb0-df8c-3aa2-afe8-379eff18252f</id>
    <updated>2019-05-10T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/5/10/context-switching/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;Context switching is hard. I noticed this when I was at the PyCon sprints, where I was bouncing from sprinter to sprinter, trying to give them each the necessary attention to get their chosen sprint tasks done. After a while, it took a toll on my brain, and I started finding it hard to concentrate on the next problem.&lt;/p&gt;
&lt;p&gt;Under such circumstances, when one is context switching often (most hopefully out of one's own volition), how do we communicate that we need some ramp-up time, and how can others help us help them?&lt;/p&gt;
&lt;p&gt;I think one practical thing that can be done is to frequently communicate on each context switch that context ramp-up is needed. In the future, when I switch contexts, first thing I'm going to ask is something along the lines of, &quot;What context do I need to help me help you?&quot; Or, if I'm lost, I can clearly communicate what I'm missing - if it's context that I'm missing - by stating, &quot;I think I'm missing some context. Can you bring me up to speed?&quot;&lt;/p&gt;
&lt;p&gt;At least while sprinting, sprinters can definitely help me help them by providing the necessary context up-front. Perhaps this applies more generally as well: when we're asking someone for help, we may be able to help them out by asking them, &quot;What context from me would help you get up-to-speed here?&quot;&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/4/29/pycon-2019-pre-journey/">
    <title type="text">PyCon 2019 Pre-Journey</title>
    <id>urn:uuid:ccbe0426-b1a5-3655-ab5f-4005fb02089d</id>
    <updated>2019-04-29T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/4/29/pycon-2019-pre-journey/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;I'm headed out to &lt;a href=&quot;https://us.pycon.org/2019&quot;&gt;PyCon 2019&lt;/a&gt;! This year, I will be co-instructing two tutorials, one on network analysis and one on Bayesian statistics, and delivering one talk on Bayesian statistics.&lt;/p&gt;
&lt;p&gt;The first tutorial on &lt;a href=&quot;https://us.pycon.org/2019/schedule/presentation/70/&quot;&gt;network analysis&lt;/a&gt; is based on &lt;a href=&quot;https://github.com/ericmjl/Network-Analysis-Made-Simple&quot;&gt;material that I first developed&lt;/a&gt; 5 years ago, and have continually updated. I've enjoyed teaching this tutorial because it represents a different way of thinking about data - in other words, relationally. This year, I will be a co-instructor for &lt;a href=&quot;https://twitter.com/Mridul_Seth&quot;&gt;Mridul&lt;/a&gt;, who has kindly agreed to step up and teach it this year at PyCon. The apprentice has exceeded the master!&lt;/p&gt;
&lt;p&gt;The second tutorial on &lt;a href=&quot;https://us.pycon.org/2019/schedule/presentation/77/&quot;&gt;Bayesian statistics&lt;/a&gt; is based on &lt;a href=&quot;https://github.com/ericmjl/bayesian-stats-modelling-tutorial&quot;&gt;material co-developed&lt;/a&gt; with &lt;a href=&quot;https://twitter.com/hugobowne&quot;&gt;Hugo Bowne-Anderson&lt;/a&gt;. Hugo is a mathematician by training, a pedagogy master, and data science aficionado. Like myself, he is a fan of Bayesian statistical modelling methods, and we first debuted the tutorial past year at SciPy. We're super excited for this one!&lt;/p&gt;
&lt;p&gt;The talk that I will deliver is on &lt;a href=&quot;https://us.pycon.org/2019/schedule/presentation/174/&quot;&gt;Bayesian statistical analysis of case/control tests&lt;/a&gt;. In particular, I noticed a content gap in the data science talks, where case/control comparisons were limited to one case and one control. One epiphany I came to was that if we use Bayesian methods to analyze our data, there's no particular reason to limit ourselves to one case and one control; we can flexibly model multiple cases vs. one control, or even multiple cases vs multiple different controls in the same analysis, in a fashion that is flexible and principled.&lt;/p&gt;
&lt;p&gt;My final involvement with PyCon this year is as Financial Aid Chair. This is the first year that I'm leading the FinAid effort; during previous years, I had learned a ton from the previous chair Karan Goel. My co-chairs this year are Denise Williams and Jigyasa Grover; I'm looking forward to meeting them in 3D!&lt;/p&gt;
&lt;p&gt;All-in-all, I'm looking forward to another fun year at PyCon!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/3/24/variance-explained/">
    <title type="text">Variance Explained</title>
    <id>urn:uuid:0732917e-8efd-3932-8fad-e75d4d69dd52</id>
    <updated>2019-03-24T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/3/24/variance-explained/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;Variance explained, as a regression quality metric, is one that I have begun to like a lot, especially when used in place of a metric like the correlation coefficient (r&lt;sup&gt;2&lt;/sup&gt;).&lt;/p&gt;
&lt;p&gt;Here's variance explained defined:&lt;/p&gt;
&lt;p&gt;
$$1 - \frac{var(y_{true} - y_{pred})}{var(y_{true})}$$
&lt;/p&gt;&lt;p&gt;Why do I like it? It’s because this metric gives us a measure of the scale of the error in predictions relative to the scale of the data.&lt;/p&gt;
&lt;p&gt;The numerator in the fraction calculates the variance in the errors, in other words, the &lt;em&gt;scale of the errors&lt;/em&gt;. The denominator in the fraction calculates the variance in the data, in other words, the &lt;em&gt;scale of the data&lt;/em&gt;. By subtracting the fraction from 1, we get a number upper-bounded at 1 (best case), and unbounded towards negative infinity.&lt;/p&gt;
&lt;p&gt;Here's a few interesting scenarios.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If the scale of the errors is small relative to the scale of the data, then variance explained will be close to 1.&lt;/li&gt;
&lt;li&gt;If the scale of the errors is about the same scale as the data, then the variance explained will be around 0. This essentially says our model is garbage.&lt;/li&gt;
&lt;li&gt;If the scale of the errors is greater than the scale of the data, then the variance explained will be negative. This is also an indication of a garbage model.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A thing that is &lt;em&gt;really nice&lt;/em&gt; about variance explained is that it can be used to compare related machine learning tasks that have different unit scales, for which we want to compare how good one model performs across all of the tasks. Mean squared error makes this an apples-to-oranges comparison, because the unit scales of each machine learning task is different. On the other hand, variance explained is unit-less.&lt;/p&gt;
&lt;p&gt;Now, we know that single metrics can have failure points, as does the coefficient of correlation r^2^, as shown in Ansecombe's quartet and the Datasaurus Dozen:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://d2f99xq7vri1nk.cloudfront.net/Anscombe_1_0_0.png&quot; alt=&quot;Ansecombe&amp;#39;s quartet, taken from Autodesk Research&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Fig. 1: Ansecombe's quartet, taken from Autodesk Research&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://blog.revolutionanalytics.com/downloads/DataSaurus%20Dozen.gif&quot; alt=&quot;Datasaurus Dozen, taken from Revolution Analytics&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Fig. 2: Datasaurus Dozen, taken from Revolution Analytics&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;One place where the variance explained can fail is if the predictions are systematically shifted off from the true values. Let's say prediction was shifted off by 2 units.&lt;/p&gt;
&lt;p&gt;
$$var(y_{true} - y_{pred}) = var([2, 2, ..., 2]) = 0$$
&lt;/p&gt;&lt;p&gt;There's no variance in errors, even though they are systematically shifted off from the true prediction. Like r&lt;sup&gt;2&lt;/sup&gt;, variance explained will fail here.&lt;/p&gt;
&lt;p&gt;As usual, &lt;a href=&quot;https://en.wikipedia.org/wiki/Anscombe%27s_quartet&quot;&gt;Ansecombe's quartet&lt;/a&gt;, as does &lt;a href=&quot;https://www.autodeskresearch.com/publications/samestats&quot;&gt;The Datasaurus Dozen&lt;/a&gt;, gives us a pertinent reminder that visually inspecting your model predictions is always a good thing!&lt;/p&gt;
&lt;p&gt;h/t to my colleague, &lt;a href=&quot;https://www.linkedin.com/in/clayton-springer-5a48072/&quot;&gt;Clayton Springer&lt;/a&gt;, for sharing this with me.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/3/22/functools-partial/">
    <title type="text">Functools Partial</title>
    <id>urn:uuid:7059f3fb-181e-305d-840a-bf1c34b42e8b</id>
    <updated>2019-03-22T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/3/22/functools-partial/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;If you’ve done Python programming for a while, I think it pays off to know some little tricks that can improve the readability of your code and decrease the amount of repetition that goes on.&lt;/p&gt;
&lt;p&gt;One such tool is &lt;code&gt;functools.partial&lt;/code&gt;. It took me a few years after my first introduction to &lt;code&gt;partial&lt;/code&gt; before I finally understood why it was such a powerful tool.&lt;/p&gt;
&lt;p&gt;Essentially, what &lt;code&gt;partial&lt;/code&gt; does is it wraps a function and sets a keyword argument to a constant. That’s it. What do we mean?&lt;/p&gt;
&lt;p&gt;Here’s a minimal example. Let’s say we have a function &lt;code&gt;f&lt;/code&gt;, not written by me, but provided by someone else.&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# do something with a and b.&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In my code, let’s say that I know that the value that &lt;code&gt;b&lt;/code&gt; takes on in my app is always the tuple &lt;code&gt;(1, 'A')&lt;/code&gt;. I now have a few options. The most obvious is assign the tuple &lt;code&gt;(1, 'A')&lt;/code&gt; to a variable, and pass that in on every function call:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;A&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# do some stuff.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# do more stuff.&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# ad nauseum&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# set value of N&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;resultN&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The other way I could do it is use &lt;code&gt;functools.partial&lt;/code&gt; and just set the keyword argument &lt;code&gt;b&lt;/code&gt; to equal to the tuple directly.&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;functools&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;partial&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;f_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;partial&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;A&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, I can repeat the code above, but now only worrying about the keyword argument &lt;code&gt;a&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;result1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# do some stuff.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# do more stuff.&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# ad nauseum&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# set value of N&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;resultN&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And there you go, that’s basically how &lt;code&gt;functools.partial&lt;/code&gt; works in a nutshell.&lt;/p&gt;
&lt;p&gt;Now, where have I used this in real life?&lt;/p&gt;
&lt;p&gt;The most common place I have used it is in Flask. I have built Flask apps where I need to dynamically keep my Bokeh version synced up between the Python and JS libraries that get called. To ensure that my HTML templates have a consistent Bokeh version, I use the following pattern:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;bokeh&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__version__&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bkversion&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;flask&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;render_template&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Flask&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;functools&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;partial&lt;/span&gt; 

&lt;span class=&quot;n&quot;&gt;render_template&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;partial&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;render_template&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bkversion&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bkversion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Flask app boilerplate&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;app&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Flask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;vm&quot;&gt;__name__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;nd&quot;&gt;@app&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;route&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;/&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;home&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;render_template&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;index.html.j2&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, because I always have &lt;code&gt;bkversion&lt;/code&gt; pre-specified in &lt;code&gt;render_template&lt;/code&gt;, I never have to repeat it over every &lt;code&gt;render_template&lt;/code&gt; function call.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/3/20/how-i-work/">
    <title type="text">How I Work</title>
    <id>urn:uuid:ecb1119f-e770-31bd-bddd-4da7130485e2</id>
    <updated>2019-03-20T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/3/20/how-i-work/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;I was inspired to write this because of Will Wolf’s interview with DeepLearning.AI, in which I found a ton of similarities between how both of us work. As such, I thought I’d write down what I use at work to get things done.&lt;/p&gt;
&lt;h2 id=&quot;tooling&quot;&gt;Tooling&lt;/h2&gt;&lt;p&gt;For a data scientist, I think tooling is of very high importance: mastery over our tools keeps us productive. Here’s a sampling of what I use at work:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Compute: I have my own MacBook, but I prefer freeloading off my colleague’s workstation, which is connected to our HPC compute cluster, allowing me to do parallelization with Dask!&lt;/li&gt;
&lt;li&gt;Editors/IDEs: VSCode + Jupyter Lab (JLab). Lots of plugins for VSCode!&lt;/li&gt;
&lt;li&gt;Terminal: iTerm, with my &lt;a href=&quot;https://github.com/ericmjl/dotfiles&quot;&gt;&lt;code&gt;dotfiles&lt;/code&gt;&lt;/a&gt; providing a high degree of customization. I also use the VSCode and JLab terminals where convenient.&lt;/li&gt;
&lt;li&gt;General Purpose: Python, Dask, git&lt;/li&gt;
&lt;li&gt;ML/Stats: &lt;code&gt;scikit-learn&lt;/code&gt;, &lt;code&gt;jax&lt;/code&gt;, &lt;code&gt;pymc3&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Data wrangling: &lt;code&gt;pandas&lt;/code&gt;, &lt;code&gt;pyjanitor&lt;/code&gt; (a package I wrote to provide convenience APIs for data cleaning)&lt;/li&gt;
&lt;li&gt;Data visualization: &lt;code&gt;matplotlib&lt;/code&gt;, &lt;code&gt;seaborn&lt;/code&gt;, &lt;code&gt;holoviews&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;App development: &lt;code&gt;flask&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As you probably can see, I’m a very Python-centric person!&lt;/p&gt;
&lt;h2 id=&quot;daily/weekly-routines&quot;&gt;Daily/Weekly Routines&lt;/h2&gt;&lt;p&gt;Most of my work necessitates long stretches of thinking and hacking time. Without that, I’m unable to get into “the zone” to do anything productive. Hence, I have a habit of packing meetings onto Mondays (a.k.a. “Meeting Mondays”). Backup times for meetings, which I prefer to not do, are 11 am and 1 pm, bookending lunch time so that I don’t end up with a fragmented morning/afternoon. The only exceptions I make are for my two high-priority team meetings, for which I defer to the rest of the team. I’m glad that my managers understand the need for long stretches of hacking time, and have stuck to Monday one-on-one meetings.&lt;/p&gt;
&lt;p&gt;Hence, almost every day from Tuesday through to Friday, I have long stretches of pre-allocated time for hacking. It’s data science scheduling bliss! It also means I turn down a lot of “can I meet you to chat” invites - unless we can pack them on Monday!&lt;/p&gt;
&lt;p&gt;On Friday, I make a point to try to work remotely. It helps with sanity, particularly in the winter, when the commute gets harsh and I can’t bike. Fridays also are the days on which I try to do my open source work.&lt;/p&gt;
&lt;h2 id=&quot;pair-coding&quot;&gt;Pair Coding&lt;/h2&gt;&lt;p&gt;Pair coding with others on mutual projects has been a very productive endeavor, &lt;a href=&quot;https://ericmjl.github.io/blog/2019/3/1/pair-coding-why-and-how-for-data-scientists/&quot;&gt;which I have written about before&lt;/a&gt;. Unlike weekly update meetings, I plan for pair coding on an as-needed basis. We have a pre-defined goal for what we want to accomplish, including a conceivably achievable goal and a stretch goal; achieving the easier one keeps us motivated. It follows the “no agenda, no meeting” rule of thumb by which I protect my time.&lt;/p&gt;
&lt;p&gt;I found that a good setup is really necessary for pair coding to be successful. A minimum is a dual-monitor setup, with one extra keyboard + mouse for my coding partner.&lt;/p&gt;
&lt;p&gt;One thing I didn’t mention in my previous blog post was how knowledge transfer happens. Here’s how I think it works. We have one in the “driver’s seat”, and the other in the observer role. Knowledge transfer generally happens from the more experienced person to the less experienced one, and the driver doesn’t necessarily have to be the more experienced one. For example, when pair coding with my intern, I play the role of observer and may dictate code or outline what needs to be done, but I don’t actively take over on my keyboard unless there’s a situation that shows up that is irrelevant to the coding session goals. On the other hand, if there’s a codebase I’ve developed for which I need to play the tour guide role, I will be in the driver’s seat, while the observer will help me catch peripheral errors that I’m making.&lt;/p&gt;
&lt;h2 id=&quot;learning-new-things&quot;&gt;Learning New Things&lt;/h2&gt;&lt;p&gt;Pair coding has been one way I learn new things. For example, with my colleague Zach as the observer, we hacked together a simple dashboard project using Flask, Holoviews and Panel.&lt;/p&gt;
&lt;p&gt;I’m not very mathematically-savvy, in that algebra is difficult for me to follow. (I’m mildly algebra-blind, but getting better now.) Ironically, code, which is algebraic in nature too, but works with plain English names, works much better for me. Implementing algorithms and statistical methods using &lt;code&gt;jax&lt;/code&gt; (for things that involve differential computing) and &lt;code&gt;PyMC3&lt;/code&gt; (for all things Bayesian) has served to be very educational. While implementing, I also impose some software abstractions on the math, and this also forces me to organize my knowledge, which also helps learning. Implementing things on the computer is also the perfect way to learn by teaching: The computer is the ultimately dumb student, as it will execute exactly as you tell it, mistakes included!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/3/1/pair-coding-why-and-how-for-data-scientists/">
    <title type="text">Pair Coding: Why and How for Data Scientists</title>
    <id>urn:uuid:639416f0-072e-3e1a-bb09-6bbec1ce20e0</id>
    <updated>2019-03-01T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/3/1/pair-coding-why-and-how-for-data-scientists/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;&lt;p&gt;While at work, I've been experimenting with pair coding with other data science-oriented colleagues. My experiences tell me that this is something extremely valuable to do. I'd like to share here the &quot;why&quot; and the &quot;how&quot; on pair coding, but focused towards data scientists.&lt;/p&gt;
&lt;h2 id=&quot;what-is-pair-coding?&quot;&gt;What is pair coding?&lt;/h2&gt;&lt;p&gt;Pair coding is a form of programming where two people work together on a single code base together. It usually involves one person on the keyboard and another talking through the problem and observing for issues, such as syntax, logic, or code style. Occasionally, they may swap who is on the keyboard. In other words, one is the &quot;creator&quot;, and the other is the &quot;critic&quot; (but in a positive, constructive fashion).&lt;/p&gt;
&lt;h2 id=&quot;what-s-your-history-with-pair-coding?&quot;&gt;What's your history with pair coding?&lt;/h2&gt;&lt;p&gt;I was inspired by a few places. Firstly, there are a wealth of blog posts detailing the potential benefits and pitfalls of pair coding, in a software developer's context. (A quick Google search will lead you to them.) Secondly, I had, at work, experimented with &quot;pair hacking&quot; sessions, which involved more than coding, including white-boarding a problem to get a feel for its scope, and it turned out to be pretty productive. Thirdly, I was inspired by &lt;a href=&quot;https://www.newyorker.com/magazine/2018/12/10/the-friendship-that-made-google-huge&quot;&gt;a New Yorker article on Jeff and Sanjay&lt;/a&gt;, in which part of it chronicled how they worked as a pair to solve the toughest problems at Google.&lt;/p&gt;
&lt;p&gt;Now, because I'm not a software engineer by training, and because don't have extensive experience beforehand, and because there are no data-science-oriented resources for pair coding that I have read before (I'd love to read them if you know of any!), I've had to be adapt what I read for software development to a data science context.&lt;/p&gt;
&lt;h2 id=&quot;what-are-the-potential-benefits-of-pair-coding?&quot;&gt;What are the potential benefits of pair coding?&lt;/h2&gt;&lt;p&gt;I can see at least the following benefits, if not more that I have yet to discover:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Instant peer review over data science logic and code. Because we are talking through a problem while coding it up, we can instantly check whether our logic is correct against each other.&lt;/li&gt;
&lt;li&gt;Knowledge transfer. In my experience, I've had productive pair-coding sessions with another colleague who has a better grasp of the project than I do. Hence, I contribute &amp;amp; teach the technical component, while I also learn the broader project context better.&lt;/li&gt;
&lt;li&gt;Building trust. We all know that the more closely you work with someone, the more rough corners get rubbed off.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;what-pre-requisites-do-you-see-for-a-productive-pair-programming-session?&quot;&gt;What pre-requisites do you see for a productive pair programming session?&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;A long, continuous, and uninterrupted time slot (at least 2-3 hours in length) to maintain continuity.&lt;/li&gt;
&lt;li&gt;A defined goal or question that we are seeking to answer - keeps us focused on what needs to be done.&lt;/li&gt;
&lt;li&gt;That goal should also be plausibly achievable within the 2-3 hour timeframe. &lt;/li&gt;
&lt;li&gt;Large monitors for both parties to look at, or a code-sharing platform where both can see the code without needing to physically huddle.&lt;/li&gt;
&lt;li&gt;A place where we can talk without feeling hindered.&lt;/li&gt;
&lt;li&gt;No impromptu interruptions from other individuals.&lt;/li&gt;
&lt;li&gt;Complementary and intersecting skillsets.&lt;/li&gt;
&lt;li&gt;Open-minded individuals who are willing to learn. (Ego-free.)&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;where-does-pair-coding-differ-for-data-scientists-vs.-software-engineers?&quot;&gt;Where does pair coding differ for data scientists vs. software engineers?&lt;/h2&gt;&lt;p&gt;I think the differences at best are subtle, not necessarily overt.&lt;/p&gt;
&lt;p&gt;The biggest difference that I can think of might be in clarity. To the best of my knowledge, software engineers work with pretty well-defined requirements. The only hiccups that I can imagine that may occur are in unforeseen logic/code blockers. Data scientists, on the other hand, often are exploring and defining the requirements as things go along. In other words, we are working with more unknowns than a software engineer might.&lt;/p&gt;
&lt;p&gt;An example is a model I built with a colleague at work that involved groups of groups of samples. We weren't able to envision the final model right at the beginning, and code towards it. Rather, we built the model iteratively, starting with highly simplifying assumptions, discussing which ones to refine, and iteratively building the model as we went forward.&lt;/p&gt;
&lt;p&gt;Perhaps a related difference is that as data scientists, because of potentially greater uncertainty surrounding the final product, we may end up talking more about project direction than one would as a software engineer. But that's probably just a minor detail.&lt;/p&gt;
&lt;h2 id=&quot;do-you-have-any-memorable-quotes-from-the-new-yorker-article?&quot;&gt;Do you have any memorable quotes from the New Yorker article?&lt;/h2&gt;&lt;p&gt;Yes, a number of them.&lt;/p&gt;
&lt;p&gt;One on scaling things up.&lt;/p&gt;
&lt;blockquote class=&quot;blockquote&quot;&gt;
&lt;p&gt;Alan Eustace became the head of the engineering team after Rosing left, in 2005. “To solve problems at scale, paradoxically, you have to know the smallest details,” Eustace said.&lt;/p&gt;
&lt;/blockquote&gt;&lt;p&gt;Another on pair programming as an uncommon practice:&lt;/p&gt;
&lt;blockquote class=&quot;blockquote&quot;&gt;
&lt;p&gt;“I don’t know why more people don’t do it,” Sanjay said, of programming with a partner.&lt;/p&gt;
&lt;p&gt;“You need to find someone that you’re gonna pair-program with who’s compatible with your way of thinking, so that the two of you together are a complementary force,” Jeff said.&lt;/p&gt;
&lt;/blockquote&gt;</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/1/28/minimum-viable-products-matter/">
    <title type="text">Minimum Viable Products (MVPs) Matter</title>
    <id>urn:uuid:e7106802-1560-3f7e-a492-9b3e5bd3d957</id>
    <updated>2019-01-28T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/1/28/minimum-viable-products-matter/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;MVPs matter because they afford us at least two things:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Psychological safety&lt;/li&gt;
&lt;li&gt;Credibility&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Psychological safety comes from knowing that we have at least a working prototype that we can deliver to whomever is going to consume our results. We aren't stuck in the land of imaginary ideas without something tangible for others to interact with.&lt;/p&gt;
&lt;p&gt;Credibility comes about because with the MVP on hand, others now can trust on our ability to execute on an idea. Prior to that, all that others have to go off are promises of &quot;a thing&quot;.&lt;/p&gt;
&lt;p&gt;Build your MVPs. They're a good thing!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2019/1/21/advi-scalable-bayesian-inference/">
    <title type="text">ADVI: Scalable Bayesian Inference</title>
    <id>urn:uuid:c601f17e-84cb-3984-8342-27dd19372045</id>
    <updated>2019-01-21T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2019/1/21/advi-scalable-bayesian-inference/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;&lt;p&gt;You never know when scalability becomes an issue. Indeed, scalability necessitates a whole different world of tooling.&lt;/p&gt;
&lt;p&gt;While at work, I've been playing with a model - a Bayesian hierarchical 4-parameter dose response model, to be specific. With this model, the overall goal (without going into proprietary specifics) was parameter learning - what's the 50th percentile concentration, what's the max, what's the minimum, etc.; what was also important was quantifying the uncertainty surrounding these parameters.&lt;/p&gt;
&lt;h2 id=&quot;prototype-phase&quot;&gt;Prototype Phase&lt;/h2&gt;&lt;p&gt;Originally, when I prototyped the model, I used just a few thousand samples, which was trivial to fit with NUTS. I also got the model specification (both the group-level and population-level priors) done using those same few thousand.&lt;/p&gt;
&lt;p&gt;At some point, I was qualitatively and quantitatively comfortable with the model specification. Qualitatively - the model structure reflected prior biochemical knowledge. Quantitatively, I saw good convergence when examining the sampling traces, as well as the shrinkage phenomena.&lt;/p&gt;
&lt;h2 id=&quot;scaling-up&quot;&gt;Scaling Up&lt;/h2&gt;&lt;p&gt;Once I reached that point, I decided to scale up to the entire dataset: 400K+ samples, 3000+ groups.&lt;/p&gt;
&lt;p&gt;Fitting this model with NUTS with the full dataset would have taken a week, with no stopping time guarantees at the end of an overnight run - when I left the day before, I was still hoping for 5 days. However, switching over to ADVI (automatic differentiation variational inference) was a key enabler for this model: I was able to finish fitting the model with ADVI in just 2.5 hours, with similar uncertainty estimates (it'll never end up being identical, given random sampling).&lt;/p&gt;
&lt;h2 id=&quot;thoughts&quot;&gt;Thoughts&lt;/h2&gt;&lt;p&gt;I used to not appreciate that ADVI could be useful for simpler models; in the past, I used to think that ADVI was mainly useful in Bayesian neural network applications - in other words, with large parameter and large data models.&lt;/p&gt;
&lt;p&gt;With this example, I'm definitely more informed about what &quot;scale&quot; can mean: both in terms of number of parameters in a model, and in terms of number of samples that the model is fitted on. In this particular example, the model is simple, but the number of samples is so large that ADVI becomes a feasible alternative to NUTS MCMC sampling.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2018/12/25/conda-hacks-for-data-science-efficiency/">
    <title type="text">Conda hacks for data science efficiency</title>
    <id>urn:uuid:251beb77-b7ce-3834-b046-ca9701afdffb</id>
    <updated>2018-12-25T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2018/12/25/conda-hacks-for-data-science-efficiency/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;The &lt;code&gt;conda&lt;/code&gt; package manager has, over the years, become an integral part of my workflow. I use it to manage project environments, and have built a bunch of very simple hacks around it that you can adopt too. I'd like to share them with you, alongside the rationale for using them.&lt;/p&gt;
&lt;h2 id=&quot;hack-1:-set-up-your-.condarc&quot;&gt;Hack #1: Set up your &lt;code&gt;.condarc&lt;/code&gt;&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Why?&lt;/strong&gt; It will save you a few keystrokes each time you want to do something with &lt;code&gt;conda&lt;/code&gt;. For example, in my &lt;code&gt;.condarc&lt;/code&gt;, I have the following:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Set the channels that the `conda install` command will &lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# automatically search through.&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;defaults&lt;/span&gt;
  &lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;conda-forge&lt;/span&gt;
  &lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;ericmjl&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Always accept installation. Is convenient, but always &lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# double-check!&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;always_yes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;true&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For more information on how to configure your &lt;code&gt;.condarc&lt;/code&gt;, check the &lt;a href=&quot;https://conda.io/docs/user-guide/configuration/use-condarc.html&quot;&gt;online documentation&lt;/a&gt;!&lt;/p&gt;
&lt;h2 id=&quot;hack-2:-use-one-environment-spec-file-per-project&quot;&gt;Hack #2: Use one environment spec file per project&lt;/h2&gt;&lt;p&gt;This assumes that you have the habit of putting all files related to one project inside one folder, using subdirectories for finer-grained organization.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Why?&lt;/strong&gt; It will ensure that you have one version-controlled, authoritative specification for the packages that are associated with the project. This is good for (1) reproducibility, as you can send it to a colleague and have them reproduce the environment, and (2) will enable Hack #3, which I will showcase afterwards.&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# file name: environment.yml&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Give your project an informative name&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;project-name&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Specify the conda channels that you wish to grab packages from, in order of priority.&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;defaults&lt;/span&gt;
&lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;conda-forge&lt;/span&gt;
&lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;ericmjl&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Specify the packages that you would like to install inside your environment. Version numbers are allowed, and conda will automatically use its dependency solver to ensure that all packages work with one another.&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;dependencies&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;python=3.7&lt;/span&gt;
&lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;conda&lt;/span&gt;
&lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;jupyterlab&lt;/span&gt;
&lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;scipy&lt;/span&gt;
&lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;numpy&lt;/span&gt;
&lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;pandas&lt;/span&gt;
&lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;pyjanitor&lt;/span&gt;
&lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;pandas&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# There are some packages which are not conda-installable. You can put the pip dependencies here instead.&lt;/span&gt;
&lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;pip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;p p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;tqdm&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# for example only, tqdm is actually available by conda.&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;A hack that I have related to this is that I use TextExpander shortcut to populate a starting environment spec file.&lt;/p&gt;
&lt;p&gt;Additionally, if I want to install a new package, rather than simply typing &lt;code&gt;conda install &amp;lt;packagename&amp;gt;&lt;/code&gt;, I will add the package to the environment spec file, and then type &lt;code&gt;conda env update -f environment.yml&lt;/code&gt;, as more often than not, my default is to continue using the package I added.&lt;/p&gt;
&lt;p&gt;For more details on what the environment spec file is all about, &lt;a href=&quot;https://conda.io/docs/user-guide/tasks/manage-environments.html#create-env-file-manually&quot;&gt;read the online docs&lt;/a&gt;!&lt;/p&gt;
&lt;h2 id=&quot;hack-3:-use-conda-auto-env&quot;&gt;Hack 3: use &lt;code&gt;conda-auto-env&lt;/code&gt;&lt;/h2&gt;&lt;p&gt;Written by &lt;a href=&quot;https://chdoig.github.io&quot;&gt;Christine Doig&lt;/a&gt;, &lt;a href=&quot;https://github.com/chdoig/conda-auto-env&quot;&gt;&lt;code&gt;conda-auto-env&lt;/code&gt;&lt;/a&gt; is a bash hack that enables you to automatically activate an environment once you enter into a project directory, &lt;em&gt;as long as an &lt;code&gt;environment.yml&lt;/code&gt; file already exists in the directory&lt;/em&gt;. If the environment does not already exist, then &lt;code&gt;conda-auto-env&lt;/code&gt; will automatically create one based on the &lt;code&gt;environment.yml&lt;/code&gt; file in your project directory.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Why?&lt;/strong&gt; If you have many projects that you are working on, then it will greatly reduce the amount of effort used to remember which project to activate.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;conda-auto-env&lt;/code&gt; looks like this:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# File: .conda-auto-env&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#!/bin/bash&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;function&lt;/span&gt; conda_auto_env&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt; -e &lt;span class=&quot;s2&quot;&gt;&amp;quot;environment.yml&amp;quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;then&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# echo &amp;quot;environment.yml file found&amp;quot;&lt;/span&gt;
    &lt;span class=&quot;nv&quot;&gt;ENV&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;$(&lt;/span&gt;head -n &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt; environment.yml &lt;span class=&quot;p&quot;&gt;|&lt;/span&gt; cut -f2 -d &lt;span class=&quot;s1&quot;&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Check if you are already in the environment&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;[[&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$PATH&lt;/span&gt; !&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; *&lt;span class=&quot;nv&quot;&gt;$ENV&lt;/span&gt;* &lt;span class=&quot;o&quot;&gt;]]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;then&lt;/span&gt;
      &lt;span class=&quot;c1&quot;&gt;# Check if the environment exists&lt;/span&gt;
      conda activate &lt;span class=&quot;nv&quot;&gt;$ENV&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$?&lt;/span&gt; -eq &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;then&lt;/span&gt;
        :
      &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# Create the environment and activate&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;Conda env &amp;#39;&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ENV&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;#39; doesn&amp;#39;t exist.&amp;quot;&lt;/span&gt;
        conda env create -q
        conda activate &lt;span class=&quot;nv&quot;&gt;$ENV&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;fi&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;fi&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;fi&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;export&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;PROMPT_COMMAND&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;conda_auto_env
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To use it, you have two options. You can either copy/paste the whole original script into your &lt;code&gt;.bashrc&lt;/code&gt;, or you can put it in a file called &lt;code&gt;.conda-auto-env&lt;/code&gt;, and source it from your &lt;code&gt;.bashrc&lt;/code&gt;. I recommend the latter, as it makes managing your &lt;code&gt;.bashrc&lt;/code&gt; easier:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# File: .bashrc&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;source&lt;/span&gt; /path/to/.conda-auto-env
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&quot;hack-4:-hijack-bash-aliases-for-conda-commands&quot;&gt;Hack 4: hijack bash aliases for &lt;code&gt;conda&lt;/code&gt; commands&lt;/h2&gt;&lt;p&gt;I use aliases to save myself a few keystrokes whenever I'm at the terminal. This is a generalizable bash hack, but here it is as applied to &lt;code&gt;conda&lt;/code&gt; commands.&lt;/p&gt;
&lt;p&gt;Anyways, these are the commands that I use most often, which I have found it useful to alias:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# File: .aliases&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;alias&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;ceu&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;conda env update&amp;quot;&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;alias&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;cl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;conda list&amp;quot;&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;alias&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;ci&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;conda install&amp;quot;&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;alias&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;cr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;conda remove&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Make sure your aliases don't clash with existing commands that you use!&lt;/p&gt;
&lt;p&gt;Then, source &lt;code&gt;.aliases&lt;/code&gt; in your &lt;code&gt;. bashrc&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# File: .bashrc&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;source&lt;/span&gt; /path/to/.aliases
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, all of your defined aliases will be available in your bash shell.&lt;/p&gt;
&lt;p&gt;The idea/pattern, as I mentioned earlier, is generalizable beyond just bash commands. (I have &lt;code&gt;ls&lt;/code&gt; aliased for &lt;code&gt;exa&lt;/code&gt;, and &lt;code&gt;l&lt;/code&gt; aliased for &lt;code&gt;ls&lt;/code&gt; - the epitome of laziness!)&lt;/p&gt;
&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;&lt;p&gt;I hope you found these bash and &lt;code&gt;conda&lt;/code&gt; hacks to be useful. Hopefully they will help you become more productive and efficient!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2018/12/16/gaussian-process-notes/">
    <title type="text">Gaussian Process Notes</title>
    <id>urn:uuid:66321576-e328-3d24-b3a2-c90b3fa831fe</id>
    <updated>2018-12-16T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2018/12/16/gaussian-process-notes/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;I first learned GPs about two years back, and have been fascinated by the idea. I learned it through a video by David MacKay, and managed to grok it enough that I could put it to use in simple settings. That was reflected in my &lt;a href=&quot;https://fluforecaster.herokuapp.com&quot;&gt;Flu Forecaster project&lt;/a&gt;, in which my GPs were trained only on individual latent spaces.&lt;/p&gt;
&lt;p&gt;Recently, though, I decided to seriously sit down and try to grok the math behind GPs (and other machine learning models). To do so, I worked through &lt;a href=&quot;https://youtu.be/4vGiHC35j9s&quot;&gt;Nando de Freitas' YouTube videos on GPs&lt;/a&gt;. (Super thankful that he has opted to put these videos up online!)&lt;/p&gt;
&lt;p&gt;The product of this learning is two-fold. Firstly, I have added a &lt;a href=&quot;https://github.com/ericmjl/bayesian-analysis-recipes/blob/master/notebooks/gp-test.ipynb&quot;&gt;GP notebook&lt;/a&gt; to my &lt;a href=&quot;https://github.com/ericmjl/bayesian-analysis-recipes&quot;&gt;Bayesian analysis recipes&lt;/a&gt; repository.&lt;/p&gt;
&lt;p&gt;Secondly, I have also put together some hand-written notes on GPs. (For those who are curious, I first hand-wrote them on paper, then copied them into my iPad mini using a Wacom stylus. We don't have the budget at the moment for an iPad Pro!) They can be downloaded &lt;a href=&quot;../../../../../blog/2018/12/16/gaussian-process-notes/gaussian-processes.pdf&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Some lessons learned:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Algebra is indeed a technology of sorts (to quote Jeremy Kun's book). Being less sloppy than I used to be gives me the opportunity to connect ideas on the page to ideas in my head, and express them more succinctly.&lt;/li&gt;
&lt;li&gt;Grokking the math behind GPs at the minimum requires one thing: remembering, or else knowing how to derive, the formula for how to get the distribution parameters of a multivariate Gaussian conditioned on some of of its variables.&lt;/li&gt;
&lt;li&gt;Once I grokked the math, implementing a GP using only NumPy was trivial; also, extending it to higher dimensions was similarly trivial!&lt;/li&gt;
&lt;/ul&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2018/12/9/mathematical-intuition/">
    <title type="text">Mathematical Intuition</title>
    <id>urn:uuid:4ae0c241-c33a-3013-8d4c-baf759793236</id>
    <updated>2018-12-09T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2018/12/9/mathematical-intuition/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;Last week, I picked up Jeremy Kun's book, &quot;A Programmer's Introduction to Mathematics&quot;. In it, I finally found an explanation for my frustrations when reading math papers:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;What programmers would consider “sloppy” notation is one symptom of the problem, but there there are other expectations on the reader that, for better or worse, decelerate the pace of reading. Unfortunately I have no solution here. Part of the power and expressiveness of mathematics is the ability for its practitioners to overload, redefine, and omit in a suggestive manner. Mathematicians also have thousands of years of “legacy” math that require backward compatibility. Enforcing a single specification for all of mathematics—a suggestion I frequently hear from software engineers—would be horrendously counterproductive.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Reading just &lt;em&gt;that&lt;/em&gt; paragraph explained, in such a lucid manner, how my frustrations reading mathematically-oriented papers, stemmed from mismatched expectations. I come into a paper thinking like a software engineer. Descriptive variable names (as encouraged by Python), which are standardized as well, with structured abstractions providing a hierarchy of logic between chunks of code... No, mathematicians are more like Shakespeare - or perhaps linguists - in that they will take a symbol and imbibe it with a subtly new meaning or interpretation inspired by a new field. That &quot;L&quot; you see in one field of math doesn't always exactly mean the same thing in another field.&lt;/p&gt;
&lt;h2 id=&quot;biology-vs.-math?&quot;&gt;Biology vs. Math?&lt;/h2&gt;&lt;p&gt;The contrast is stark when compared against reading a biology paper. With a biology paper, if you know the key wet-bench experiment types (and there's not that many), you can essentially get the gist of a paper by reading the abstract and dissecting the figures, which, granted, are described and labelled with field-specific jargon, but are at least descriptive names. With a math-oriented paper, the equations are the star, and one has to really grok each element of the equations to know what they mean. It means taking the time to dissect each equation and ask what each symbol is, what each group of symbols means, and how those underlying ideas connect with one another and with other ideas. It's not unlike a biology paper, but requiring a different kind of patience, one that I wasn't trained in.&lt;/p&gt;
&lt;h2 id=&quot;learning-to-learn-by-teaching&quot;&gt;Learning to Learn by Teaching&lt;/h2&gt;&lt;p&gt;As Jeremy Kun wrote in his book, programmers do have some sort of a leg-up when it comes to reading and understanding math. It's a bit more than what Kun wrote, I think - yes, many programming ideas have deep mathematical connections. But I think there's more.&lt;/p&gt;
&lt;p&gt;One thing we know from research into how people learn is that teaching someone something is an incredible way to learn that something. From my prior experience, the less background a student has in a material, the more demands are placed on the teacher's understanding of the material, as we work out how the multiple representations in our head to try to communicate it to them.&lt;/p&gt;
&lt;p&gt;As it turns out, we programmers have the ultimate dumb &quot;student&quot; available at our fingertips: Our computers! By implementing mathematical ideas in code, we are essentially &quot;teaching&quot; the computer to do something mathematical. Computers are not smart; they are programmed to do exactly what we input to them. If we get an idea wrong, our implementation of the math will likely be wrong. That fundamental law of computing shows up again: Garbage in, garbage out.&lt;/p&gt;
&lt;h2 id=&quot;hierarchy-of-ideas&quot;&gt;Hierarchy of Ideas&lt;/h2&gt;&lt;p&gt;More than just that, when we programmers implement a mathematical idea in code, we can start putting our &quot;good software engineering&quot; ideas into place! It helps the math become stickier when we can see, through code, the hierarchy of concepts that are involved.&lt;/p&gt;
&lt;p&gt;An example, for me, comes from the deep learning world. I had an attempt dissecting two math-y deep learning papers last week. Skimming through the papers didn't do much good for my understanding of the paper. Neither did trying to read the paper like I do a biology paper. Sure, I could perhaps just read the ideas that the authors were describing in prose, but I had no intuition on which to base a proper critique of the idea's usefulness. It took implementing those papers in Python code, writing tests for them, and using abstractions that I had previously written, to come to a place where I felt like the ideas in the paper were a flexibly wieldable tool in my toolkit.&lt;/p&gt;
&lt;p&gt;Reinventing the wheel, such that we can learn the wheel, can in fact help us decompose the wheel so that we can do other new things with it. Human creativity is such a wonderful thing!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2018/11/13/solving-problems-actionably/">
    <title type="text">Solving Problems Actionably</title>
    <id>urn:uuid:47287890-e99d-3c82-b19f-7d2ddd0735ac</id>
    <updated>2018-11-13T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2018/11/13/solving-problems-actionably/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;There's a quote by John Tukey that has been a recurrent theme at work.&lt;/p&gt;
&lt;blockquote class=&quot;blockquote&quot;&gt;
&lt;p&gt;It's better to solve the right problem approximately than to solve the wrong problem exactly.&lt;/p&gt;
&lt;/blockquote&gt;&lt;p&gt;Continuing on the theme of quoting two Georges:&lt;/p&gt;
&lt;blockquote class=&quot;blockquote&quot;&gt; 
&lt;p&gt;All models are wrong, but some are more useful than others.&lt;/p&gt;
&lt;/blockquote&gt;&lt;p&gt;H/T Allen Downey for pointing out that our minds think alike.&lt;/p&gt;
&lt;p&gt;I have been working on a modelling effort for colleagues at work. There were two curves involved, and the second depended on the first one.&lt;/p&gt;
&lt;p&gt;In both cases, I started with a simple model, and made judgment calls along the way as to whether to continue improving the model, or to stop there because the current iteration of the model was sufficient enough to act on. With first curve, the first model was actionable for me. With the second curve, the first model I wrote clearly wasn't good enough to be actionable, so I spent lots more rounds of iteration on it.&lt;/p&gt;
&lt;p&gt;But wait, how does one determine &quot;actionability&quot;?&lt;/p&gt;
&lt;h2 id=&quot;actionability&quot;&gt;Actionability&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;For myself&lt;/strong&gt;, it has generally meant that I'm confident enough in the results to take the next modelling step. My second curves depended on the first curves, and after double-checking multiple ways, I thought the first curve fits, though not perfect, were good enough when applied across a large number of samples that I could instead move on to the second curves.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;For others&lt;/strong&gt;, particularly at my workplace, it generally means a scientist can make a decision about what next experiment to run.&lt;/p&gt;
&lt;h2 id=&quot;insight-s-mvp-influence&quot;&gt;Insight's MVP Influence&lt;/h2&gt;&lt;p&gt;Going through Insight Data Science drilled into us an instinct for developing an MVP for our problem before going on to perfect it. I think that general model works well. My project's final modelling results will be the result of chains of modelling assumptions at every step. Documenting those steps clearly, and then being willing to revisit those assumptions, is going always a good thing.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2018/11/12/thoughts-on-black/">
    <title type="text">Thoughts on Black</title>
    <id>urn:uuid:08f2ca8d-df54-3740-8d8b-b93a5bd7e728</id>
    <updated>2018-11-12T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2018/11/12/thoughts-on-black/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;Having used Black for quite a while now, I have a hunch that it will continue to surpass its current popularity amongst projects.&lt;/p&gt;
&lt;p&gt;It's one thing to be opinionated about things that matter for a project, but don't matter personally. Like code style. It's another thing to actually build a tool that, with one command, realizes those opinions in (milli)seconds. That's exactly what Black does.&lt;/p&gt;
&lt;p&gt;At the end of the day, it was, and still is, a tool that has a very good human API - that of convenience.&lt;/p&gt;
&lt;p&gt;By being opinionated about what code &lt;em&gt;ought&lt;/em&gt; to look like, &lt;code&gt;black&lt;/code&gt; has very few configurable parameters. Its interface is very simple. &lt;em&gt;Convenient.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;By automagically formatting &lt;em&gt;every&lt;/em&gt; Python file in subdirectories (if not otherwise configured so), it makes code formatting quick and easy. &lt;em&gt;Convenient.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In particular, by being opinionated about conforming to community standards for code style with Python, &lt;code&gt;black&lt;/code&gt; ensures that formatted code is consistently formatted and thus easy to read. &lt;em&gt;Convenient!&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Because of this, I highly recommend the use of &lt;code&gt;black&lt;/code&gt; for code formatting.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install black
&lt;/code&gt;&lt;/pre&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2018/11/7/bayesian-modelling-is-hard-work/">
    <title type="text">Bayesian Modelling is Hard Work!</title>
    <id>urn:uuid:b544d44a-b92e-3f93-80b0-83a5f38ea17b</id>
    <updated>2018-11-07T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2018/11/7/bayesian-modelling-is-hard-work/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;It’s definitely not easy work; anybody trying to tell you that you can &quot;just apply this 
model and just be done with it&quot; is probably wrong.&lt;/p&gt;
&lt;h2 id=&quot;simple-models&quot;&gt;Simple Models&lt;/h2&gt;&lt;p&gt;Let me clarify: I agree that doing the first half of the statement, &quot;just apply this model&quot;, 
is a good starting point, but I disagree with the latter half, &quot;and just be done with it&quot;. I 
have found that writing and fitting a very naive Bayesian model to the data I have is a very 
simple thing. But doing the right thing is not. Let’s not be confused: I don’t mean a Naive 
Bayes model, I mean naively writing down a Bayesian model that is structured very simply 
with the simplest of priors that you can think of.&lt;/p&gt;
&lt;p&gt;Write down the model, including any transformations that you may need on the variables, and 
then lazily put in a bunch of priors. For example, you might just start with Gaussians 
everywhere a parameter could take on negative to positive infinity values, or a bounded Half 
Gaussian if it can only take values above (or below) a certain value. You might assume 
Gaussian-distributed noise in the output.&lt;/p&gt;
&lt;p&gt;Let’s still not be confused: Obviously this would not apply to a beta-bernoulli/binomial 
model!&lt;/p&gt;
&lt;p&gt;Doing the right thing, however, is where the tricky parts come in. To butcher and mash-up 
two quotes:&lt;/p&gt;
&lt;blockquote class=&quot;blockquote&quot;&gt;
&lt;p&gt;All models are wrong, but some are useful (Box), yet some models are more wrong than others (modifying from Orwell). 
&lt;/blockquote&gt;&lt;h2 id=&quot;critiquing-models&quot;&gt;Critiquing Models&lt;/h2&gt;&lt;p&gt;When doing modeling, a series of questions comes up:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Do my naive assumptions about &quot;Gaussians everywhere&quot; hold? &lt;/li&gt;
&lt;li&gt;Given that my output data are continuous, is there a better distribution that can describe 
the likelihood?&lt;/li&gt;
&lt;li&gt;Is there are more principled prior for some of the variables?&lt;/li&gt;
&lt;li&gt;Does my link function, which joins the input data to the output parameters, properly 
describe their relationship?&lt;/li&gt;
&lt;li&gt;Instead of independent priors per group, would a group prior be justifiable?&lt;/li&gt;
&lt;li&gt;Does my model yield posterior distributions that are within bounds of reasonable ranges, 
which come from my prior knowledge? If it does not, do I need to bound my priors instead of 
naively assuming the full support for those distributions?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I am quite sure that this list is non-exhaustive, and probably only covers the bare minimum 
we have to think about.&lt;/p&gt;
&lt;p&gt;Doing these model critiques is not easy. Yet, if we are to work towards truthful and 
actionable conclusions, it is a necessity. We want to know ground truth, so that we can act 
on it accordingly, and hence take appropriate actions.&lt;/p&gt;
&lt;h2 id=&quot;prior-experience&quot;&gt;Prior Experience&lt;/h2&gt;&lt;p&gt;I have experienced this modeling loop that Mike Betancourt describes (in his &lt;a href=&quot;https://github.com/betanalpha/jupyter_case_studies/blob/master/principled_bayesian_workflow/principled_bayesian_workflow.ipynb&quot;&gt;Principled 
Bayesian Workflow notebook&lt;/a&gt;) more than once. One involved count data, with a data 
scientist from TripAdvisor last year at the SciPy conference; another involved estimating 
cycle time distributions at work, and yet another involved a whole 4-parameter dose-response 
curve. In each scenario, model fitting and critique took hours at the minimum; I’d also note 
that with real world data, I didn’t necessarily get to the &quot;win&quot; was looking for.&lt;/p&gt;
&lt;p&gt;With the count data, the TripAdvisor data scientist and I reached a point where after 5 
rounds of tweaking his model, we had a model that fit the data, and described a data 
generating process that mimics closely to what we would expect given his process. It took us 
5 rounds, and 3 hours of staring at his model and data, to get there!&lt;/p&gt;
&lt;p&gt;Yet with cycle time distributions from work, a task ostensibly much easier (&quot;just fit a 
distribution to the data&quot;), none of my distribution choices, which reflected what I thought 
would be the data generating process, gave me a &quot;good fit&quot; to the data. I checked by many 
means: K-S tests, visual inspection, etc. I ended up abandoning the fitting procedure, and 
used empirical distributions instead.&lt;/p&gt;
&lt;p&gt;With a 4-parameter dose-response curve, it took me 6 hours to go through 6 rounds of 
modeling to get to a point where I felt comfortable with the model. I started with a 
simplifying &quot;Gaussians everywhere&quot; assumption. Later, though, I hesitantly and tentatively 
putting in bound priors because I knew some posterior distributions were completely out of 
range under the naive assumptions of the first model, and were likely a result of 
insufficient range in the concentrations tested. Yet even that model remained unsatisfying: 
I was stuck with some compounds that didn’t change the output regardless of concentration, 
and that data are fundamentally very hard to fit with a dose response curve. Thus I the next 
afternoon,I modeled the dose response relationship using a Gaussian Process instead. Neither 
model is completely satisfying to the degree that the count data model was, but both the GP 
and the dose-response curve are and will be roughly correct modeling choices (with the GP 
probably being more flexible), and importantly, both are actionable by the experimentalists.&lt;/p&gt;
&lt;h2 id=&quot;thoughts&quot;&gt;Thoughts&lt;/h2&gt;&lt;p&gt;As you probably can see, whenever we either (1) don’t know ground truth, and/or (2) have 
messy, real world data that don’t fit idealized assumptions about the data generating 
process, &lt;strong&gt;getting the model &quot;right&quot; is a very hard thing to do!&lt;/strong&gt; Moreover, data are 
insufficient on their own to critique the model; we will always need to bring in prior 
knowledge. Much as all probability is conditional probability (Venn), all modeling involves 
prior knowledge. Sometimes it comes up in non-modellable ways, though as far as possible, 
it’s a good exercise to try incorporating that into the model definition.&lt;/p&gt;
&lt;h2 id=&quot;canned-models?&quot;&gt;Canned Models?&lt;/h2&gt;&lt;p&gt;Even with that said, I’m still a fan of canned models, such as those provided by 
&lt;code&gt;pymc-learn&lt;/code&gt; and &lt;code&gt;scikit-learn&lt;/code&gt; - provided we recognize that their &quot;canned&quot; nature and are 
equipped to critique and modify said models. Yes, they provide easy, convenient baselines 
that we can get started with. We can &quot;just apply this model&quot;. But we can’t &quot;just be done 
with it&quot;: the hard part of getting the model right takes much longer and much more hard 
work. &lt;strong&gt;&lt;em&gt;Veritas!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2018/10/26/more-dask-pre-scattering-data/">
    <title type="text">More Dask: Pre-Scattering Data</title>
    <id>urn:uuid:8b67f328-eb2a-3367-a47c-537108e7c883</id>
    <updated>2018-10-26T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2018/10/26/more-dask-pre-scattering-data/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;I learned a new thing about &lt;code&gt;dask&lt;/code&gt; yesterday: pre-scattering data properly!&lt;/p&gt;
&lt;p&gt;Turns out, you can pre-scatter your data across worker nodes, and have them access that data later when submitting functions to the scheduler.&lt;/p&gt;
&lt;h2 id=&quot;how-to&quot;&gt;How-To&lt;/h2&gt;&lt;p&gt;To do so, we first call on &lt;code&gt;client.scatter&lt;/code&gt;, pass in the data that I want to scatter across all nodes, ensure that broadcasting is turned on (if and only if I am sure that all worker nodes will need it), and finally assign it to a new variable.&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;dask_jobqueue&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SGECluster&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;dask.distributed&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Client&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;cluster&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SGECluster&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# put parameters in there.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;client&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Client&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cluster&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;data_future&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;client&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;broadcast&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;One key thing to remember here is to assign the result of &lt;code&gt;client.scatter&lt;/code&gt; to a variable. This becomes a pointer that you pass into other functions that are submitted via the &lt;code&gt;client.submit&lt;/code&gt; interface. Because this point is not immediately clear from the &lt;code&gt;client.scatter&lt;/code&gt; docs, &lt;a href=&quot;https://github.com/dask/distributed/pull/2320&quot;&gt;I put in a pull request (PR) to provide some just-in-time documentation&lt;/a&gt;, which just got merged this morning. By the way, not every PR has to be code - documentation help is always good!&lt;/p&gt;
&lt;p&gt;Once we've scattered the data across our worker nodes and obtained a pointer for the scattered data, we can parallel submit our function across worker nodes.&lt;/p&gt;
&lt;p&gt;Let's say we have a function, called &lt;code&gt;func&lt;/code&gt;, that takes in the &lt;code&gt;data&lt;/code&gt; variable and returns a number. The key characteristic of this function is that it takes anywhere from a few seconds to minutes to run, but I need it run many times (think hundreds to thousands of times).&lt;/p&gt;
&lt;p&gt;In serial, I would usually do this as a list comprehension:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;results&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If done in parallel, I can now use the &lt;code&gt;client&lt;/code&gt; object to submit the function across all worker nodes. For clarity, let me switch to a &lt;code&gt;for-loop&lt;/code&gt; instead:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;results&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;results&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;client&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;submit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data_future&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;results&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;client&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gather&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;results&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Because the &lt;code&gt;client&lt;/code&gt; does not have to worry about sending the large &lt;code&gt;data&lt;/code&gt; object across the network of cluster nodes, it is very fast to submit the functions to the scheduler, which then dispatches it to the worker nodes, which all know where &lt;code&gt;data_future&lt;/code&gt; is on their own &quot;virtual cluster&quot; memory.&lt;/p&gt;
&lt;h2 id=&quot;advantages&quot;&gt;Advantages&lt;/h2&gt;&lt;p&gt;By pre-scattering, we invest a bit of time pre-allocating memory on worker nodes to hold data that are relatively expensive to transfer. This time investment reaps dividends later when we are working with functions that operate on the data.&lt;/p&gt;
&lt;h2 id=&quot;cautions&quot;&gt;Cautions&lt;/h2&gt;&lt;p&gt;Not really disadvantages (as I can't think of any), just some things to note:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;You need to know how much memory my data requires, and have to request for at least that amount of memory first per worker node at the the &lt;code&gt;SGECluster&lt;/code&gt; instantiation step.&lt;/li&gt;
&lt;li&gt;Pre-scattering sometimes takes a bit of time, but I have not seen it take as much time as having the scheduler handle everything.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;acknowledgments&quot;&gt;Acknowledgments&lt;/h2&gt;&lt;p&gt;Special thanks goes to &lt;a href=&quot;https://matthewrocklin.com&quot;&gt;Matt Rocklin&lt;/a&gt;, who answered my question on &lt;a href=&quot;https://stackoverflow.com/questions/52997229/is-there-an-advantage-to-pre-scattering-data-objects-in-dask&quot;&gt;StackOverflow&lt;/a&gt;, which in turn inspired this blog post.&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2018/10/11/parallel-processing-with-dask-on-gridengine-clusters/">
    <title type="text">Parallel Processing with Dask on GridEngine Clusters</title>
    <id>urn:uuid:18e951d8-6df5-3751-bf52-1bb750d90d1a</id>
    <updated>2018-10-11T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2018/10/11/parallel-processing-with-dask-on-gridengine-clusters/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;p&gt;I recently just figured out how to get this working... and it's awesome! :D&lt;/p&gt;
&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;&lt;p&gt;If I'm developing an analysis in the Jupyter notebook, and I have one semi-long-running function (e.g. takes dozens of seconds) that I need to run over tens to hundreds of thousands of similar inputs, it'll take &lt;em&gt;ages&lt;/em&gt; for this to complete in serial. For a sense of scale, a function that takes ~20 seconds per call run serially over 10,000 similar inputs would take 200,000 seconds, which is 2 days of run-time (not including any other overhead). That's not feasible for interactive exploration of data. If I could somehow parallelize just the function over 500 compute nodes, we could take the time down to 7 minutes.&lt;/p&gt;
&lt;p&gt;GridEngine-based compute clusters are one of many options for parallelizing work. During grad school at MIT, and at work at Novartis, the primary compute cluster environment that I've encountered has been GridEngine-based. However, because they are designed for batch jobs, as a computational scientist, we have to jump out of whatever development environment we're currently using, and move to custom scripts.&lt;/p&gt;
&lt;p&gt;In order to do parallelism with traditional GridEngine systems, I would have to jump out of the notebook and start writing job submission scripts, which disrupts my workflow. I would be disrupting my thought process, and lose the interactivity that I might need to prototype my work faster.&lt;/p&gt;
&lt;h2 id=&quot;enter-dask&quot;&gt;Enter Dask&lt;/h2&gt;&lt;p&gt;&lt;code&gt;dask&lt;/code&gt;, alongside &lt;code&gt;dask-jobqueue&lt;/code&gt; enables computational scientists like myself to take advantage of existing GridEngine setups to do interactive, parallel work. As long as I have a Jupyter notebook server running on a GridEngine-connected compute node, I can submit functions to the GridEngine cluster and collect back those results to do further processing, in a fraction of the time that it would take, thus enabling me to do my science faster than if I did everything single core/single node.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;In this blog post, I'd like to share an annotated, minimal setup for using Dask on a GridEngine cluster.&lt;/strong&gt; (Because we use Dask, more complicated pipelines are possible as well - I would encourage you to read the Dask docs for more complex examples.) I will assume that you are working in a Jupyter notebook environment, and that the notebook you are working out of is hosted on a GridEngine-connected compute node, from which you are able to &lt;code&gt;qsub&lt;/code&gt; tasks. Don't worry, you won't be qsub-ing anything though!&lt;/p&gt;
&lt;h2 id=&quot;setup&quot;&gt;Setup&lt;/h2&gt;&lt;p&gt;To start, we need a cell that houses the following code block:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;dask_jobqueue&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SGECluster&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;dask&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;delayed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;compute&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;cluster&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SGECluster&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;queue&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;default.q&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                     &lt;span class=&quot;n&quot;&gt;walltime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;1500000&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                     &lt;span class=&quot;n&quot;&gt;processes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                     &lt;span class=&quot;n&quot;&gt;memory&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;1GB&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                     &lt;span class=&quot;n&quot;&gt;cores&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                     &lt;span class=&quot;n&quot;&gt;env_extra&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;source /path/to/custom/script.sh&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                &lt;span class=&quot;s1&quot;&gt;&amp;#39;export ENV_VARIABLE=&amp;quot;SOMETHING&amp;quot;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
                       &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Here, we are instantiating an &lt;code&gt;SGECluster&lt;/code&gt; object under the variable name &lt;code&gt;cluster&lt;/code&gt;. What &lt;code&gt;cluster&lt;/code&gt; stores is essentially a configuration for a block of worker nodes that you will be requesting. Under the hood, what &lt;code&gt;dask-jobqueue&lt;/code&gt; is doing is submitting jobs to the GridEngine scheduler, which will block off a specified amount of compute resources (e.g. number of cores, amount of RAM, whether you want GPUs or not, etc.) for a pre-specified amount of time, on which Dask then starts a worker process to communicate with the head process coordinating tasks amongst workers.&lt;/p&gt;
&lt;p&gt;As such, you do need to know two pieces of information:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;queue&lt;/code&gt;: The queue that jobs are to be submitted to. Usually, it is named something like &lt;code&gt;default.q&lt;/code&gt;, but you will need to obtain this through GridEngine. If you have the ability to view all jobs that are running, you can call &lt;code&gt;qstat&lt;/code&gt; at the command line to see what queues are being used. Otherwise, you might have to ping your system administrator for this information.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;walltime&lt;/code&gt;: You will also need to pre-estimate the wall clock time, in seconds, that you want the worker node to be alive for. It should be significantly longer than the expected time you think you will need, so that your function call doesn't timeout unexpectedly. I have defaulted to 1.5 million seconds, which is about 18 days of continual runtime. In practice, I usually kill those worker processes after just a few hours.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Besides that, you also need to specify the resources that you need per worker process. In my example above, I'm asking for each worker process to use only 1GB of RAM, 1 core, and to use only 1 process per worker (i.e. no multiprocessing, I think).&lt;/p&gt;
&lt;p&gt;Finally, I can also specify extra environment setups that I will need. Because each worker process is a new process that has no knowledge of the parent process' environment, you might have to source some bash script, or activate a Python environment, or export some environment variable. This can be done under the &lt;code&gt;env_extra&lt;/code&gt; keyword, which accepts a list of strings.&lt;/p&gt;
&lt;h2 id=&quot;request-for-worker-compute-nodes&quot;&gt;Request for worker compute &quot;nodes&quot;&lt;/h2&gt;&lt;p&gt;I put &quot;nodes&quot; in quotation marks, because they are effectively logical nodes, rather than actual compute nodes. (Technically, I think a compute node minimally means one physical hardware unit with CPUs and RAM).&lt;/p&gt;
&lt;p&gt;In order to request for worker nodes to run your jobs, you need the next line of code:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cluster&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scale&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;With this line, under the hood, &lt;code&gt;dask-jobqueue&lt;/code&gt; will start submitting 500 jobs, each requesting 1GB of RAM and 1 core, populating my compute environment according to the instructions I provided under &lt;code&gt;env_extra&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;At the end of this, I effectively have a 500-node cluster on the larger GridEngine cluster (let's call this a &quot;virtual cluster&quot;), each with 1GB of RAM and 1 core available to it, on which I can submit functions to run.&lt;/p&gt;
&lt;h2 id=&quot;start-a-client-process&quot;&gt;Start a client process&lt;/h2&gt;&lt;p&gt;In order to submit jobs to my virtual cluster, I have to instantiate a client that is connected to the cluster, and is responsible for sending functions there.&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;dask.distributed&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Client&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;client&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Client&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cluster&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&quot;compute&quot;&gt;Compute!&lt;/h2&gt;&lt;p&gt;With this setup complete (I have it stored as a TextExpander snippets), we can now start submitting functions to the virtual cluster!&lt;/p&gt;
&lt;p&gt;To simulate this, let's define a square-rooting function that takes 2-3 seconds to run each time it is called, and returns the square of its inputs. This simulates a function call that is computationally semi-expensive to run a few times, but because call on this hundreds of thousands of time, the total running time to run it serially would be too much.&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;time&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sleep&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;math&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;random&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;slow_sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    Simulates the run time needed for a semi-expensive function call.&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;assert&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# define sleeping time in seconds, between 2-3 seconds.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sleep&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id=&quot;serial-execution&quot;&gt;Serial Execution&lt;/h3&gt;&lt;p&gt;In a naive, serial setting, we would call on the function in a for-loop:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;results&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;results&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;slow_sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This would take us anywhere between 20,000 to 30,000 seconds (approximately 8 hours, basically).&lt;/p&gt;
&lt;h3 id=&quot;parallel-execution&quot;&gt;Parallel Execution&lt;/h3&gt;&lt;p&gt;In order to execute this in parallel instead, we could do one of the following three ways:&lt;/p&gt;
&lt;h4 id=&quot;map&quot;&gt;map&lt;/h4&gt;&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sq_roots&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;client&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;slow_sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sq_roots&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;client&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gather&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sq_roots&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h4 id=&quot;for-loop&quot;&gt;for-loop&lt;/h4&gt;&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sq_roots&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sq_roots&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;client&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;submit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;slow_sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;restart&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# submit the function as first argument, then rest of arguments&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sq_roots&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;client&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gather&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sq_roots&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h4 id=&quot;delayed&quot;&gt;delayed&lt;/h4&gt;&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sq_roots&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sq_roots&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;delayed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;slow_sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sq_roots&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;compute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sq_roots&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;I have some comments on each of the three methods, each of which I have used.&lt;/p&gt;
&lt;p&gt;First off, &lt;strong&gt;each of them do require us to change the code that we would have written in serial&lt;/strong&gt;. This little bit of overhead is the only tradeoff we really need to make in order to gain parallelism.&lt;/p&gt;
&lt;p&gt;In terms of &lt;strong&gt;readability&lt;/strong&gt;, all of them are quite readable, though in my case, I tend to favour the for-loop with &lt;code&gt;client.submit&lt;/code&gt;. Here is why.&lt;/p&gt;
&lt;p&gt;For readability, the for-loop explicitly indicates that we are looping over something. It's probably more easy for novices to approach my code that way.&lt;/p&gt;
&lt;p&gt;For &lt;strong&gt;debuggability&lt;/strong&gt;, &lt;code&gt;client.submit&lt;/code&gt; returns a Futures object (same goes for &lt;code&gt;client.map&lt;/code&gt;). A &quot;Futures&quot; object might be confusing at first, so let me start by demystifying that. A Futures object promises that the result that is computed from &lt;code&gt;slow_sqrt&lt;/code&gt; will exist, and actually contains a ton of diagnostic information, including the &lt;code&gt;type&lt;/code&gt; of the object (which can be useful for diagnosing whether my function actually ran correctly). In addition to that, I can call on &lt;code&gt;Futures.result()&lt;/code&gt; to inspect the actual result (in this case, &lt;code&gt;sq_roots[0].result()&lt;/code&gt;). This is good for debugging the function call, in case there are issues when scaling up. (At work, I was pinging a database in parallel, and sometimes the ping would fail; debugging led me to include some failsafe code, including retries and sleeps with random lengths to stagger out database calls.)&lt;/p&gt;
&lt;p&gt;Finally, &lt;strong&gt;the Futures interface is non-blocking&lt;/strong&gt; on my Jupyter notebook session. Once I've submitted the jobs, I can continue with other development work in my notebook in later cells, and check back when the Dask dashboard indicates that the jobs are done.&lt;/p&gt;
&lt;p&gt;That said, I like the &lt;code&gt;delayed&lt;/code&gt; interface as well. Once I was done debugging and confident that my own data pipeline at work wouldn't encounter the failure modes I was seeing, I switched over to the &lt;code&gt;delayed&lt;/code&gt; interface and scaled up my analysis. I was willing to trade in the interactivity using the &lt;code&gt;Futures&lt;/code&gt; interface for the automation provided by the &lt;code&gt;delayed&lt;/code&gt; interface. (I also first used Dask on a single node through the delayed interface as well).&lt;/p&gt;
&lt;p&gt;Of course, there's something also to be said for the simplicity of two lines of code for parallelism (with the &lt;code&gt;client.map&lt;/code&gt; example).&lt;/p&gt;
&lt;p&gt;The final line in each of the code blocks allows us to &quot;gather&quot; the results back into my coordinator node's memory, thus completing the function call and giving us the result we needed.&lt;/p&gt;
&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;&lt;p&gt;That concludes it! The two key ideas illustrated in this blog post were:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;To set up a virtual cluster on a GridEngine system, we essentially harness the existing job submission system to generate workers that listen for tasks.&lt;/li&gt;
&lt;li&gt;A useful programming pattern is to &lt;code&gt;submit&lt;/code&gt; functions using the &lt;code&gt;client&lt;/code&gt; object using &lt;code&gt;client.submit(func, *args, **kwargs)&lt;/code&gt;. This requires minimal changes from serial code.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;practical-tips&quot;&gt;Practical Tips&lt;/h2&gt;&lt;p&gt;Here's some tips for doing parallel processing, which I've learned over the years.&lt;/p&gt;
&lt;p&gt;Firstly, never prematurely parallelize. It's as bad as prematurely optimizing code. If your code is running slowly, check first to make sure that there aren't algorithmic complexity issues, or bandwidths being clogged up (e.g. I/O bound). As the Dask docs state, it is easier to achieve those gains first before doing parallelization.&lt;/p&gt;
&lt;p&gt;Secondly, when developing parallel workflows, make sure to test the pipeline on subsets of input data first, and slowly scale up. It is during this period that you can also profile memory usage to check to see if you need to request for more RAM per worker.&lt;/p&gt;
&lt;p&gt;Thirdly, for GridEngine clusters, it is usually easier to request for many small worker nodes that consume few cores and small amounts of RAM. If your job is trivially parallelizable, this may be a good thing.&lt;/p&gt;
&lt;p&gt;Fourthly, it's useful to have realistic expectations on the kinds of speed-ups you can expect to gain. At work, through some ad-hoc profiling, I quickly came to the realization that concurrent database pings were the most likely bottleneck in my code's speed, and that nothing apart from increasing the number of concurrent database pings allowed would make my parallel code go faster.&lt;/p&gt;
&lt;p&gt;Finally, on a shared cluster, be respectful of others' usage. Don't request for unreasonable amounts of compute time. And when you're confirmed done with your analysis work, remember to shut down the virtual cluster! :)&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2018/9/4/optimizing-block-sparse-matrix-creation-with-python/">
    <title type="text">Optimizing Block Sparse Matrix Creation with Python</title>
    <id>urn:uuid:77088e4f-4e42-31ba-8d87-ad8dca59c91a</id>
    <updated>2018-09-04T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2018/9/4/optimizing-block-sparse-matrix-creation-with-python/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;&lt;p&gt;At work, I recently encountered a neat problem. I'd like to share it with you all.&lt;/p&gt;
&lt;p&gt;One of my projects involves graphs; specifically, it involves taking individual graphs and turning them into one big graph. If you've taken my Network Analysis Made Simple workshops before, you'll have learned that graphs can be represented as a matrix, such as the one below:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;networkx&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;nx&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;scipy.sparse&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sp&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;seaborn&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sns&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tqdm&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tqdm&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;typing&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;List&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numba&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;jit&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;sns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_context&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;talk&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_style&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;white&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load_ext&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;autoreload&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;autoreload&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matplotlib&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inline&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;InlineBackend&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure_format&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;retina&amp;#39;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;G&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;erdos_renyi_graph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_numpy_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;G&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;sns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;heatmap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;../../../../../blog/2018/9/4/optimizing-block-sparse-matrix-creation-with-python/output_2_1.png&quot; alt=&quot;png&quot;&gt;&lt;/p&gt;
&lt;p&gt;Because the matrix is so sparse, we can actually store it as a &lt;strong&gt;sparse matrix&lt;/strong&gt;:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A_sparse&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;adjacency_matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;G&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tocoo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A_sparse&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;row&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A_sparse&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A_sparse&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;[array([0, 0, 1, 2, 3, 3, 3, 4, 4, 5, 5, 5, 5, 6, 7, 7, 8, 8], dtype=int32),
 array([5, 7, 4, 7, 5, 6, 8, 1, 5, 0, 3, 4, 8, 3, 0, 2, 3, 5], dtype=int32),
 array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int64)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The most straightforward way of storing sparse matrices is in the COO (COOrdinate) format, which is also known as the &quot;triplet&quot; format, or the &quot;ijv&quot; format. (&quot;i&quot; is row, &quot;j&quot; is col, &quot;v&quot; is value)&lt;/p&gt;
&lt;p&gt;If we want to have two or more graphs stored together in a single matrix, which was what my projects required, then one way of representing them is as follows:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;G2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;erdos_renyi_graph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;A2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_numpy_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;G2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;heatmap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;block_diag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;todense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;../../../../../blog/2018/9/4/optimizing-block-sparse-matrix-creation-with-python/output_6_1.png&quot; alt=&quot;png&quot;&gt;&lt;/p&gt;
&lt;p&gt;Now, notice how there's 25 nodes in total (0 to 24), and that they form what we call a &quot;block diagonal&quot; format. In its &quot;dense&quot; form, we have to represent $25^2$ values inside the matrix. That's fine for small amounts of data, but if we have tens of thousands of graphs, that'll be impossible to deal with!&lt;/p&gt;
&lt;p&gt;You'll notice I used a function from &lt;code&gt;scipy.sparse&lt;/code&gt;, the &lt;code&gt;block_diag&lt;/code&gt; function, which will create a block diagonal sparse matrix from an iterable of input matrices.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;block_diag&lt;/code&gt; is the function that I want to talk about in this post.&lt;/p&gt;
&lt;h1 id=&quot;profiling-block_diag-performance&quot;&gt;Profiling &lt;code&gt;block_diag&lt;/code&gt; performance&lt;/h1&gt;&lt;p&gt;I had noticed that when dealing with tens of thousands of graphs, &lt;code&gt;block_diag&lt;/code&gt; was not performing up to scratch. Specifically, the time it needed would scale quadratically with the number of matrices provided.&lt;/p&gt;
&lt;p&gt;Let's take a look at some simulated data to illustrate this.&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Gs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;As&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;30&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;G&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;erdos_renyi_graph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Gs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;G&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_numpy_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;G&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;As&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Let's now define a function to profile the code.&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;time&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;random&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;profile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n_graphs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;400&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;600&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;800&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1400&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1600&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1800&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tqdm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_graphs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 3 replicates per n&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;start&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;As&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;data_sp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;profile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;block_diag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_sp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;number of graphs&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;time (s)&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;../../../../../blog/2018/9/4/optimizing-block-sparse-matrix-creation-with-python/output_12_1.png&quot; alt=&quot;png&quot;&gt;&lt;/p&gt;
&lt;p&gt;It is quite clear that the increase in time is super-linear, showing $O(n^2)$ scaling. (Out of impatience, I did not go beyond 50,000 graphs in this post, but at work, I did profile performance up to that many graphs. For reference, it took about 5 minutes to finish creating the scipy sparse matrix for 50K graphs.&lt;/p&gt;
&lt;h1 id=&quot;optimizing-block_diag-performance&quot;&gt;Optimizing &lt;code&gt;block_diag&lt;/code&gt; performance&lt;/h1&gt;&lt;p&gt;I decided to take a stab at creating an optimized version of &lt;code&gt;block_diag&lt;/code&gt;. Having profiled my code and discovering that sparse block diagonal matrix creation was a bottleneck, I implemented my own sparse block diagonal matrix creation routine using pure Python.&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_block_diag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;As&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;List&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]):&lt;/span&gt;
    &lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    Return the (row, col, data) triplet for a block diagonal matrix.&lt;/span&gt;

&lt;span class=&quot;sd&quot;&gt;    Intended to be put into a coo_matrix. Can be from scipy.sparse, but&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    also can be cupy.sparse, or Torch sparse etc.&lt;/span&gt;

&lt;span class=&quot;sd&quot;&gt;    Example usage:&lt;/span&gt;

&lt;span class=&quot;sd&quot;&gt;    &amp;gt;&amp;gt;&amp;gt; row, col, data = _block_diag(As)&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    &amp;gt;&amp;gt;&amp;gt; coo_matrix((data, (row, col)))&lt;/span&gt;

&lt;span class=&quot;sd&quot;&gt;    :param As: A list of numpy arrays to create a block diagonal matrix.&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    :returns: (row, col, data), each as lists.&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;row&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;col&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;start_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;As&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;nrows&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ncols&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nrows&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ncols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;row&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;start_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nrows&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;row&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Running it through the same profiling routine:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_custom&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;profile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_block_diag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_custom&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;custom&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_sp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;scipy.sparse&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;number of graphs&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;time (s)&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;../../../../../blog/2018/9/4/optimizing-block-sparse-matrix-creation-with-python/output_18_1.png&quot; alt=&quot;png&quot;&gt;&lt;/p&gt;
&lt;p&gt;I also happened to have listened in on a &lt;a href=&quot;https://www.youtube.com/watch?v=6oXedk2tGfk&quot;&gt;talk by Siu Kwan Lam&lt;/a&gt; during lunch, on &lt;code&gt;numba&lt;/code&gt;, the JIT optimizer that he has been developing for the past 5 years now. Seeing as how the code I had written in &lt;code&gt;_block_diag&lt;/code&gt; was all numeric code, which is exactly what &lt;code&gt;numba&lt;/code&gt; was designed for, I decided to try optimizing it with JIT.&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numba&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;jit&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;data_jit&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;profile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_block_diag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_custom&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;custom&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_sp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;scipy.sparse&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_jit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;jit&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;number of graphs&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;time (s)&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;../../../../../blog/2018/9/4/optimizing-block-sparse-matrix-creation-with-python/output_21_1.png&quot; alt=&quot;png&quot;&gt;&lt;/p&gt;
&lt;p&gt;Notice the speed-up that JIT-ing the code provided! (Granted, that first run was a &quot;warm-up&quot; run; once JIT-compiled, everything is really fast!)&lt;/p&gt;
&lt;p&gt;My custom implementation only returns the (row, col, data) triplet. This is an intentional design choice - having profiled the code with and without calling a COO matrix creation routine, I found the JIT-optimized performance to be significantly better without creating the COO matrix routine. As I still have to create a sparse matrix, I ended up with the following design:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;block_diag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;As&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;row&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;jit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_block_diag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;As&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;coo_matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;row&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;data_wrap&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;profile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;block_diag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_custom&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;custom&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_sp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;scipy.sparse&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_jit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;jit&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_wrap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;wrapped&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;number of graphs&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;time (s)&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;../../../../../blog/2018/9/4/optimizing-block-sparse-matrix-creation-with-python/output_24_1.png&quot; alt=&quot;png&quot;&gt;&lt;/p&gt;
&lt;p&gt;You'll notice that the array creation step induces a consistent overhead on top of the sparse matrix triplet creation routine, but stays flat and trends the &quot;jit&quot; dots quite consistently. It intersects the &quot;custom&quot; dots at about $10^3$ graphs. Given the problem that I've been tackling, which involves $10^4$ to $10^6$ graphs at a time, it is an absolutely worthwhile improvement to JIT-compile the &lt;code&gt;_block_diag&lt;/code&gt; function.&lt;/p&gt;
&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;&lt;p&gt;This was simultaneously a fun and useful exercise in optimizing my code!&lt;/p&gt;
&lt;p&gt;A few things I would take away from this:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Profiling code for bottlenecks can be really handy, and can be especially useful if we have a hypothesis on how to optimize it.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;numba&lt;/code&gt; can really speed up array-oriented Python computation. It lives up to the claims on its documentation.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I hope you learned something new, and I hope you also enjoyed reading this post as much as I enjoyed writing it!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2018/9/2/3d-printed-wifi-access-qr-codes-part-2/">
    <title type="text">3D Printed WiFi Access QR Codes: Part 2</title>
    <id>urn:uuid:b3ccc8af-4ef1-3088-a681-02870224322b</id>
    <updated>2018-09-02T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2018/9/2/3d-printed-wifi-access-qr-codes-part-2/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;&lt;p&gt;In this blog post, I'll detail how to create a 3D printable QR code model using Python.&lt;/p&gt;
&lt;h2 id=&quot;recap&quot;&gt;Recap&lt;/h2&gt;&lt;p&gt;In the previous blog post, I detailed how to use &lt;code&gt;pyqrcode&lt;/code&gt; to create a QR code for a WiFi string. The most important parts were:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The WiFi string. It should have the following pattern:&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;WIFI:S:&amp;lt;SSID&amp;gt;;T:&amp;lt;WPA|WEP|&amp;gt;;P:&amp;lt;password&amp;gt;;;
&lt;/pre&gt;&lt;/div&gt;
&lt;ol&gt;
&lt;li&gt;The QR code creator.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pyqrcode&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pq&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ssid&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;Family Guest Network&amp;quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;security&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;WPA&amp;quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;password&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;vn8h2sncu093y3nd!&amp;quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;qr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;create&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;WIFI:S:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{ssid}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;;T:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{security}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;;P:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{password}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;;;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&quot;creating-a-3d-printed-qr-code-with-python&quot;&gt;Creating a 3D Printed QR Code with Python&lt;/h2&gt;&lt;p&gt;Now, let's see how we can create 3D models with Python code. We will need a package called &lt;code&gt;SolidPython&lt;/code&gt;, and optionally &lt;code&gt;numpy&lt;/code&gt; to help us with some array processing. (It can be done entirely using built-in lists if needed.)&lt;/p&gt;
&lt;h3 id=&quot;create-qr-code-object&quot;&gt;Create QR Code Object&lt;/h3&gt;&lt;p&gt;To start, I first defined a convenience function that let me create and return a &lt;code&gt;QRCode&lt;/code&gt; object that can be passed around and manipulated.&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;create_wifi_qr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ssid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;security&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;password&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;qr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;create&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;WIFI:S:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{ssid}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;;T:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{security}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;;P:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{password}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;;;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;qr&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Its use will become evident later. You'll also notice I'm using type hints inside the function.&lt;/p&gt;
&lt;h3 id=&quot;create-text-representation&quot;&gt;Create Text Representation&lt;/h3&gt;&lt;p&gt;Using the function, we can create a text representation of the QR code:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;create_wifi_qr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ssid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;security&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;password&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This will give essentially a series of 1s and 0s. This is a string, though, not a &lt;code&gt;numpy&lt;/code&gt; array. Hence, we may have to convert this into a list of lists, or a &lt;code&gt;numpy&lt;/code&gt; array (as a user of the scientific Python stack, I prefer using arrays where possible, but in this case there is no real practical advantage to doing so because we are not doing linear algebra).&lt;/p&gt;
&lt;h3 id=&quot;create-array-representation&quot;&gt;Create Array Representation&lt;/h3&gt;&lt;p&gt;Let's now define a function that takes in the &lt;code&gt;QRCode&lt;/code&gt; object and return an array version of the text rendering.&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;qr2array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;qr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bit&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vstack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;With that, we can create an array version of our QR code above:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;qr2array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id=&quot;create-3d-model&quot;&gt;Create 3D Model&lt;/h3&gt;&lt;p&gt;Now, we're ready to play with &lt;code&gt;SolidPython&lt;/code&gt;!&lt;/p&gt;
&lt;p&gt;&lt;code&gt;SolidPython&lt;/code&gt; is a Python package that provides an interface to the OpenSCAD language. The OpenSCAD language allows a programmer to programmatically define 3D models using the language of geometry. This includes the creation of cubes and other 3D objects, as well as object manipulations, such as translation, coloring, and union-ing.&lt;/p&gt;
&lt;p&gt;For brevity, I'll not introduce you to more detail on what OpenSCAD is. Rather, I'll recommend two readings, to be read in order:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;https://www.makeuseof.com/tag/beginners-guide-openscad-programming-3d-printed-models/&quot;&gt;Beginner's Guide to OpenSCAD&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://solidpython.readthedocs.io/en/latest/&quot;&gt;SolidPython's Documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Take a look at the code below for an example of how we create the 3D object.&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;solid&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cube&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scad_render&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;translate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;union&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;SCALE&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# output defaults to 1 mm per unit; this lets us increase the size of objects proportionally.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cubes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;translate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SCALE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SCALE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;black&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cube&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SCALE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SCALE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;HEIGHT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])))&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;row&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;col&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;row&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;base_plate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;white&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cube&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SCALE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SCALE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;HEIGHT&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;qrobj&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;union&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cubes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;base_plate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scad_render&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qrobj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This will give the following OpenSCAD code, which I've truncated for brevity:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;union() {
    translate(v = [8, 8, 0]) {
        color(c = &amp;quot;black&amp;quot;) {
            cube(size = [2, 2, 4]);
        }
    }
    translate(v = [8, 10, 0]) {
        color(c = &amp;quot;black&amp;quot;) {
            cube(size = [2, 2, 4]);
        }
  ...
    translate(v = [88, 80, 0]) {
        color(c = &amp;quot;black&amp;quot;) {
            cube(size = [2, 2, 4]);
        }
    }
    translate(v = [88, 82, 0]) {
        color(c = &amp;quot;black&amp;quot;) {
            cube(size = [2, 2, 4]);
        }
    }
    color(c = &amp;quot;white&amp;quot;) {
        cube(size = [98, 98, 2.0000000000]);
    }
}
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;What we've done here is take the 1s and created cubes where they are supposed to be, and leave the zeros empty. Then, we add a &quot;base plate&quot; so that everything stays nice and connected, and finally union all of the cubes with the base plate, so that we get one solid piece that is 3D printed.&lt;/p&gt;
&lt;p&gt;If you observe the output of the function &lt;code&gt;scad_render&lt;/code&gt;, it will essentially be valid OpenSCAD text. With OpenSCAD text, you can paste it into OpenSCAD to render it:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;../../../../../blog/2018/9/2/3d-printed-wifi-access-qr-codes-part-2/openscad.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;Following that, it can be exported as an STL file. The export process in OpenSCAD takes some time, but once done, it then has to be first converted into a &lt;code&gt;.gcode&lt;/code&gt; file, which gives a 3D printer the necessary instructions to move its printhead around to print the QR code.&lt;/p&gt;
&lt;p&gt;In short, the flow is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;SolidPython -&amp;gt; OpenSCAD -&amp;gt; STL -&amp;gt; .gcode
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;&lt;p&gt;The key things to take away from this blog post are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;How to create a text representation of the QR code.&lt;/li&gt;
&lt;li&gt;How to convert the text representation into an array.&lt;/li&gt;
&lt;li&gt;How to create a 3D model of the QR code using the array.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now that you have an example of how to create an OpenSCAD file from Python using &lt;code&gt;SolidPython&lt;/code&gt;, I hope you'll go forth and make a ton of fun stuff!&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="http://www.ericmjl.com/blog/2018/9/1/3d-printed-wifi-access-qr-codes-part-1/">
    <title type="text">3D Printed WiFi Access QR Codes: Part 1</title>
    <id>urn:uuid:7893d2eb-dfb8-31af-96ff-99cf28127789</id>
    <updated>2018-09-01T00:00:00Z</updated>
    <link href="http://www.ericmjl.com/blog/2018/9/1/3d-printed-wifi-access-qr-codes-part-1/" />
    <author>
      <name>Eric J. Ma</name>
    </author>
    <content type="html">&lt;h2 id=&quot;the-project&quot;&gt;The Project&lt;/h2&gt;&lt;p&gt;Over the weekend, I embarked on a project to create a 3D printed QR code that guests at our house could scan to gain access to our guest wireless network.&lt;/p&gt;
&lt;h2 id=&quot;why-i-decided-to-do-this&quot;&gt;Why I decided to do this&lt;/h2&gt;&lt;p&gt;From the standpoint of practicality, sure, it's trivial to open up phone settings, find the WiFi network name, and give them the password, but... this has the coolness factor associated with it! Imagine scanning a 3D-printed QR code! Until this becomes commonplace, it's a cool thing to be able to do.&lt;/p&gt;
&lt;p&gt;Anyways, there's a ton of QR code generators out there on the web, and there's more than a handful of WiFi QR code generators out there - so why did I embark on this project?&lt;/p&gt;
&lt;p&gt;Partly it's borne out of security reasons - I am not giving my WiFi password up to some random website. Who knows whether they're actually storing the passwords?&lt;/p&gt;
&lt;p&gt;Another part of this is borne out of me wanting to scratch my itch surrounding QR codes. The last time I went to China (Xi'an and Shanghai, specifically), I saw QR codes &lt;em&gt;everywhere&lt;/em&gt;. There surely had to be something good we could use this for at home that didn't involve just packing and storage.&lt;/p&gt;
&lt;h2 id=&quot;getting-setup&quot;&gt;Getting Setup&lt;/h2&gt;&lt;p&gt;Ok, let's get started! To create QR codes, all you need are the following packages installed in your environment:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;pyqrcode&lt;/code&gt; [&lt;code&gt;pip&lt;/code&gt;]&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pypng&lt;/code&gt; [&lt;code&gt;pip&lt;/code&gt;]&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If you want to do the 3D printing part, you'll need another package:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;SolidPython&lt;/code&gt; [&lt;code&gt;pip&lt;/code&gt;]&lt;/li&gt;
&lt;li&gt;&lt;code&gt;numpy&lt;/code&gt; [&lt;code&gt;conda&lt;/code&gt;/&lt;code&gt;pip&lt;/code&gt;]&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Finally, if you'd like to work with command line interfaces and Flask, you'll need:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;click&lt;/code&gt; [&lt;code&gt;pip&lt;/code&gt;/&lt;code&gt;conda&lt;/code&gt;]&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Flask&lt;/code&gt; [&lt;code&gt;pip&lt;/code&gt;/&lt;code&gt;conda&lt;/code&gt;]&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;encoding-wifi-credentials-in-a-qr-code&quot;&gt;Encoding WiFi credentials in a QR code&lt;/h2&gt;&lt;p&gt;Let's start by creating a QR code for our WiFi guest network.&lt;/p&gt;
&lt;p&gt;Let's say that these are the security credentials for the network:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SSID (a.k.a. Network Name): &lt;code&gt;Family Guest Network&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Password: &lt;code&gt;vn8h2sncu093y3nd!&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Security Type (one of WPA or WEP): &lt;code&gt;WPA&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Because QR codes are merely two-dimensional barcodes that encode a string that can be parsed by another program, in order to create a QR-code that is readable for accessing WiFi, we need a string that can be parsed. This string is structured as follows:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;WIFI:S:&amp;lt;SSID&amp;gt;;T:&amp;lt;WPA|WEP|&amp;gt;;P:&amp;lt;password&amp;gt;;;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;So in our case, we would want a string that looks like:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;WIFI:S:Family Guest Network;T:WPA;P:vn8h2sncu093y3nd!;;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, we can code up our Python program do encode the QR code for us. I'll assume you're running Python 3.6 or later.&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pyqrcode&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pq&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ssid&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;Family Guest Network&amp;quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;security&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;WPA&amp;quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;password&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;vn8h2sncu093y3nd!&amp;quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;qr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;create&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;WIFI:S:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{ssid}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;;T:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{security}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;;P:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{password}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;;;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;terminal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;With that block of code, you should get a QR code printed to your terminal, just like that!&lt;/p&gt;
&lt;p&gt;Let's say you wanted to do the simple thing, and just have a regular laser/inkjet printer make a printout of the QR code. To do so, you can save the QR code to disk as a PNG file:&lt;/p&gt;
&lt;div class=&quot;hll&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;png&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;home_guest_wifi.png&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;&lt;p&gt;And just like that, you've used Python to create a WiFi QR code!  How easy was that?&lt;/p&gt;
&lt;p&gt;Along the way, I also used Kite in the Atom text editor while embarking on this project - this allowed me to view documentation and common usage patterns for the packages I imported.&lt;/p&gt;
&lt;p&gt;Now, if you remember that QR codes are just &quot;ASCII strings encoded in a 2D barcode&quot;, then you'll know that you can pass any arbitrary string into the &lt;code&gt;pyqrcode.create()&lt;/code&gt; function. That means you can come up with any creative use of a short string that needs to be scanned to be useful! For example, you can create business cards with your LinkedIn profile URL embedded in the QR code, or use it to encode a serial number information on your possessions, or more!&lt;/p&gt;
&lt;p&gt;Stay tuned for the coming blog posts!&lt;/p&gt;
</content>
  </entry>
</feed>
